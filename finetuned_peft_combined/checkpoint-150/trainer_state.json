{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.05375380756136893,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00035835871707579287,
      "grad_norm": 20.3043155670166,
      "learning_rate": 0.0,
      "loss": 15.7337,
      "step": 1
    },
    {
      "epoch": 0.0007167174341515857,
      "grad_norm": 18.318492889404297,
      "learning_rate": 5e-05,
      "loss": 15.8805,
      "step": 2
    },
    {
      "epoch": 0.0010750761512273786,
      "grad_norm": 18.94686508178711,
      "learning_rate": 4.999402771141902e-05,
      "loss": 15.5972,
      "step": 3
    },
    {
      "epoch": 0.0014334348683031715,
      "grad_norm": 20.0352840423584,
      "learning_rate": 4.9988055422838034e-05,
      "loss": 15.2382,
      "step": 4
    },
    {
      "epoch": 0.0017917935853789643,
      "grad_norm": 20.746166229248047,
      "learning_rate": 4.998208313425705e-05,
      "loss": 14.7711,
      "step": 5
    },
    {
      "epoch": 0.002150152302454757,
      "grad_norm": 24.307992935180664,
      "learning_rate": 4.9976110845676066e-05,
      "loss": 14.3096,
      "step": 6
    },
    {
      "epoch": 0.00250851101953055,
      "grad_norm": 25.649250030517578,
      "learning_rate": 4.997013855709508e-05,
      "loss": 14.0469,
      "step": 7
    },
    {
      "epoch": 0.002866869736606343,
      "grad_norm": 29.355260848999023,
      "learning_rate": 4.99641662685141e-05,
      "loss": 13.1152,
      "step": 8
    },
    {
      "epoch": 0.003225228453682136,
      "grad_norm": 28.649314880371094,
      "learning_rate": 4.995819397993311e-05,
      "loss": 12.9541,
      "step": 9
    },
    {
      "epoch": 0.0035835871707579287,
      "grad_norm": 33.65465545654297,
      "learning_rate": 4.995222169135213e-05,
      "loss": 12.207,
      "step": 10
    },
    {
      "epoch": 0.003941945887833721,
      "grad_norm": 35.941829681396484,
      "learning_rate": 4.9946249402771146e-05,
      "loss": 11.6176,
      "step": 11
    },
    {
      "epoch": 0.004300304604909514,
      "grad_norm": 40.75389862060547,
      "learning_rate": 4.994027711419016e-05,
      "loss": 10.6479,
      "step": 12
    },
    {
      "epoch": 0.004658663321985307,
      "grad_norm": 44.15620803833008,
      "learning_rate": 4.993430482560917e-05,
      "loss": 9.8768,
      "step": 13
    },
    {
      "epoch": 0.0050170220390611,
      "grad_norm": 44.37617492675781,
      "learning_rate": 4.9928332537028194e-05,
      "loss": 9.2517,
      "step": 14
    },
    {
      "epoch": 0.005375380756136893,
      "grad_norm": 46.66408157348633,
      "learning_rate": 4.992236024844721e-05,
      "loss": 8.0375,
      "step": 15
    },
    {
      "epoch": 0.005733739473212686,
      "grad_norm": 50.66767501831055,
      "learning_rate": 4.9916387959866226e-05,
      "loss": 6.8855,
      "step": 16
    },
    {
      "epoch": 0.006092098190288478,
      "grad_norm": 55.534969329833984,
      "learning_rate": 4.9910415671285235e-05,
      "loss": 5.8985,
      "step": 17
    },
    {
      "epoch": 0.006450456907364272,
      "grad_norm": 57.557167053222656,
      "learning_rate": 4.990444338270425e-05,
      "loss": 4.6042,
      "step": 18
    },
    {
      "epoch": 0.006808815624440064,
      "grad_norm": 55.75872039794922,
      "learning_rate": 4.9898471094123274e-05,
      "loss": 3.8512,
      "step": 19
    },
    {
      "epoch": 0.007167174341515857,
      "grad_norm": 49.14149856567383,
      "learning_rate": 4.989249880554229e-05,
      "loss": 2.5176,
      "step": 20
    },
    {
      "epoch": 0.00752553305859165,
      "grad_norm": 30.030378341674805,
      "learning_rate": 4.98865265169613e-05,
      "loss": 1.3699,
      "step": 21
    },
    {
      "epoch": 0.007883891775667442,
      "grad_norm": 19.32173728942871,
      "learning_rate": 4.9880554228380315e-05,
      "loss": 1.1864,
      "step": 22
    },
    {
      "epoch": 0.008242250492743236,
      "grad_norm": 6.639308452606201,
      "learning_rate": 4.987458193979933e-05,
      "loss": 0.7265,
      "step": 23
    },
    {
      "epoch": 0.008600609209819029,
      "grad_norm": 5.71967887878418,
      "learning_rate": 4.9868609651218354e-05,
      "loss": 0.6254,
      "step": 24
    },
    {
      "epoch": 0.008958967926894821,
      "grad_norm": 2.0705337524414062,
      "learning_rate": 4.9862637362637363e-05,
      "loss": 0.6671,
      "step": 25
    },
    {
      "epoch": 0.009317326643970614,
      "grad_norm": 1.0532517433166504,
      "learning_rate": 4.985666507405638e-05,
      "loss": 0.5446,
      "step": 26
    },
    {
      "epoch": 0.009675685361046408,
      "grad_norm": 0.8239157795906067,
      "learning_rate": 4.9850692785475395e-05,
      "loss": 0.6769,
      "step": 27
    },
    {
      "epoch": 0.0100340440781222,
      "grad_norm": 0.5259680151939392,
      "learning_rate": 4.984472049689442e-05,
      "loss": 0.5561,
      "step": 28
    },
    {
      "epoch": 0.010392402795197993,
      "grad_norm": 0.37262845039367676,
      "learning_rate": 4.983874820831343e-05,
      "loss": 0.5671,
      "step": 29
    },
    {
      "epoch": 0.010750761512273785,
      "grad_norm": 0.3263879716396332,
      "learning_rate": 4.983277591973244e-05,
      "loss": 0.532,
      "step": 30
    },
    {
      "epoch": 0.01110912022934958,
      "grad_norm": 0.3708037734031677,
      "learning_rate": 4.982680363115146e-05,
      "loss": 0.6144,
      "step": 31
    },
    {
      "epoch": 0.011467478946425372,
      "grad_norm": 0.3715900182723999,
      "learning_rate": 4.9820831342570475e-05,
      "loss": 0.8116,
      "step": 32
    },
    {
      "epoch": 0.011825837663501164,
      "grad_norm": 0.2281980961561203,
      "learning_rate": 4.981485905398949e-05,
      "loss": 0.536,
      "step": 33
    },
    {
      "epoch": 0.012184196380576957,
      "grad_norm": 0.20869657397270203,
      "learning_rate": 4.980888676540851e-05,
      "loss": 0.5718,
      "step": 34
    },
    {
      "epoch": 0.01254255509765275,
      "grad_norm": 0.26453036069869995,
      "learning_rate": 4.980291447682752e-05,
      "loss": 0.5782,
      "step": 35
    },
    {
      "epoch": 0.012900913814728543,
      "grad_norm": 0.20718535780906677,
      "learning_rate": 4.979694218824654e-05,
      "loss": 0.5933,
      "step": 36
    },
    {
      "epoch": 0.013259272531804336,
      "grad_norm": 0.2096778005361557,
      "learning_rate": 4.979096989966555e-05,
      "loss": 0.5738,
      "step": 37
    },
    {
      "epoch": 0.013617631248880128,
      "grad_norm": 0.1907060295343399,
      "learning_rate": 4.978499761108457e-05,
      "loss": 0.5671,
      "step": 38
    },
    {
      "epoch": 0.013975989965955922,
      "grad_norm": 0.42075276374816895,
      "learning_rate": 4.977902532250359e-05,
      "loss": 0.5518,
      "step": 39
    },
    {
      "epoch": 0.014334348683031715,
      "grad_norm": 0.1869766116142273,
      "learning_rate": 4.97730530339226e-05,
      "loss": 0.6313,
      "step": 40
    },
    {
      "epoch": 0.014692707400107507,
      "grad_norm": 0.2915135324001312,
      "learning_rate": 4.976708074534161e-05,
      "loss": 0.5431,
      "step": 41
    },
    {
      "epoch": 0.0150510661171833,
      "grad_norm": 0.21010395884513855,
      "learning_rate": 4.9761108456760635e-05,
      "loss": 0.5521,
      "step": 42
    },
    {
      "epoch": 0.015409424834259094,
      "grad_norm": 0.17786701023578644,
      "learning_rate": 4.975513616817965e-05,
      "loss": 0.5294,
      "step": 43
    },
    {
      "epoch": 0.015767783551334884,
      "grad_norm": 0.19910797476768494,
      "learning_rate": 4.974916387959867e-05,
      "loss": 0.5891,
      "step": 44
    },
    {
      "epoch": 0.01612614226841068,
      "grad_norm": 0.20209500193595886,
      "learning_rate": 4.9743191591017676e-05,
      "loss": 0.5571,
      "step": 45
    },
    {
      "epoch": 0.016484500985486473,
      "grad_norm": 0.18157163262367249,
      "learning_rate": 4.973721930243669e-05,
      "loss": 0.5636,
      "step": 46
    },
    {
      "epoch": 0.016842859702562264,
      "grad_norm": 0.20442518591880798,
      "learning_rate": 4.9731247013855715e-05,
      "loss": 0.6321,
      "step": 47
    },
    {
      "epoch": 0.017201218419638058,
      "grad_norm": 0.1713770627975464,
      "learning_rate": 4.972527472527473e-05,
      "loss": 0.5126,
      "step": 48
    },
    {
      "epoch": 0.017559577136713852,
      "grad_norm": 0.17678509652614594,
      "learning_rate": 4.971930243669374e-05,
      "loss": 0.5431,
      "step": 49
    },
    {
      "epoch": 0.017917935853789643,
      "grad_norm": 0.17626811563968658,
      "learning_rate": 4.9713330148112756e-05,
      "loss": 0.4775,
      "step": 50
    },
    {
      "epoch": 0.018276294570865437,
      "grad_norm": 0.1643982082605362,
      "learning_rate": 4.970735785953177e-05,
      "loss": 0.5168,
      "step": 51
    },
    {
      "epoch": 0.018634653287941227,
      "grad_norm": 0.16538436710834503,
      "learning_rate": 4.9701385570950795e-05,
      "loss": 0.4727,
      "step": 52
    },
    {
      "epoch": 0.01899301200501702,
      "grad_norm": 0.17588351666927338,
      "learning_rate": 4.9695413282369804e-05,
      "loss": 0.5673,
      "step": 53
    },
    {
      "epoch": 0.019351370722092816,
      "grad_norm": 0.1993636041879654,
      "learning_rate": 4.968944099378882e-05,
      "loss": 0.7076,
      "step": 54
    },
    {
      "epoch": 0.019709729439168606,
      "grad_norm": 0.19924584031105042,
      "learning_rate": 4.9683468705207836e-05,
      "loss": 0.5164,
      "step": 55
    },
    {
      "epoch": 0.0200680881562444,
      "grad_norm": 0.17927846312522888,
      "learning_rate": 4.967749641662686e-05,
      "loss": 0.5028,
      "step": 56
    },
    {
      "epoch": 0.020426446873320195,
      "grad_norm": 0.18965227901935577,
      "learning_rate": 4.967152412804587e-05,
      "loss": 0.515,
      "step": 57
    },
    {
      "epoch": 0.020784805590395986,
      "grad_norm": 0.16628322005271912,
      "learning_rate": 4.9665551839464884e-05,
      "loss": 0.4669,
      "step": 58
    },
    {
      "epoch": 0.02114316430747178,
      "grad_norm": 0.1917169988155365,
      "learning_rate": 4.96595795508839e-05,
      "loss": 0.5215,
      "step": 59
    },
    {
      "epoch": 0.02150152302454757,
      "grad_norm": 0.17563919723033905,
      "learning_rate": 4.9653607262302916e-05,
      "loss": 0.5229,
      "step": 60
    },
    {
      "epoch": 0.021859881741623365,
      "grad_norm": 0.19013163447380066,
      "learning_rate": 4.964763497372193e-05,
      "loss": 0.5886,
      "step": 61
    },
    {
      "epoch": 0.02221824045869916,
      "grad_norm": 0.17237918078899384,
      "learning_rate": 4.964166268514095e-05,
      "loss": 0.5383,
      "step": 62
    },
    {
      "epoch": 0.02257659917577495,
      "grad_norm": 0.16468462347984314,
      "learning_rate": 4.9635690396559964e-05,
      "loss": 0.4271,
      "step": 63
    },
    {
      "epoch": 0.022934957892850744,
      "grad_norm": 0.1634742170572281,
      "learning_rate": 4.962971810797898e-05,
      "loss": 0.3951,
      "step": 64
    },
    {
      "epoch": 0.023293316609926538,
      "grad_norm": 0.18364879488945007,
      "learning_rate": 4.9623745819397996e-05,
      "loss": 0.495,
      "step": 65
    },
    {
      "epoch": 0.02365167532700233,
      "grad_norm": 0.19067758321762085,
      "learning_rate": 4.961777353081701e-05,
      "loss": 0.5052,
      "step": 66
    },
    {
      "epoch": 0.024010034044078123,
      "grad_norm": 0.17453205585479736,
      "learning_rate": 4.961180124223603e-05,
      "loss": 0.4463,
      "step": 67
    },
    {
      "epoch": 0.024368392761153913,
      "grad_norm": 0.18469516932964325,
      "learning_rate": 4.9605828953655044e-05,
      "loss": 0.489,
      "step": 68
    },
    {
      "epoch": 0.024726751478229708,
      "grad_norm": 0.18034368753433228,
      "learning_rate": 4.959985666507406e-05,
      "loss": 0.5132,
      "step": 69
    },
    {
      "epoch": 0.0250851101953055,
      "grad_norm": 0.20600056648254395,
      "learning_rate": 4.9593884376493076e-05,
      "loss": 0.5373,
      "step": 70
    },
    {
      "epoch": 0.025443468912381292,
      "grad_norm": 0.1936316043138504,
      "learning_rate": 4.958791208791209e-05,
      "loss": 0.5291,
      "step": 71
    },
    {
      "epoch": 0.025801827629457087,
      "grad_norm": 0.1939651519060135,
      "learning_rate": 4.958193979933111e-05,
      "loss": 0.4826,
      "step": 72
    },
    {
      "epoch": 0.02616018634653288,
      "grad_norm": 0.18426266312599182,
      "learning_rate": 4.9575967510750124e-05,
      "loss": 0.4285,
      "step": 73
    },
    {
      "epoch": 0.02651854506360867,
      "grad_norm": 0.1788136512041092,
      "learning_rate": 4.9569995222169133e-05,
      "loss": 0.3925,
      "step": 74
    },
    {
      "epoch": 0.026876903780684466,
      "grad_norm": 0.31094104051589966,
      "learning_rate": 4.9564022933588156e-05,
      "loss": 0.4312,
      "step": 75
    },
    {
      "epoch": 0.027235262497760256,
      "grad_norm": 0.2370317578315735,
      "learning_rate": 4.955805064500717e-05,
      "loss": 0.5078,
      "step": 76
    },
    {
      "epoch": 0.02759362121483605,
      "grad_norm": 0.21878604590892792,
      "learning_rate": 4.955207835642619e-05,
      "loss": 0.4294,
      "step": 77
    },
    {
      "epoch": 0.027951979931911845,
      "grad_norm": 0.19398300349712372,
      "learning_rate": 4.95461060678452e-05,
      "loss": 0.3609,
      "step": 78
    },
    {
      "epoch": 0.028310338648987635,
      "grad_norm": 0.19758161902427673,
      "learning_rate": 4.954013377926421e-05,
      "loss": 0.397,
      "step": 79
    },
    {
      "epoch": 0.02866869736606343,
      "grad_norm": 0.21422527730464935,
      "learning_rate": 4.9534161490683236e-05,
      "loss": 0.4557,
      "step": 80
    },
    {
      "epoch": 0.029027056083139224,
      "grad_norm": 0.22260454297065735,
      "learning_rate": 4.9528189202102245e-05,
      "loss": 0.4256,
      "step": 81
    },
    {
      "epoch": 0.029385414800215014,
      "grad_norm": 0.2079811841249466,
      "learning_rate": 4.952221691352126e-05,
      "loss": 0.4305,
      "step": 82
    },
    {
      "epoch": 0.02974377351729081,
      "grad_norm": 0.2266877442598343,
      "learning_rate": 4.951624462494028e-05,
      "loss": 0.478,
      "step": 83
    },
    {
      "epoch": 0.0301021322343666,
      "grad_norm": 0.2325388342142105,
      "learning_rate": 4.95102723363593e-05,
      "loss": 0.4552,
      "step": 84
    },
    {
      "epoch": 0.030460490951442393,
      "grad_norm": 0.218533456325531,
      "learning_rate": 4.950430004777831e-05,
      "loss": 0.3833,
      "step": 85
    },
    {
      "epoch": 0.030818849668518188,
      "grad_norm": 0.24167372286319733,
      "learning_rate": 4.9498327759197325e-05,
      "loss": 0.4908,
      "step": 86
    },
    {
      "epoch": 0.03117720838559398,
      "grad_norm": 0.2361288219690323,
      "learning_rate": 4.949235547061634e-05,
      "loss": 0.4302,
      "step": 87
    },
    {
      "epoch": 0.03153556710266977,
      "grad_norm": 0.24358461797237396,
      "learning_rate": 4.948638318203536e-05,
      "loss": 0.4491,
      "step": 88
    },
    {
      "epoch": 0.03189392581974557,
      "grad_norm": 0.25181522965431213,
      "learning_rate": 4.948041089345437e-05,
      "loss": 0.4773,
      "step": 89
    },
    {
      "epoch": 0.03225228453682136,
      "grad_norm": 0.275645911693573,
      "learning_rate": 4.947443860487339e-05,
      "loss": 0.4813,
      "step": 90
    },
    {
      "epoch": 0.03261064325389715,
      "grad_norm": 0.3181723654270172,
      "learning_rate": 4.9468466316292405e-05,
      "loss": 0.5817,
      "step": 91
    },
    {
      "epoch": 0.032969001970972946,
      "grad_norm": 0.28946229815483093,
      "learning_rate": 4.946249402771142e-05,
      "loss": 0.3797,
      "step": 92
    },
    {
      "epoch": 0.033327360688048736,
      "grad_norm": 0.4307362735271454,
      "learning_rate": 4.945652173913044e-05,
      "loss": 0.4234,
      "step": 93
    },
    {
      "epoch": 0.03368571940512453,
      "grad_norm": 0.28920120000839233,
      "learning_rate": 4.945054945054945e-05,
      "loss": 0.3889,
      "step": 94
    },
    {
      "epoch": 0.034044078122200325,
      "grad_norm": 0.3062282204627991,
      "learning_rate": 4.944457716196847e-05,
      "loss": 0.327,
      "step": 95
    },
    {
      "epoch": 0.034402436839276115,
      "grad_norm": 0.309128999710083,
      "learning_rate": 4.9438604873387485e-05,
      "loss": 0.3212,
      "step": 96
    },
    {
      "epoch": 0.034760795556351906,
      "grad_norm": 0.3194722533226013,
      "learning_rate": 4.94326325848065e-05,
      "loss": 0.3503,
      "step": 97
    },
    {
      "epoch": 0.035119154273427704,
      "grad_norm": 0.36419907212257385,
      "learning_rate": 4.942666029622552e-05,
      "loss": 0.4363,
      "step": 98
    },
    {
      "epoch": 0.035477512990503494,
      "grad_norm": 0.35646557807922363,
      "learning_rate": 4.942068800764453e-05,
      "loss": 0.3646,
      "step": 99
    },
    {
      "epoch": 0.035835871707579285,
      "grad_norm": 0.3518977165222168,
      "learning_rate": 4.941471571906355e-05,
      "loss": 0.3358,
      "step": 100
    },
    {
      "epoch": 0.03619423042465508,
      "grad_norm": 0.37824687361717224,
      "learning_rate": 4.9408743430482565e-05,
      "loss": 0.4602,
      "step": 101
    },
    {
      "epoch": 0.036552589141730873,
      "grad_norm": 0.33916598558425903,
      "learning_rate": 4.9402771141901574e-05,
      "loss": 0.2513,
      "step": 102
    },
    {
      "epoch": 0.036910947858806664,
      "grad_norm": 0.37959322333335876,
      "learning_rate": 4.93967988533206e-05,
      "loss": 0.3577,
      "step": 103
    },
    {
      "epoch": 0.037269306575882455,
      "grad_norm": 0.32848769426345825,
      "learning_rate": 4.939082656473961e-05,
      "loss": 0.2202,
      "step": 104
    },
    {
      "epoch": 0.03762766529295825,
      "grad_norm": 0.3451696038246155,
      "learning_rate": 4.938485427615863e-05,
      "loss": 0.4048,
      "step": 105
    },
    {
      "epoch": 0.03798602401003404,
      "grad_norm": 0.33096784353256226,
      "learning_rate": 4.937888198757764e-05,
      "loss": 0.4056,
      "step": 106
    },
    {
      "epoch": 0.038344382727109834,
      "grad_norm": 0.30412161350250244,
      "learning_rate": 4.9372909698996654e-05,
      "loss": 0.3498,
      "step": 107
    },
    {
      "epoch": 0.03870274144418563,
      "grad_norm": 0.2997516393661499,
      "learning_rate": 4.936693741041568e-05,
      "loss": 0.2483,
      "step": 108
    },
    {
      "epoch": 0.03906110016126142,
      "grad_norm": 0.2972385585308075,
      "learning_rate": 4.936096512183469e-05,
      "loss": 0.3165,
      "step": 109
    },
    {
      "epoch": 0.03941945887833721,
      "grad_norm": 0.29209890961647034,
      "learning_rate": 4.93549928332537e-05,
      "loss": 0.247,
      "step": 110
    },
    {
      "epoch": 0.03977781759541301,
      "grad_norm": 0.27570584416389465,
      "learning_rate": 4.934902054467272e-05,
      "loss": 0.3293,
      "step": 111
    },
    {
      "epoch": 0.0401361763124888,
      "grad_norm": 0.2765166759490967,
      "learning_rate": 4.9343048256091734e-05,
      "loss": 0.2333,
      "step": 112
    },
    {
      "epoch": 0.04049453502956459,
      "grad_norm": 0.26391762495040894,
      "learning_rate": 4.933707596751076e-05,
      "loss": 0.2451,
      "step": 113
    },
    {
      "epoch": 0.04085289374664039,
      "grad_norm": 0.3353630304336548,
      "learning_rate": 4.9331103678929766e-05,
      "loss": 0.512,
      "step": 114
    },
    {
      "epoch": 0.04121125246371618,
      "grad_norm": 0.26737532019615173,
      "learning_rate": 4.932513139034878e-05,
      "loss": 0.2342,
      "step": 115
    },
    {
      "epoch": 0.04156961118079197,
      "grad_norm": 0.22714956104755402,
      "learning_rate": 4.93191591017678e-05,
      "loss": 0.2209,
      "step": 116
    },
    {
      "epoch": 0.04192796989786777,
      "grad_norm": 0.23078717291355133,
      "learning_rate": 4.931318681318682e-05,
      "loss": 0.2065,
      "step": 117
    },
    {
      "epoch": 0.04228632861494356,
      "grad_norm": 0.276682585477829,
      "learning_rate": 4.930721452460583e-05,
      "loss": 0.2281,
      "step": 118
    },
    {
      "epoch": 0.04264468733201935,
      "grad_norm": 0.3047322928905487,
      "learning_rate": 4.9301242236024846e-05,
      "loss": 0.2358,
      "step": 119
    },
    {
      "epoch": 0.04300304604909514,
      "grad_norm": 0.33730417490005493,
      "learning_rate": 4.929526994744386e-05,
      "loss": 0.3056,
      "step": 120
    },
    {
      "epoch": 0.04336140476617094,
      "grad_norm": 0.2963522970676422,
      "learning_rate": 4.928929765886288e-05,
      "loss": 0.2079,
      "step": 121
    },
    {
      "epoch": 0.04371976348324673,
      "grad_norm": 0.47749483585357666,
      "learning_rate": 4.9283325370281894e-05,
      "loss": 0.3273,
      "step": 122
    },
    {
      "epoch": 0.04407812220032252,
      "grad_norm": 0.30452725291252136,
      "learning_rate": 4.927735308170091e-05,
      "loss": 0.2249,
      "step": 123
    },
    {
      "epoch": 0.04443648091739832,
      "grad_norm": 0.24357815086841583,
      "learning_rate": 4.9271380793119926e-05,
      "loss": 0.1787,
      "step": 124
    },
    {
      "epoch": 0.04479483963447411,
      "grad_norm": 0.2482309490442276,
      "learning_rate": 4.926540850453894e-05,
      "loss": 0.1985,
      "step": 125
    },
    {
      "epoch": 0.0451531983515499,
      "grad_norm": 0.278746634721756,
      "learning_rate": 4.925943621595796e-05,
      "loss": 0.215,
      "step": 126
    },
    {
      "epoch": 0.045511557068625696,
      "grad_norm": 0.29208019375801086,
      "learning_rate": 4.9253463927376974e-05,
      "loss": 0.2115,
      "step": 127
    },
    {
      "epoch": 0.04586991578570149,
      "grad_norm": 0.34888461232185364,
      "learning_rate": 4.924749163879599e-05,
      "loss": 0.2906,
      "step": 128
    },
    {
      "epoch": 0.04622827450277728,
      "grad_norm": 0.25117626786231995,
      "learning_rate": 4.9241519350215006e-05,
      "loss": 0.1571,
      "step": 129
    },
    {
      "epoch": 0.046586633219853076,
      "grad_norm": 0.27335798740386963,
      "learning_rate": 4.9235547061634015e-05,
      "loss": 0.1428,
      "step": 130
    },
    {
      "epoch": 0.046944991936928866,
      "grad_norm": 0.28515613079071045,
      "learning_rate": 4.922957477305304e-05,
      "loss": 0.2184,
      "step": 131
    },
    {
      "epoch": 0.04730335065400466,
      "grad_norm": 0.25557321310043335,
      "learning_rate": 4.9223602484472054e-05,
      "loss": 0.1955,
      "step": 132
    },
    {
      "epoch": 0.047661709371080455,
      "grad_norm": 0.18515686690807343,
      "learning_rate": 4.921763019589107e-05,
      "loss": 0.1739,
      "step": 133
    },
    {
      "epoch": 0.048020068088156245,
      "grad_norm": 0.2710229754447937,
      "learning_rate": 4.921165790731008e-05,
      "loss": 0.2893,
      "step": 134
    },
    {
      "epoch": 0.048378426805232036,
      "grad_norm": 0.2276855856180191,
      "learning_rate": 4.9205685618729095e-05,
      "loss": 0.242,
      "step": 135
    },
    {
      "epoch": 0.04873678552230783,
      "grad_norm": 0.15873204171657562,
      "learning_rate": 4.919971333014812e-05,
      "loss": 0.1818,
      "step": 136
    },
    {
      "epoch": 0.049095144239383624,
      "grad_norm": 0.2027217149734497,
      "learning_rate": 4.9193741041567134e-05,
      "loss": 0.181,
      "step": 137
    },
    {
      "epoch": 0.049453502956459415,
      "grad_norm": 0.23012855648994446,
      "learning_rate": 4.918776875298614e-05,
      "loss": 0.206,
      "step": 138
    },
    {
      "epoch": 0.049811861673535206,
      "grad_norm": 0.18605534732341766,
      "learning_rate": 4.918179646440516e-05,
      "loss": 0.1549,
      "step": 139
    },
    {
      "epoch": 0.050170220390611,
      "grad_norm": 0.24357332289218903,
      "learning_rate": 4.9175824175824175e-05,
      "loss": 0.2215,
      "step": 140
    },
    {
      "epoch": 0.050528579107686794,
      "grad_norm": 0.3944641947746277,
      "learning_rate": 4.91698518872432e-05,
      "loss": 0.2344,
      "step": 141
    },
    {
      "epoch": 0.050886937824762585,
      "grad_norm": 0.17388564348220825,
      "learning_rate": 4.916387959866221e-05,
      "loss": 0.1727,
      "step": 142
    },
    {
      "epoch": 0.05124529654183838,
      "grad_norm": 0.16706374287605286,
      "learning_rate": 4.915790731008122e-05,
      "loss": 0.1469,
      "step": 143
    },
    {
      "epoch": 0.05160365525891417,
      "grad_norm": 0.16845233738422394,
      "learning_rate": 4.915193502150024e-05,
      "loss": 0.1376,
      "step": 144
    },
    {
      "epoch": 0.051962013975989964,
      "grad_norm": 0.2852131724357605,
      "learning_rate": 4.914596273291926e-05,
      "loss": 0.3542,
      "step": 145
    },
    {
      "epoch": 0.05232037269306576,
      "grad_norm": 0.245087668299675,
      "learning_rate": 4.913999044433827e-05,
      "loss": 0.2857,
      "step": 146
    },
    {
      "epoch": 0.05267873141014155,
      "grad_norm": 0.26394104957580566,
      "learning_rate": 4.913401815575729e-05,
      "loss": 0.2417,
      "step": 147
    },
    {
      "epoch": 0.05303709012721734,
      "grad_norm": 0.25003480911254883,
      "learning_rate": 4.91280458671763e-05,
      "loss": 0.234,
      "step": 148
    },
    {
      "epoch": 0.05339544884429314,
      "grad_norm": 0.19619432091712952,
      "learning_rate": 4.912207357859532e-05,
      "loss": 0.1799,
      "step": 149
    },
    {
      "epoch": 0.05375380756136893,
      "grad_norm": 0.268304705619812,
      "learning_rate": 4.9116101290014335e-05,
      "loss": 0.2243,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 8373,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5199743051366400.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
