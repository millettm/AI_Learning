{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.10392402795197993,
  "eval_steps": 500,
  "global_step": 290,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00035835871707579287,
      "grad_norm": 20.3043155670166,
      "learning_rate": 0.0,
      "loss": 15.7337,
      "step": 1
    },
    {
      "epoch": 0.0007167174341515857,
      "grad_norm": 18.318492889404297,
      "learning_rate": 5e-05,
      "loss": 15.8805,
      "step": 2
    },
    {
      "epoch": 0.0010750761512273786,
      "grad_norm": 18.94686508178711,
      "learning_rate": 4.999402771141902e-05,
      "loss": 15.5972,
      "step": 3
    },
    {
      "epoch": 0.0014334348683031715,
      "grad_norm": 20.0352840423584,
      "learning_rate": 4.9988055422838034e-05,
      "loss": 15.2382,
      "step": 4
    },
    {
      "epoch": 0.0017917935853789643,
      "grad_norm": 20.746166229248047,
      "learning_rate": 4.998208313425705e-05,
      "loss": 14.7711,
      "step": 5
    },
    {
      "epoch": 0.002150152302454757,
      "grad_norm": 24.307992935180664,
      "learning_rate": 4.9976110845676066e-05,
      "loss": 14.3096,
      "step": 6
    },
    {
      "epoch": 0.00250851101953055,
      "grad_norm": 25.649250030517578,
      "learning_rate": 4.997013855709508e-05,
      "loss": 14.0469,
      "step": 7
    },
    {
      "epoch": 0.002866869736606343,
      "grad_norm": 29.355260848999023,
      "learning_rate": 4.99641662685141e-05,
      "loss": 13.1152,
      "step": 8
    },
    {
      "epoch": 0.003225228453682136,
      "grad_norm": 28.649314880371094,
      "learning_rate": 4.995819397993311e-05,
      "loss": 12.9541,
      "step": 9
    },
    {
      "epoch": 0.0035835871707579287,
      "grad_norm": 33.65465545654297,
      "learning_rate": 4.995222169135213e-05,
      "loss": 12.207,
      "step": 10
    },
    {
      "epoch": 0.003941945887833721,
      "grad_norm": 35.941829681396484,
      "learning_rate": 4.9946249402771146e-05,
      "loss": 11.6176,
      "step": 11
    },
    {
      "epoch": 0.004300304604909514,
      "grad_norm": 40.75389862060547,
      "learning_rate": 4.994027711419016e-05,
      "loss": 10.6479,
      "step": 12
    },
    {
      "epoch": 0.004658663321985307,
      "grad_norm": 44.15620803833008,
      "learning_rate": 4.993430482560917e-05,
      "loss": 9.8768,
      "step": 13
    },
    {
      "epoch": 0.0050170220390611,
      "grad_norm": 44.37617492675781,
      "learning_rate": 4.9928332537028194e-05,
      "loss": 9.2517,
      "step": 14
    },
    {
      "epoch": 0.005375380756136893,
      "grad_norm": 46.66408157348633,
      "learning_rate": 4.992236024844721e-05,
      "loss": 8.0375,
      "step": 15
    },
    {
      "epoch": 0.005733739473212686,
      "grad_norm": 50.66767501831055,
      "learning_rate": 4.9916387959866226e-05,
      "loss": 6.8855,
      "step": 16
    },
    {
      "epoch": 0.006092098190288478,
      "grad_norm": 55.534969329833984,
      "learning_rate": 4.9910415671285235e-05,
      "loss": 5.8985,
      "step": 17
    },
    {
      "epoch": 0.006450456907364272,
      "grad_norm": 57.557167053222656,
      "learning_rate": 4.990444338270425e-05,
      "loss": 4.6042,
      "step": 18
    },
    {
      "epoch": 0.006808815624440064,
      "grad_norm": 55.75872039794922,
      "learning_rate": 4.9898471094123274e-05,
      "loss": 3.8512,
      "step": 19
    },
    {
      "epoch": 0.007167174341515857,
      "grad_norm": 49.14149856567383,
      "learning_rate": 4.989249880554229e-05,
      "loss": 2.5176,
      "step": 20
    },
    {
      "epoch": 0.00752553305859165,
      "grad_norm": 30.030378341674805,
      "learning_rate": 4.98865265169613e-05,
      "loss": 1.3699,
      "step": 21
    },
    {
      "epoch": 0.007883891775667442,
      "grad_norm": 19.32173728942871,
      "learning_rate": 4.9880554228380315e-05,
      "loss": 1.1864,
      "step": 22
    },
    {
      "epoch": 0.008242250492743236,
      "grad_norm": 6.639308452606201,
      "learning_rate": 4.987458193979933e-05,
      "loss": 0.7265,
      "step": 23
    },
    {
      "epoch": 0.008600609209819029,
      "grad_norm": 5.71967887878418,
      "learning_rate": 4.9868609651218354e-05,
      "loss": 0.6254,
      "step": 24
    },
    {
      "epoch": 0.008958967926894821,
      "grad_norm": 2.0705337524414062,
      "learning_rate": 4.9862637362637363e-05,
      "loss": 0.6671,
      "step": 25
    },
    {
      "epoch": 0.009317326643970614,
      "grad_norm": 1.0532517433166504,
      "learning_rate": 4.985666507405638e-05,
      "loss": 0.5446,
      "step": 26
    },
    {
      "epoch": 0.009675685361046408,
      "grad_norm": 0.8239157795906067,
      "learning_rate": 4.9850692785475395e-05,
      "loss": 0.6769,
      "step": 27
    },
    {
      "epoch": 0.0100340440781222,
      "grad_norm": 0.5259680151939392,
      "learning_rate": 4.984472049689442e-05,
      "loss": 0.5561,
      "step": 28
    },
    {
      "epoch": 0.010392402795197993,
      "grad_norm": 0.37262845039367676,
      "learning_rate": 4.983874820831343e-05,
      "loss": 0.5671,
      "step": 29
    },
    {
      "epoch": 0.010750761512273785,
      "grad_norm": 0.3263879716396332,
      "learning_rate": 4.983277591973244e-05,
      "loss": 0.532,
      "step": 30
    },
    {
      "epoch": 0.01110912022934958,
      "grad_norm": 0.3708037734031677,
      "learning_rate": 4.982680363115146e-05,
      "loss": 0.6144,
      "step": 31
    },
    {
      "epoch": 0.011467478946425372,
      "grad_norm": 0.3715900182723999,
      "learning_rate": 4.9820831342570475e-05,
      "loss": 0.8116,
      "step": 32
    },
    {
      "epoch": 0.011825837663501164,
      "grad_norm": 0.2281980961561203,
      "learning_rate": 4.981485905398949e-05,
      "loss": 0.536,
      "step": 33
    },
    {
      "epoch": 0.012184196380576957,
      "grad_norm": 0.20869657397270203,
      "learning_rate": 4.980888676540851e-05,
      "loss": 0.5718,
      "step": 34
    },
    {
      "epoch": 0.01254255509765275,
      "grad_norm": 0.26453036069869995,
      "learning_rate": 4.980291447682752e-05,
      "loss": 0.5782,
      "step": 35
    },
    {
      "epoch": 0.012900913814728543,
      "grad_norm": 0.20718535780906677,
      "learning_rate": 4.979694218824654e-05,
      "loss": 0.5933,
      "step": 36
    },
    {
      "epoch": 0.013259272531804336,
      "grad_norm": 0.2096778005361557,
      "learning_rate": 4.979096989966555e-05,
      "loss": 0.5738,
      "step": 37
    },
    {
      "epoch": 0.013617631248880128,
      "grad_norm": 0.1907060295343399,
      "learning_rate": 4.978499761108457e-05,
      "loss": 0.5671,
      "step": 38
    },
    {
      "epoch": 0.013975989965955922,
      "grad_norm": 0.42075276374816895,
      "learning_rate": 4.977902532250359e-05,
      "loss": 0.5518,
      "step": 39
    },
    {
      "epoch": 0.014334348683031715,
      "grad_norm": 0.1869766116142273,
      "learning_rate": 4.97730530339226e-05,
      "loss": 0.6313,
      "step": 40
    },
    {
      "epoch": 0.014692707400107507,
      "grad_norm": 0.2915135324001312,
      "learning_rate": 4.976708074534161e-05,
      "loss": 0.5431,
      "step": 41
    },
    {
      "epoch": 0.0150510661171833,
      "grad_norm": 0.21010395884513855,
      "learning_rate": 4.9761108456760635e-05,
      "loss": 0.5521,
      "step": 42
    },
    {
      "epoch": 0.015409424834259094,
      "grad_norm": 0.17786701023578644,
      "learning_rate": 4.975513616817965e-05,
      "loss": 0.5294,
      "step": 43
    },
    {
      "epoch": 0.015767783551334884,
      "grad_norm": 0.19910797476768494,
      "learning_rate": 4.974916387959867e-05,
      "loss": 0.5891,
      "step": 44
    },
    {
      "epoch": 0.01612614226841068,
      "grad_norm": 0.20209500193595886,
      "learning_rate": 4.9743191591017676e-05,
      "loss": 0.5571,
      "step": 45
    },
    {
      "epoch": 0.016484500985486473,
      "grad_norm": 0.18157163262367249,
      "learning_rate": 4.973721930243669e-05,
      "loss": 0.5636,
      "step": 46
    },
    {
      "epoch": 0.016842859702562264,
      "grad_norm": 0.20442518591880798,
      "learning_rate": 4.9731247013855715e-05,
      "loss": 0.6321,
      "step": 47
    },
    {
      "epoch": 0.017201218419638058,
      "grad_norm": 0.1713770627975464,
      "learning_rate": 4.972527472527473e-05,
      "loss": 0.5126,
      "step": 48
    },
    {
      "epoch": 0.017559577136713852,
      "grad_norm": 0.17678509652614594,
      "learning_rate": 4.971930243669374e-05,
      "loss": 0.5431,
      "step": 49
    },
    {
      "epoch": 0.017917935853789643,
      "grad_norm": 0.17626811563968658,
      "learning_rate": 4.9713330148112756e-05,
      "loss": 0.4775,
      "step": 50
    },
    {
      "epoch": 0.018276294570865437,
      "grad_norm": 0.1643982082605362,
      "learning_rate": 4.970735785953177e-05,
      "loss": 0.5168,
      "step": 51
    },
    {
      "epoch": 0.018634653287941227,
      "grad_norm": 0.16538436710834503,
      "learning_rate": 4.9701385570950795e-05,
      "loss": 0.4727,
      "step": 52
    },
    {
      "epoch": 0.01899301200501702,
      "grad_norm": 0.17588351666927338,
      "learning_rate": 4.9695413282369804e-05,
      "loss": 0.5673,
      "step": 53
    },
    {
      "epoch": 0.019351370722092816,
      "grad_norm": 0.1993636041879654,
      "learning_rate": 4.968944099378882e-05,
      "loss": 0.7076,
      "step": 54
    },
    {
      "epoch": 0.019709729439168606,
      "grad_norm": 0.19924584031105042,
      "learning_rate": 4.9683468705207836e-05,
      "loss": 0.5164,
      "step": 55
    },
    {
      "epoch": 0.0200680881562444,
      "grad_norm": 0.17927846312522888,
      "learning_rate": 4.967749641662686e-05,
      "loss": 0.5028,
      "step": 56
    },
    {
      "epoch": 0.020426446873320195,
      "grad_norm": 0.18965227901935577,
      "learning_rate": 4.967152412804587e-05,
      "loss": 0.515,
      "step": 57
    },
    {
      "epoch": 0.020784805590395986,
      "grad_norm": 0.16628322005271912,
      "learning_rate": 4.9665551839464884e-05,
      "loss": 0.4669,
      "step": 58
    },
    {
      "epoch": 0.02114316430747178,
      "grad_norm": 0.1917169988155365,
      "learning_rate": 4.96595795508839e-05,
      "loss": 0.5215,
      "step": 59
    },
    {
      "epoch": 0.02150152302454757,
      "grad_norm": 0.17563919723033905,
      "learning_rate": 4.9653607262302916e-05,
      "loss": 0.5229,
      "step": 60
    },
    {
      "epoch": 0.021859881741623365,
      "grad_norm": 0.19013163447380066,
      "learning_rate": 4.964763497372193e-05,
      "loss": 0.5886,
      "step": 61
    },
    {
      "epoch": 0.02221824045869916,
      "grad_norm": 0.17237918078899384,
      "learning_rate": 4.964166268514095e-05,
      "loss": 0.5383,
      "step": 62
    },
    {
      "epoch": 0.02257659917577495,
      "grad_norm": 0.16468462347984314,
      "learning_rate": 4.9635690396559964e-05,
      "loss": 0.4271,
      "step": 63
    },
    {
      "epoch": 0.022934957892850744,
      "grad_norm": 0.1634742170572281,
      "learning_rate": 4.962971810797898e-05,
      "loss": 0.3951,
      "step": 64
    },
    {
      "epoch": 0.023293316609926538,
      "grad_norm": 0.18364879488945007,
      "learning_rate": 4.9623745819397996e-05,
      "loss": 0.495,
      "step": 65
    },
    {
      "epoch": 0.02365167532700233,
      "grad_norm": 0.19067758321762085,
      "learning_rate": 4.961777353081701e-05,
      "loss": 0.5052,
      "step": 66
    },
    {
      "epoch": 0.024010034044078123,
      "grad_norm": 0.17453205585479736,
      "learning_rate": 4.961180124223603e-05,
      "loss": 0.4463,
      "step": 67
    },
    {
      "epoch": 0.024368392761153913,
      "grad_norm": 0.18469516932964325,
      "learning_rate": 4.9605828953655044e-05,
      "loss": 0.489,
      "step": 68
    },
    {
      "epoch": 0.024726751478229708,
      "grad_norm": 0.18034368753433228,
      "learning_rate": 4.959985666507406e-05,
      "loss": 0.5132,
      "step": 69
    },
    {
      "epoch": 0.0250851101953055,
      "grad_norm": 0.20600056648254395,
      "learning_rate": 4.9593884376493076e-05,
      "loss": 0.5373,
      "step": 70
    },
    {
      "epoch": 0.025443468912381292,
      "grad_norm": 0.1936316043138504,
      "learning_rate": 4.958791208791209e-05,
      "loss": 0.5291,
      "step": 71
    },
    {
      "epoch": 0.025801827629457087,
      "grad_norm": 0.1939651519060135,
      "learning_rate": 4.958193979933111e-05,
      "loss": 0.4826,
      "step": 72
    },
    {
      "epoch": 0.02616018634653288,
      "grad_norm": 0.18426266312599182,
      "learning_rate": 4.9575967510750124e-05,
      "loss": 0.4285,
      "step": 73
    },
    {
      "epoch": 0.02651854506360867,
      "grad_norm": 0.1788136512041092,
      "learning_rate": 4.9569995222169133e-05,
      "loss": 0.3925,
      "step": 74
    },
    {
      "epoch": 0.026876903780684466,
      "grad_norm": 0.31094104051589966,
      "learning_rate": 4.9564022933588156e-05,
      "loss": 0.4312,
      "step": 75
    },
    {
      "epoch": 0.027235262497760256,
      "grad_norm": 0.2370317578315735,
      "learning_rate": 4.955805064500717e-05,
      "loss": 0.5078,
      "step": 76
    },
    {
      "epoch": 0.02759362121483605,
      "grad_norm": 0.21878604590892792,
      "learning_rate": 4.955207835642619e-05,
      "loss": 0.4294,
      "step": 77
    },
    {
      "epoch": 0.027951979931911845,
      "grad_norm": 0.19398300349712372,
      "learning_rate": 4.95461060678452e-05,
      "loss": 0.3609,
      "step": 78
    },
    {
      "epoch": 0.028310338648987635,
      "grad_norm": 0.19758161902427673,
      "learning_rate": 4.954013377926421e-05,
      "loss": 0.397,
      "step": 79
    },
    {
      "epoch": 0.02866869736606343,
      "grad_norm": 0.21422527730464935,
      "learning_rate": 4.9534161490683236e-05,
      "loss": 0.4557,
      "step": 80
    },
    {
      "epoch": 0.029027056083139224,
      "grad_norm": 0.22260454297065735,
      "learning_rate": 4.9528189202102245e-05,
      "loss": 0.4256,
      "step": 81
    },
    {
      "epoch": 0.029385414800215014,
      "grad_norm": 0.2079811841249466,
      "learning_rate": 4.952221691352126e-05,
      "loss": 0.4305,
      "step": 82
    },
    {
      "epoch": 0.02974377351729081,
      "grad_norm": 0.2266877442598343,
      "learning_rate": 4.951624462494028e-05,
      "loss": 0.478,
      "step": 83
    },
    {
      "epoch": 0.0301021322343666,
      "grad_norm": 0.2325388342142105,
      "learning_rate": 4.95102723363593e-05,
      "loss": 0.4552,
      "step": 84
    },
    {
      "epoch": 0.030460490951442393,
      "grad_norm": 0.218533456325531,
      "learning_rate": 4.950430004777831e-05,
      "loss": 0.3833,
      "step": 85
    },
    {
      "epoch": 0.030818849668518188,
      "grad_norm": 0.24167372286319733,
      "learning_rate": 4.9498327759197325e-05,
      "loss": 0.4908,
      "step": 86
    },
    {
      "epoch": 0.03117720838559398,
      "grad_norm": 0.2361288219690323,
      "learning_rate": 4.949235547061634e-05,
      "loss": 0.4302,
      "step": 87
    },
    {
      "epoch": 0.03153556710266977,
      "grad_norm": 0.24358461797237396,
      "learning_rate": 4.948638318203536e-05,
      "loss": 0.4491,
      "step": 88
    },
    {
      "epoch": 0.03189392581974557,
      "grad_norm": 0.25181522965431213,
      "learning_rate": 4.948041089345437e-05,
      "loss": 0.4773,
      "step": 89
    },
    {
      "epoch": 0.03225228453682136,
      "grad_norm": 0.275645911693573,
      "learning_rate": 4.947443860487339e-05,
      "loss": 0.4813,
      "step": 90
    },
    {
      "epoch": 0.03261064325389715,
      "grad_norm": 0.3181723654270172,
      "learning_rate": 4.9468466316292405e-05,
      "loss": 0.5817,
      "step": 91
    },
    {
      "epoch": 0.032969001970972946,
      "grad_norm": 0.28946229815483093,
      "learning_rate": 4.946249402771142e-05,
      "loss": 0.3797,
      "step": 92
    },
    {
      "epoch": 0.033327360688048736,
      "grad_norm": 0.4307362735271454,
      "learning_rate": 4.945652173913044e-05,
      "loss": 0.4234,
      "step": 93
    },
    {
      "epoch": 0.03368571940512453,
      "grad_norm": 0.28920120000839233,
      "learning_rate": 4.945054945054945e-05,
      "loss": 0.3889,
      "step": 94
    },
    {
      "epoch": 0.034044078122200325,
      "grad_norm": 0.3062282204627991,
      "learning_rate": 4.944457716196847e-05,
      "loss": 0.327,
      "step": 95
    },
    {
      "epoch": 0.034402436839276115,
      "grad_norm": 0.309128999710083,
      "learning_rate": 4.9438604873387485e-05,
      "loss": 0.3212,
      "step": 96
    },
    {
      "epoch": 0.034760795556351906,
      "grad_norm": 0.3194722533226013,
      "learning_rate": 4.94326325848065e-05,
      "loss": 0.3503,
      "step": 97
    },
    {
      "epoch": 0.035119154273427704,
      "grad_norm": 0.36419907212257385,
      "learning_rate": 4.942666029622552e-05,
      "loss": 0.4363,
      "step": 98
    },
    {
      "epoch": 0.035477512990503494,
      "grad_norm": 0.35646557807922363,
      "learning_rate": 4.942068800764453e-05,
      "loss": 0.3646,
      "step": 99
    },
    {
      "epoch": 0.035835871707579285,
      "grad_norm": 0.3518977165222168,
      "learning_rate": 4.941471571906355e-05,
      "loss": 0.3358,
      "step": 100
    },
    {
      "epoch": 0.03619423042465508,
      "grad_norm": 0.37824687361717224,
      "learning_rate": 4.9408743430482565e-05,
      "loss": 0.4602,
      "step": 101
    },
    {
      "epoch": 0.036552589141730873,
      "grad_norm": 0.33916598558425903,
      "learning_rate": 4.9402771141901574e-05,
      "loss": 0.2513,
      "step": 102
    },
    {
      "epoch": 0.036910947858806664,
      "grad_norm": 0.37959322333335876,
      "learning_rate": 4.93967988533206e-05,
      "loss": 0.3577,
      "step": 103
    },
    {
      "epoch": 0.037269306575882455,
      "grad_norm": 0.32848769426345825,
      "learning_rate": 4.939082656473961e-05,
      "loss": 0.2202,
      "step": 104
    },
    {
      "epoch": 0.03762766529295825,
      "grad_norm": 0.3451696038246155,
      "learning_rate": 4.938485427615863e-05,
      "loss": 0.4048,
      "step": 105
    },
    {
      "epoch": 0.03798602401003404,
      "grad_norm": 0.33096784353256226,
      "learning_rate": 4.937888198757764e-05,
      "loss": 0.4056,
      "step": 106
    },
    {
      "epoch": 0.038344382727109834,
      "grad_norm": 0.30412161350250244,
      "learning_rate": 4.9372909698996654e-05,
      "loss": 0.3498,
      "step": 107
    },
    {
      "epoch": 0.03870274144418563,
      "grad_norm": 0.2997516393661499,
      "learning_rate": 4.936693741041568e-05,
      "loss": 0.2483,
      "step": 108
    },
    {
      "epoch": 0.03906110016126142,
      "grad_norm": 0.2972385585308075,
      "learning_rate": 4.936096512183469e-05,
      "loss": 0.3165,
      "step": 109
    },
    {
      "epoch": 0.03941945887833721,
      "grad_norm": 0.29209890961647034,
      "learning_rate": 4.93549928332537e-05,
      "loss": 0.247,
      "step": 110
    },
    {
      "epoch": 0.03977781759541301,
      "grad_norm": 0.27570584416389465,
      "learning_rate": 4.934902054467272e-05,
      "loss": 0.3293,
      "step": 111
    },
    {
      "epoch": 0.0401361763124888,
      "grad_norm": 0.2765166759490967,
      "learning_rate": 4.9343048256091734e-05,
      "loss": 0.2333,
      "step": 112
    },
    {
      "epoch": 0.04049453502956459,
      "grad_norm": 0.26391762495040894,
      "learning_rate": 4.933707596751076e-05,
      "loss": 0.2451,
      "step": 113
    },
    {
      "epoch": 0.04085289374664039,
      "grad_norm": 0.3353630304336548,
      "learning_rate": 4.9331103678929766e-05,
      "loss": 0.512,
      "step": 114
    },
    {
      "epoch": 0.04121125246371618,
      "grad_norm": 0.26737532019615173,
      "learning_rate": 4.932513139034878e-05,
      "loss": 0.2342,
      "step": 115
    },
    {
      "epoch": 0.04156961118079197,
      "grad_norm": 0.22714956104755402,
      "learning_rate": 4.93191591017678e-05,
      "loss": 0.2209,
      "step": 116
    },
    {
      "epoch": 0.04192796989786777,
      "grad_norm": 0.23078717291355133,
      "learning_rate": 4.931318681318682e-05,
      "loss": 0.2065,
      "step": 117
    },
    {
      "epoch": 0.04228632861494356,
      "grad_norm": 0.276682585477829,
      "learning_rate": 4.930721452460583e-05,
      "loss": 0.2281,
      "step": 118
    },
    {
      "epoch": 0.04264468733201935,
      "grad_norm": 0.3047322928905487,
      "learning_rate": 4.9301242236024846e-05,
      "loss": 0.2358,
      "step": 119
    },
    {
      "epoch": 0.04300304604909514,
      "grad_norm": 0.33730417490005493,
      "learning_rate": 4.929526994744386e-05,
      "loss": 0.3056,
      "step": 120
    },
    {
      "epoch": 0.04336140476617094,
      "grad_norm": 0.2963522970676422,
      "learning_rate": 4.928929765886288e-05,
      "loss": 0.2079,
      "step": 121
    },
    {
      "epoch": 0.04371976348324673,
      "grad_norm": 0.47749483585357666,
      "learning_rate": 4.9283325370281894e-05,
      "loss": 0.3273,
      "step": 122
    },
    {
      "epoch": 0.04407812220032252,
      "grad_norm": 0.30452725291252136,
      "learning_rate": 4.927735308170091e-05,
      "loss": 0.2249,
      "step": 123
    },
    {
      "epoch": 0.04443648091739832,
      "grad_norm": 0.24357815086841583,
      "learning_rate": 4.9271380793119926e-05,
      "loss": 0.1787,
      "step": 124
    },
    {
      "epoch": 0.04479483963447411,
      "grad_norm": 0.2482309490442276,
      "learning_rate": 4.926540850453894e-05,
      "loss": 0.1985,
      "step": 125
    },
    {
      "epoch": 0.0451531983515499,
      "grad_norm": 0.278746634721756,
      "learning_rate": 4.925943621595796e-05,
      "loss": 0.215,
      "step": 126
    },
    {
      "epoch": 0.045511557068625696,
      "grad_norm": 0.29208019375801086,
      "learning_rate": 4.9253463927376974e-05,
      "loss": 0.2115,
      "step": 127
    },
    {
      "epoch": 0.04586991578570149,
      "grad_norm": 0.34888461232185364,
      "learning_rate": 4.924749163879599e-05,
      "loss": 0.2906,
      "step": 128
    },
    {
      "epoch": 0.04622827450277728,
      "grad_norm": 0.25117626786231995,
      "learning_rate": 4.9241519350215006e-05,
      "loss": 0.1571,
      "step": 129
    },
    {
      "epoch": 0.046586633219853076,
      "grad_norm": 0.27335798740386963,
      "learning_rate": 4.9235547061634015e-05,
      "loss": 0.1428,
      "step": 130
    },
    {
      "epoch": 0.046944991936928866,
      "grad_norm": 0.28515613079071045,
      "learning_rate": 4.922957477305304e-05,
      "loss": 0.2184,
      "step": 131
    },
    {
      "epoch": 0.04730335065400466,
      "grad_norm": 0.25557321310043335,
      "learning_rate": 4.9223602484472054e-05,
      "loss": 0.1955,
      "step": 132
    },
    {
      "epoch": 0.047661709371080455,
      "grad_norm": 0.18515686690807343,
      "learning_rate": 4.921763019589107e-05,
      "loss": 0.1739,
      "step": 133
    },
    {
      "epoch": 0.048020068088156245,
      "grad_norm": 0.2710229754447937,
      "learning_rate": 4.921165790731008e-05,
      "loss": 0.2893,
      "step": 134
    },
    {
      "epoch": 0.048378426805232036,
      "grad_norm": 0.2276855856180191,
      "learning_rate": 4.9205685618729095e-05,
      "loss": 0.242,
      "step": 135
    },
    {
      "epoch": 0.04873678552230783,
      "grad_norm": 0.15873204171657562,
      "learning_rate": 4.919971333014812e-05,
      "loss": 0.1818,
      "step": 136
    },
    {
      "epoch": 0.049095144239383624,
      "grad_norm": 0.2027217149734497,
      "learning_rate": 4.9193741041567134e-05,
      "loss": 0.181,
      "step": 137
    },
    {
      "epoch": 0.049453502956459415,
      "grad_norm": 0.23012855648994446,
      "learning_rate": 4.918776875298614e-05,
      "loss": 0.206,
      "step": 138
    },
    {
      "epoch": 0.049811861673535206,
      "grad_norm": 0.18605534732341766,
      "learning_rate": 4.918179646440516e-05,
      "loss": 0.1549,
      "step": 139
    },
    {
      "epoch": 0.050170220390611,
      "grad_norm": 0.24357332289218903,
      "learning_rate": 4.9175824175824175e-05,
      "loss": 0.2215,
      "step": 140
    },
    {
      "epoch": 0.050528579107686794,
      "grad_norm": 0.3944641947746277,
      "learning_rate": 4.91698518872432e-05,
      "loss": 0.2344,
      "step": 141
    },
    {
      "epoch": 0.050886937824762585,
      "grad_norm": 0.17388564348220825,
      "learning_rate": 4.916387959866221e-05,
      "loss": 0.1727,
      "step": 142
    },
    {
      "epoch": 0.05124529654183838,
      "grad_norm": 0.16706374287605286,
      "learning_rate": 4.915790731008122e-05,
      "loss": 0.1469,
      "step": 143
    },
    {
      "epoch": 0.05160365525891417,
      "grad_norm": 0.16845233738422394,
      "learning_rate": 4.915193502150024e-05,
      "loss": 0.1376,
      "step": 144
    },
    {
      "epoch": 0.051962013975989964,
      "grad_norm": 0.2852131724357605,
      "learning_rate": 4.914596273291926e-05,
      "loss": 0.3542,
      "step": 145
    },
    {
      "epoch": 0.05232037269306576,
      "grad_norm": 0.245087668299675,
      "learning_rate": 4.913999044433827e-05,
      "loss": 0.2857,
      "step": 146
    },
    {
      "epoch": 0.05267873141014155,
      "grad_norm": 0.26394104957580566,
      "learning_rate": 4.913401815575729e-05,
      "loss": 0.2417,
      "step": 147
    },
    {
      "epoch": 0.05303709012721734,
      "grad_norm": 0.25003480911254883,
      "learning_rate": 4.91280458671763e-05,
      "loss": 0.234,
      "step": 148
    },
    {
      "epoch": 0.05339544884429314,
      "grad_norm": 0.19619432091712952,
      "learning_rate": 4.912207357859532e-05,
      "loss": 0.1799,
      "step": 149
    },
    {
      "epoch": 0.05375380756136893,
      "grad_norm": 0.268304705619812,
      "learning_rate": 4.9116101290014335e-05,
      "loss": 0.2243,
      "step": 150
    },
    {
      "epoch": 0.05411216627844472,
      "grad_norm": 0.2035079300403595,
      "learning_rate": 4.911012900143335e-05,
      "loss": 0.1853,
      "step": 151
    },
    {
      "epoch": 0.05447052499552051,
      "grad_norm": 0.4195922613143921,
      "learning_rate": 4.910415671285237e-05,
      "loss": 0.2935,
      "step": 152
    },
    {
      "epoch": 0.05482888371259631,
      "grad_norm": 0.1838088184595108,
      "learning_rate": 4.909818442427138e-05,
      "loss": 0.2057,
      "step": 153
    },
    {
      "epoch": 0.0551872424296721,
      "grad_norm": 0.20268432796001434,
      "learning_rate": 4.90922121356904e-05,
      "loss": 0.2003,
      "step": 154
    },
    {
      "epoch": 0.05554560114674789,
      "grad_norm": 0.16689835488796234,
      "learning_rate": 4.9086239847109415e-05,
      "loss": 0.0988,
      "step": 155
    },
    {
      "epoch": 0.05590395986382369,
      "grad_norm": 0.17259439826011658,
      "learning_rate": 4.908026755852843e-05,
      "loss": 0.1697,
      "step": 156
    },
    {
      "epoch": 0.05626231858089948,
      "grad_norm": 0.425293505191803,
      "learning_rate": 4.907429526994745e-05,
      "loss": 0.8619,
      "step": 157
    },
    {
      "epoch": 0.05662067729797527,
      "grad_norm": 0.202299565076828,
      "learning_rate": 4.906832298136646e-05,
      "loss": 0.2007,
      "step": 158
    },
    {
      "epoch": 0.05697903601505107,
      "grad_norm": 0.2395390123128891,
      "learning_rate": 4.906235069278548e-05,
      "loss": 0.2047,
      "step": 159
    },
    {
      "epoch": 0.05733739473212686,
      "grad_norm": 0.2528805434703827,
      "learning_rate": 4.9056378404204495e-05,
      "loss": 0.2257,
      "step": 160
    },
    {
      "epoch": 0.05769575344920265,
      "grad_norm": 0.22283227741718292,
      "learning_rate": 4.905040611562351e-05,
      "loss": 0.195,
      "step": 161
    },
    {
      "epoch": 0.05805411216627845,
      "grad_norm": 0.16007082164287567,
      "learning_rate": 4.904443382704253e-05,
      "loss": 0.2034,
      "step": 162
    },
    {
      "epoch": 0.05841247088335424,
      "grad_norm": 0.20828954875469208,
      "learning_rate": 4.9038461538461536e-05,
      "loss": 0.2275,
      "step": 163
    },
    {
      "epoch": 0.05877082960043003,
      "grad_norm": 0.14837133884429932,
      "learning_rate": 4.903248924988056e-05,
      "loss": 0.1344,
      "step": 164
    },
    {
      "epoch": 0.059129188317505826,
      "grad_norm": 0.19820012152194977,
      "learning_rate": 4.9026516961299575e-05,
      "loss": 0.1575,
      "step": 165
    },
    {
      "epoch": 0.05948754703458162,
      "grad_norm": 0.15185175836086273,
      "learning_rate": 4.902054467271859e-05,
      "loss": 0.1556,
      "step": 166
    },
    {
      "epoch": 0.05984590575165741,
      "grad_norm": 0.188672736287117,
      "learning_rate": 4.90145723841376e-05,
      "loss": 0.191,
      "step": 167
    },
    {
      "epoch": 0.0602042644687332,
      "grad_norm": 0.3711465895175934,
      "learning_rate": 4.9008600095556616e-05,
      "loss": 0.2497,
      "step": 168
    },
    {
      "epoch": 0.060562623185808996,
      "grad_norm": 0.22556760907173157,
      "learning_rate": 4.900262780697564e-05,
      "loss": 0.1873,
      "step": 169
    },
    {
      "epoch": 0.06092098190288479,
      "grad_norm": 0.16881833970546722,
      "learning_rate": 4.8996655518394655e-05,
      "loss": 0.1777,
      "step": 170
    },
    {
      "epoch": 0.06127934061996058,
      "grad_norm": 0.15546242892742157,
      "learning_rate": 4.8990683229813664e-05,
      "loss": 0.163,
      "step": 171
    },
    {
      "epoch": 0.061637699337036375,
      "grad_norm": 0.1774119883775711,
      "learning_rate": 4.898471094123268e-05,
      "loss": 0.1222,
      "step": 172
    },
    {
      "epoch": 0.061996058054112166,
      "grad_norm": 0.17930057644844055,
      "learning_rate": 4.89787386526517e-05,
      "loss": 0.1517,
      "step": 173
    },
    {
      "epoch": 0.06235441677118796,
      "grad_norm": 0.1834963858127594,
      "learning_rate": 4.897276636407072e-05,
      "loss": 0.1497,
      "step": 174
    },
    {
      "epoch": 0.06271277548826375,
      "grad_norm": 0.1875227838754654,
      "learning_rate": 4.896679407548973e-05,
      "loss": 0.1545,
      "step": 175
    },
    {
      "epoch": 0.06307113420533954,
      "grad_norm": 0.1656855046749115,
      "learning_rate": 4.8960821786908744e-05,
      "loss": 0.1983,
      "step": 176
    },
    {
      "epoch": 0.06342949292241534,
      "grad_norm": 0.22786028683185577,
      "learning_rate": 4.895484949832776e-05,
      "loss": 0.1876,
      "step": 177
    },
    {
      "epoch": 0.06378785163949113,
      "grad_norm": 0.17633187770843506,
      "learning_rate": 4.894887720974678e-05,
      "loss": 0.1076,
      "step": 178
    },
    {
      "epoch": 0.06414621035656692,
      "grad_norm": 0.29575493931770325,
      "learning_rate": 4.894290492116579e-05,
      "loss": 0.3248,
      "step": 179
    },
    {
      "epoch": 0.06450456907364271,
      "grad_norm": 0.16504469513893127,
      "learning_rate": 4.893693263258481e-05,
      "loss": 0.1622,
      "step": 180
    },
    {
      "epoch": 0.0648629277907185,
      "grad_norm": 0.14046438038349152,
      "learning_rate": 4.8930960344003824e-05,
      "loss": 0.1325,
      "step": 181
    },
    {
      "epoch": 0.0652212865077943,
      "grad_norm": 0.14508800208568573,
      "learning_rate": 4.892498805542284e-05,
      "loss": 0.1274,
      "step": 182
    },
    {
      "epoch": 0.0655796452248701,
      "grad_norm": 0.2110680788755417,
      "learning_rate": 4.8919015766841856e-05,
      "loss": 0.1441,
      "step": 183
    },
    {
      "epoch": 0.06593800394194589,
      "grad_norm": 0.3298022747039795,
      "learning_rate": 4.891304347826087e-05,
      "loss": 0.1579,
      "step": 184
    },
    {
      "epoch": 0.06629636265902168,
      "grad_norm": 0.22833582758903503,
      "learning_rate": 4.890707118967989e-05,
      "loss": 0.1632,
      "step": 185
    },
    {
      "epoch": 0.06665472137609747,
      "grad_norm": 0.18038471043109894,
      "learning_rate": 4.8901098901098904e-05,
      "loss": 0.1662,
      "step": 186
    },
    {
      "epoch": 0.06701308009317326,
      "grad_norm": 0.1786368489265442,
      "learning_rate": 4.889512661251792e-05,
      "loss": 0.163,
      "step": 187
    },
    {
      "epoch": 0.06737143881024905,
      "grad_norm": 0.16630782186985016,
      "learning_rate": 4.8889154323936936e-05,
      "loss": 0.1424,
      "step": 188
    },
    {
      "epoch": 0.06772979752732486,
      "grad_norm": 0.35446611046791077,
      "learning_rate": 4.888318203535595e-05,
      "loss": 0.2509,
      "step": 189
    },
    {
      "epoch": 0.06808815624440065,
      "grad_norm": 0.2174365222454071,
      "learning_rate": 4.887720974677497e-05,
      "loss": 0.2056,
      "step": 190
    },
    {
      "epoch": 0.06844651496147644,
      "grad_norm": 0.20328527688980103,
      "learning_rate": 4.887123745819398e-05,
      "loss": 0.1799,
      "step": 191
    },
    {
      "epoch": 0.06880487367855223,
      "grad_norm": 0.264043390750885,
      "learning_rate": 4.8865265169613e-05,
      "loss": 0.2879,
      "step": 192
    },
    {
      "epoch": 0.06916323239562802,
      "grad_norm": 0.22081898152828217,
      "learning_rate": 4.8859292881032016e-05,
      "loss": 0.2481,
      "step": 193
    },
    {
      "epoch": 0.06952159111270381,
      "grad_norm": 0.15981683135032654,
      "learning_rate": 4.885332059245103e-05,
      "loss": 0.127,
      "step": 194
    },
    {
      "epoch": 0.0698799498297796,
      "grad_norm": 0.17570604383945465,
      "learning_rate": 4.884734830387004e-05,
      "loss": 0.1021,
      "step": 195
    },
    {
      "epoch": 0.07023830854685541,
      "grad_norm": 0.17946885526180267,
      "learning_rate": 4.884137601528906e-05,
      "loss": 0.174,
      "step": 196
    },
    {
      "epoch": 0.0705966672639312,
      "grad_norm": 0.16112278401851654,
      "learning_rate": 4.883540372670808e-05,
      "loss": 0.1913,
      "step": 197
    },
    {
      "epoch": 0.07095502598100699,
      "grad_norm": 0.2605041563510895,
      "learning_rate": 4.8829431438127096e-05,
      "loss": 0.2226,
      "step": 198
    },
    {
      "epoch": 0.07131338469808278,
      "grad_norm": 0.19358345866203308,
      "learning_rate": 4.8823459149546105e-05,
      "loss": 0.1755,
      "step": 199
    },
    {
      "epoch": 0.07167174341515857,
      "grad_norm": 0.17513500154018402,
      "learning_rate": 4.881748686096512e-05,
      "loss": 0.1092,
      "step": 200
    },
    {
      "epoch": 0.07203010213223436,
      "grad_norm": 0.28034499287605286,
      "learning_rate": 4.8811514572384144e-05,
      "loss": 0.1965,
      "step": 201
    },
    {
      "epoch": 0.07238846084931017,
      "grad_norm": 0.20823431015014648,
      "learning_rate": 4.880554228380316e-05,
      "loss": 0.2224,
      "step": 202
    },
    {
      "epoch": 0.07274681956638596,
      "grad_norm": 0.20210617780685425,
      "learning_rate": 4.879956999522217e-05,
      "loss": 0.2133,
      "step": 203
    },
    {
      "epoch": 0.07310517828346175,
      "grad_norm": 0.1569713056087494,
      "learning_rate": 4.8793597706641185e-05,
      "loss": 0.1345,
      "step": 204
    },
    {
      "epoch": 0.07346353700053754,
      "grad_norm": 0.20693735778331757,
      "learning_rate": 4.87876254180602e-05,
      "loss": 0.2205,
      "step": 205
    },
    {
      "epoch": 0.07382189571761333,
      "grad_norm": 0.2902527153491974,
      "learning_rate": 4.8781653129479224e-05,
      "loss": 0.1108,
      "step": 206
    },
    {
      "epoch": 0.07418025443468912,
      "grad_norm": 0.24802440404891968,
      "learning_rate": 4.877568084089823e-05,
      "loss": 0.1391,
      "step": 207
    },
    {
      "epoch": 0.07453861315176491,
      "grad_norm": 0.17619319260120392,
      "learning_rate": 4.876970855231725e-05,
      "loss": 0.1344,
      "step": 208
    },
    {
      "epoch": 0.07489697186884071,
      "grad_norm": 0.1938244104385376,
      "learning_rate": 4.8763736263736265e-05,
      "loss": 0.1763,
      "step": 209
    },
    {
      "epoch": 0.0752553305859165,
      "grad_norm": 0.3311384618282318,
      "learning_rate": 4.875776397515528e-05,
      "loss": 0.3614,
      "step": 210
    },
    {
      "epoch": 0.0756136893029923,
      "grad_norm": 0.2320941835641861,
      "learning_rate": 4.87517916865743e-05,
      "loss": 0.203,
      "step": 211
    },
    {
      "epoch": 0.07597204802006809,
      "grad_norm": 0.16116313636302948,
      "learning_rate": 4.874581939799331e-05,
      "loss": 0.1668,
      "step": 212
    },
    {
      "epoch": 0.07633040673714388,
      "grad_norm": 0.2070159614086151,
      "learning_rate": 4.873984710941233e-05,
      "loss": 0.2494,
      "step": 213
    },
    {
      "epoch": 0.07668876545421967,
      "grad_norm": 0.1838577836751938,
      "learning_rate": 4.8733874820831345e-05,
      "loss": 0.1427,
      "step": 214
    },
    {
      "epoch": 0.07704712417129547,
      "grad_norm": 0.18143275380134583,
      "learning_rate": 4.872790253225036e-05,
      "loss": 0.1571,
      "step": 215
    },
    {
      "epoch": 0.07740548288837126,
      "grad_norm": 0.16583941876888275,
      "learning_rate": 4.872193024366938e-05,
      "loss": 0.1505,
      "step": 216
    },
    {
      "epoch": 0.07776384160544705,
      "grad_norm": 0.3242007791996002,
      "learning_rate": 4.871595795508839e-05,
      "loss": 0.2162,
      "step": 217
    },
    {
      "epoch": 0.07812220032252284,
      "grad_norm": 0.1706347018480301,
      "learning_rate": 4.870998566650741e-05,
      "loss": 0.1433,
      "step": 218
    },
    {
      "epoch": 0.07848055903959864,
      "grad_norm": 0.20231230556964874,
      "learning_rate": 4.8704013377926425e-05,
      "loss": 0.2143,
      "step": 219
    },
    {
      "epoch": 0.07883891775667443,
      "grad_norm": 0.15817378461360931,
      "learning_rate": 4.869804108934544e-05,
      "loss": 0.1418,
      "step": 220
    },
    {
      "epoch": 0.07919727647375023,
      "grad_norm": 0.21155042946338654,
      "learning_rate": 4.869206880076446e-05,
      "loss": 0.2045,
      "step": 221
    },
    {
      "epoch": 0.07955563519082602,
      "grad_norm": 0.2261083722114563,
      "learning_rate": 4.868609651218347e-05,
      "loss": 0.1401,
      "step": 222
    },
    {
      "epoch": 0.07991399390790181,
      "grad_norm": 0.2058093547821045,
      "learning_rate": 4.868012422360249e-05,
      "loss": 0.1936,
      "step": 223
    },
    {
      "epoch": 0.0802723526249776,
      "grad_norm": 0.17346060276031494,
      "learning_rate": 4.86741519350215e-05,
      "loss": 0.1408,
      "step": 224
    },
    {
      "epoch": 0.0806307113420534,
      "grad_norm": 0.17506147921085358,
      "learning_rate": 4.866817964644052e-05,
      "loss": 0.1448,
      "step": 225
    },
    {
      "epoch": 0.08098907005912918,
      "grad_norm": 0.20292793214321136,
      "learning_rate": 4.866220735785954e-05,
      "loss": 0.2196,
      "step": 226
    },
    {
      "epoch": 0.08134742877620497,
      "grad_norm": 0.20290492475032806,
      "learning_rate": 4.865623506927855e-05,
      "loss": 0.1728,
      "step": 227
    },
    {
      "epoch": 0.08170578749328078,
      "grad_norm": 0.16371123492717743,
      "learning_rate": 4.865026278069756e-05,
      "loss": 0.1339,
      "step": 228
    },
    {
      "epoch": 0.08206414621035657,
      "grad_norm": 0.3035982549190521,
      "learning_rate": 4.864429049211658e-05,
      "loss": 0.197,
      "step": 229
    },
    {
      "epoch": 0.08242250492743236,
      "grad_norm": 0.2649548649787903,
      "learning_rate": 4.86383182035356e-05,
      "loss": 0.2061,
      "step": 230
    },
    {
      "epoch": 0.08278086364450815,
      "grad_norm": 0.18573452532291412,
      "learning_rate": 4.863234591495461e-05,
      "loss": 0.1768,
      "step": 231
    },
    {
      "epoch": 0.08313922236158394,
      "grad_norm": 0.2315385937690735,
      "learning_rate": 4.8626373626373626e-05,
      "loss": 0.1966,
      "step": 232
    },
    {
      "epoch": 0.08349758107865973,
      "grad_norm": 0.18457846343517303,
      "learning_rate": 4.862040133779264e-05,
      "loss": 0.183,
      "step": 233
    },
    {
      "epoch": 0.08385593979573554,
      "grad_norm": 0.21711626648902893,
      "learning_rate": 4.8614429049211665e-05,
      "loss": 0.2104,
      "step": 234
    },
    {
      "epoch": 0.08421429851281133,
      "grad_norm": 0.34826910495758057,
      "learning_rate": 4.8608456760630674e-05,
      "loss": 0.4612,
      "step": 235
    },
    {
      "epoch": 0.08457265722988712,
      "grad_norm": 0.19323702156543732,
      "learning_rate": 4.860248447204969e-05,
      "loss": 0.1248,
      "step": 236
    },
    {
      "epoch": 0.08493101594696291,
      "grad_norm": 0.2868496775627136,
      "learning_rate": 4.8596512183468706e-05,
      "loss": 0.2578,
      "step": 237
    },
    {
      "epoch": 0.0852893746640387,
      "grad_norm": 0.25920167565345764,
      "learning_rate": 4.859053989488772e-05,
      "loss": 0.1426,
      "step": 238
    },
    {
      "epoch": 0.08564773338111449,
      "grad_norm": 0.2116648554801941,
      "learning_rate": 4.858456760630674e-05,
      "loss": 0.2168,
      "step": 239
    },
    {
      "epoch": 0.08600609209819028,
      "grad_norm": 0.23402974009513855,
      "learning_rate": 4.8578595317725754e-05,
      "loss": 0.2948,
      "step": 240
    },
    {
      "epoch": 0.08636445081526609,
      "grad_norm": 0.19198866188526154,
      "learning_rate": 4.857262302914477e-05,
      "loss": 0.1261,
      "step": 241
    },
    {
      "epoch": 0.08672280953234188,
      "grad_norm": 0.21391993761062622,
      "learning_rate": 4.8566650740563786e-05,
      "loss": 0.1751,
      "step": 242
    },
    {
      "epoch": 0.08708116824941767,
      "grad_norm": 0.20832787454128265,
      "learning_rate": 4.85606784519828e-05,
      "loss": 0.1741,
      "step": 243
    },
    {
      "epoch": 0.08743952696649346,
      "grad_norm": 0.19579313695430756,
      "learning_rate": 4.855470616340182e-05,
      "loss": 0.1336,
      "step": 244
    },
    {
      "epoch": 0.08779788568356925,
      "grad_norm": 0.23382562398910522,
      "learning_rate": 4.8548733874820834e-05,
      "loss": 0.2087,
      "step": 245
    },
    {
      "epoch": 0.08815624440064504,
      "grad_norm": 0.2735370993614197,
      "learning_rate": 4.854276158623985e-05,
      "loss": 0.1453,
      "step": 246
    },
    {
      "epoch": 0.08851460311772084,
      "grad_norm": 0.18614327907562256,
      "learning_rate": 4.8536789297658866e-05,
      "loss": 0.205,
      "step": 247
    },
    {
      "epoch": 0.08887296183479663,
      "grad_norm": 0.2037499099969864,
      "learning_rate": 4.853081700907788e-05,
      "loss": 0.1712,
      "step": 248
    },
    {
      "epoch": 0.08923132055187243,
      "grad_norm": 0.19306589663028717,
      "learning_rate": 4.85248447204969e-05,
      "loss": 0.1752,
      "step": 249
    },
    {
      "epoch": 0.08958967926894822,
      "grad_norm": 0.17412534356117249,
      "learning_rate": 4.8518872431915914e-05,
      "loss": 0.1176,
      "step": 250
    },
    {
      "epoch": 0.08994803798602401,
      "grad_norm": 0.14464156329631805,
      "learning_rate": 4.851290014333493e-05,
      "loss": 0.1402,
      "step": 251
    },
    {
      "epoch": 0.0903063967030998,
      "grad_norm": 0.15614837408065796,
      "learning_rate": 4.850692785475394e-05,
      "loss": 0.1253,
      "step": 252
    },
    {
      "epoch": 0.0906647554201756,
      "grad_norm": 0.17321301996707916,
      "learning_rate": 4.850095556617296e-05,
      "loss": 0.1661,
      "step": 253
    },
    {
      "epoch": 0.09102311413725139,
      "grad_norm": 0.2313585728406906,
      "learning_rate": 4.849498327759198e-05,
      "loss": 0.2048,
      "step": 254
    },
    {
      "epoch": 0.09138147285432718,
      "grad_norm": 0.2553004026412964,
      "learning_rate": 4.8489010989010994e-05,
      "loss": 0.2078,
      "step": 255
    },
    {
      "epoch": 0.09173983157140297,
      "grad_norm": 0.20027665793895721,
      "learning_rate": 4.848303870043e-05,
      "loss": 0.1978,
      "step": 256
    },
    {
      "epoch": 0.09209819028847877,
      "grad_norm": 0.2089284062385559,
      "learning_rate": 4.847706641184902e-05,
      "loss": 0.1701,
      "step": 257
    },
    {
      "epoch": 0.09245654900555456,
      "grad_norm": 0.1998046636581421,
      "learning_rate": 4.847109412326804e-05,
      "loss": 0.1732,
      "step": 258
    },
    {
      "epoch": 0.09281490772263035,
      "grad_norm": 0.16686256229877472,
      "learning_rate": 4.846512183468706e-05,
      "loss": 0.161,
      "step": 259
    },
    {
      "epoch": 0.09317326643970615,
      "grad_norm": 0.19310146570205688,
      "learning_rate": 4.845914954610607e-05,
      "loss": 0.1586,
      "step": 260
    },
    {
      "epoch": 0.09353162515678194,
      "grad_norm": 0.28791970014572144,
      "learning_rate": 4.845317725752508e-05,
      "loss": 0.1772,
      "step": 261
    },
    {
      "epoch": 0.09388998387385773,
      "grad_norm": 0.23530277609825134,
      "learning_rate": 4.8447204968944106e-05,
      "loss": 0.192,
      "step": 262
    },
    {
      "epoch": 0.09424834259093352,
      "grad_norm": 0.19018632173538208,
      "learning_rate": 4.844123268036312e-05,
      "loss": 0.1543,
      "step": 263
    },
    {
      "epoch": 0.09460670130800931,
      "grad_norm": 0.1882745772600174,
      "learning_rate": 4.843526039178213e-05,
      "loss": 0.144,
      "step": 264
    },
    {
      "epoch": 0.0949650600250851,
      "grad_norm": 0.21120695769786835,
      "learning_rate": 4.842928810320115e-05,
      "loss": 0.1976,
      "step": 265
    },
    {
      "epoch": 0.09532341874216091,
      "grad_norm": 0.30511561036109924,
      "learning_rate": 4.842331581462016e-05,
      "loss": 0.4284,
      "step": 266
    },
    {
      "epoch": 0.0956817774592367,
      "grad_norm": 0.2265871912240982,
      "learning_rate": 4.8417343526039186e-05,
      "loss": 0.1448,
      "step": 267
    },
    {
      "epoch": 0.09604013617631249,
      "grad_norm": 0.1520865559577942,
      "learning_rate": 4.8411371237458195e-05,
      "loss": 0.143,
      "step": 268
    },
    {
      "epoch": 0.09639849489338828,
      "grad_norm": 0.2078649401664734,
      "learning_rate": 4.840539894887721e-05,
      "loss": 0.1769,
      "step": 269
    },
    {
      "epoch": 0.09675685361046407,
      "grad_norm": 0.19680829346179962,
      "learning_rate": 4.839942666029623e-05,
      "loss": 0.1909,
      "step": 270
    },
    {
      "epoch": 0.09711521232753986,
      "grad_norm": 0.3116722106933594,
      "learning_rate": 4.839345437171524e-05,
      "loss": 0.1664,
      "step": 271
    },
    {
      "epoch": 0.09747357104461565,
      "grad_norm": 0.22728586196899414,
      "learning_rate": 4.838748208313426e-05,
      "loss": 0.2675,
      "step": 272
    },
    {
      "epoch": 0.09783192976169146,
      "grad_norm": 0.18516074120998383,
      "learning_rate": 4.8381509794553275e-05,
      "loss": 0.183,
      "step": 273
    },
    {
      "epoch": 0.09819028847876725,
      "grad_norm": 0.1576915681362152,
      "learning_rate": 4.837553750597229e-05,
      "loss": 0.1276,
      "step": 274
    },
    {
      "epoch": 0.09854864719584304,
      "grad_norm": 0.2539936602115631,
      "learning_rate": 4.836956521739131e-05,
      "loss": 0.3053,
      "step": 275
    },
    {
      "epoch": 0.09890700591291883,
      "grad_norm": 0.259624719619751,
      "learning_rate": 4.836359292881032e-05,
      "loss": 0.3226,
      "step": 276
    },
    {
      "epoch": 0.09926536462999462,
      "grad_norm": 0.39196422696113586,
      "learning_rate": 4.835762064022934e-05,
      "loss": 0.4496,
      "step": 277
    },
    {
      "epoch": 0.09962372334707041,
      "grad_norm": 0.18025647103786469,
      "learning_rate": 4.8351648351648355e-05,
      "loss": 0.1605,
      "step": 278
    },
    {
      "epoch": 0.09998208206414622,
      "grad_norm": 0.2623847723007202,
      "learning_rate": 4.834567606306737e-05,
      "loss": 0.23,
      "step": 279
    },
    {
      "epoch": 0.100340440781222,
      "grad_norm": 0.19100594520568848,
      "learning_rate": 4.833970377448638e-05,
      "loss": 0.1692,
      "step": 280
    },
    {
      "epoch": 0.1006987994982978,
      "grad_norm": 0.3554922342300415,
      "learning_rate": 4.83337314859054e-05,
      "loss": 0.188,
      "step": 281
    },
    {
      "epoch": 0.10105715821537359,
      "grad_norm": 0.20372998714447021,
      "learning_rate": 4.832775919732442e-05,
      "loss": 0.2066,
      "step": 282
    },
    {
      "epoch": 0.10141551693244938,
      "grad_norm": 0.17304092645645142,
      "learning_rate": 4.8321786908743435e-05,
      "loss": 0.11,
      "step": 283
    },
    {
      "epoch": 0.10177387564952517,
      "grad_norm": 0.19256143271923065,
      "learning_rate": 4.8315814620162444e-05,
      "loss": 0.1998,
      "step": 284
    },
    {
      "epoch": 0.10213223436660097,
      "grad_norm": 0.18189814686775208,
      "learning_rate": 4.830984233158146e-05,
      "loss": 0.1903,
      "step": 285
    },
    {
      "epoch": 0.10249059308367676,
      "grad_norm": 0.2188909649848938,
      "learning_rate": 4.830387004300048e-05,
      "loss": 0.174,
      "step": 286
    },
    {
      "epoch": 0.10284895180075256,
      "grad_norm": 0.274290531873703,
      "learning_rate": 4.82978977544195e-05,
      "loss": 0.2737,
      "step": 287
    },
    {
      "epoch": 0.10320731051782835,
      "grad_norm": 0.18820689618587494,
      "learning_rate": 4.829192546583851e-05,
      "loss": 0.2183,
      "step": 288
    },
    {
      "epoch": 0.10356566923490414,
      "grad_norm": 0.23179493844509125,
      "learning_rate": 4.8285953177257524e-05,
      "loss": 0.2787,
      "step": 289
    },
    {
      "epoch": 0.10392402795197993,
      "grad_norm": 0.1640051007270813,
      "learning_rate": 4.827998088867655e-05,
      "loss": 0.0916,
      "step": 290
    }
  ],
  "logging_steps": 1,
  "max_steps": 8373,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.005283656597504e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
