{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.18276294570865437,
  "eval_steps": 500,
  "global_step": 510,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00035835871707579287,
      "grad_norm": 20.3043155670166,
      "learning_rate": 0.0,
      "loss": 15.7337,
      "step": 1
    },
    {
      "epoch": 0.0007167174341515857,
      "grad_norm": 18.318492889404297,
      "learning_rate": 5e-05,
      "loss": 15.8805,
      "step": 2
    },
    {
      "epoch": 0.0010750761512273786,
      "grad_norm": 18.94686508178711,
      "learning_rate": 4.999402771141902e-05,
      "loss": 15.5972,
      "step": 3
    },
    {
      "epoch": 0.0014334348683031715,
      "grad_norm": 20.0352840423584,
      "learning_rate": 4.9988055422838034e-05,
      "loss": 15.2382,
      "step": 4
    },
    {
      "epoch": 0.0017917935853789643,
      "grad_norm": 20.746166229248047,
      "learning_rate": 4.998208313425705e-05,
      "loss": 14.7711,
      "step": 5
    },
    {
      "epoch": 0.002150152302454757,
      "grad_norm": 24.307992935180664,
      "learning_rate": 4.9976110845676066e-05,
      "loss": 14.3096,
      "step": 6
    },
    {
      "epoch": 0.00250851101953055,
      "grad_norm": 25.649250030517578,
      "learning_rate": 4.997013855709508e-05,
      "loss": 14.0469,
      "step": 7
    },
    {
      "epoch": 0.002866869736606343,
      "grad_norm": 29.355260848999023,
      "learning_rate": 4.99641662685141e-05,
      "loss": 13.1152,
      "step": 8
    },
    {
      "epoch": 0.003225228453682136,
      "grad_norm": 28.649314880371094,
      "learning_rate": 4.995819397993311e-05,
      "loss": 12.9541,
      "step": 9
    },
    {
      "epoch": 0.0035835871707579287,
      "grad_norm": 33.65465545654297,
      "learning_rate": 4.995222169135213e-05,
      "loss": 12.207,
      "step": 10
    },
    {
      "epoch": 0.003941945887833721,
      "grad_norm": 35.941829681396484,
      "learning_rate": 4.9946249402771146e-05,
      "loss": 11.6176,
      "step": 11
    },
    {
      "epoch": 0.004300304604909514,
      "grad_norm": 40.75389862060547,
      "learning_rate": 4.994027711419016e-05,
      "loss": 10.6479,
      "step": 12
    },
    {
      "epoch": 0.004658663321985307,
      "grad_norm": 44.15620803833008,
      "learning_rate": 4.993430482560917e-05,
      "loss": 9.8768,
      "step": 13
    },
    {
      "epoch": 0.0050170220390611,
      "grad_norm": 44.37617492675781,
      "learning_rate": 4.9928332537028194e-05,
      "loss": 9.2517,
      "step": 14
    },
    {
      "epoch": 0.005375380756136893,
      "grad_norm": 46.66408157348633,
      "learning_rate": 4.992236024844721e-05,
      "loss": 8.0375,
      "step": 15
    },
    {
      "epoch": 0.005733739473212686,
      "grad_norm": 50.66767501831055,
      "learning_rate": 4.9916387959866226e-05,
      "loss": 6.8855,
      "step": 16
    },
    {
      "epoch": 0.006092098190288478,
      "grad_norm": 55.534969329833984,
      "learning_rate": 4.9910415671285235e-05,
      "loss": 5.8985,
      "step": 17
    },
    {
      "epoch": 0.006450456907364272,
      "grad_norm": 57.557167053222656,
      "learning_rate": 4.990444338270425e-05,
      "loss": 4.6042,
      "step": 18
    },
    {
      "epoch": 0.006808815624440064,
      "grad_norm": 55.75872039794922,
      "learning_rate": 4.9898471094123274e-05,
      "loss": 3.8512,
      "step": 19
    },
    {
      "epoch": 0.007167174341515857,
      "grad_norm": 49.14149856567383,
      "learning_rate": 4.989249880554229e-05,
      "loss": 2.5176,
      "step": 20
    },
    {
      "epoch": 0.00752553305859165,
      "grad_norm": 30.030378341674805,
      "learning_rate": 4.98865265169613e-05,
      "loss": 1.3699,
      "step": 21
    },
    {
      "epoch": 0.007883891775667442,
      "grad_norm": 19.32173728942871,
      "learning_rate": 4.9880554228380315e-05,
      "loss": 1.1864,
      "step": 22
    },
    {
      "epoch": 0.008242250492743236,
      "grad_norm": 6.639308452606201,
      "learning_rate": 4.987458193979933e-05,
      "loss": 0.7265,
      "step": 23
    },
    {
      "epoch": 0.008600609209819029,
      "grad_norm": 5.71967887878418,
      "learning_rate": 4.9868609651218354e-05,
      "loss": 0.6254,
      "step": 24
    },
    {
      "epoch": 0.008958967926894821,
      "grad_norm": 2.0705337524414062,
      "learning_rate": 4.9862637362637363e-05,
      "loss": 0.6671,
      "step": 25
    },
    {
      "epoch": 0.009317326643970614,
      "grad_norm": 1.0532517433166504,
      "learning_rate": 4.985666507405638e-05,
      "loss": 0.5446,
      "step": 26
    },
    {
      "epoch": 0.009675685361046408,
      "grad_norm": 0.8239157795906067,
      "learning_rate": 4.9850692785475395e-05,
      "loss": 0.6769,
      "step": 27
    },
    {
      "epoch": 0.0100340440781222,
      "grad_norm": 0.5259680151939392,
      "learning_rate": 4.984472049689442e-05,
      "loss": 0.5561,
      "step": 28
    },
    {
      "epoch": 0.010392402795197993,
      "grad_norm": 0.37262845039367676,
      "learning_rate": 4.983874820831343e-05,
      "loss": 0.5671,
      "step": 29
    },
    {
      "epoch": 0.010750761512273785,
      "grad_norm": 0.3263879716396332,
      "learning_rate": 4.983277591973244e-05,
      "loss": 0.532,
      "step": 30
    },
    {
      "epoch": 0.01110912022934958,
      "grad_norm": 0.3708037734031677,
      "learning_rate": 4.982680363115146e-05,
      "loss": 0.6144,
      "step": 31
    },
    {
      "epoch": 0.011467478946425372,
      "grad_norm": 0.3715900182723999,
      "learning_rate": 4.9820831342570475e-05,
      "loss": 0.8116,
      "step": 32
    },
    {
      "epoch": 0.011825837663501164,
      "grad_norm": 0.2281980961561203,
      "learning_rate": 4.981485905398949e-05,
      "loss": 0.536,
      "step": 33
    },
    {
      "epoch": 0.012184196380576957,
      "grad_norm": 0.20869657397270203,
      "learning_rate": 4.980888676540851e-05,
      "loss": 0.5718,
      "step": 34
    },
    {
      "epoch": 0.01254255509765275,
      "grad_norm": 0.26453036069869995,
      "learning_rate": 4.980291447682752e-05,
      "loss": 0.5782,
      "step": 35
    },
    {
      "epoch": 0.012900913814728543,
      "grad_norm": 0.20718535780906677,
      "learning_rate": 4.979694218824654e-05,
      "loss": 0.5933,
      "step": 36
    },
    {
      "epoch": 0.013259272531804336,
      "grad_norm": 0.2096778005361557,
      "learning_rate": 4.979096989966555e-05,
      "loss": 0.5738,
      "step": 37
    },
    {
      "epoch": 0.013617631248880128,
      "grad_norm": 0.1907060295343399,
      "learning_rate": 4.978499761108457e-05,
      "loss": 0.5671,
      "step": 38
    },
    {
      "epoch": 0.013975989965955922,
      "grad_norm": 0.42075276374816895,
      "learning_rate": 4.977902532250359e-05,
      "loss": 0.5518,
      "step": 39
    },
    {
      "epoch": 0.014334348683031715,
      "grad_norm": 0.1869766116142273,
      "learning_rate": 4.97730530339226e-05,
      "loss": 0.6313,
      "step": 40
    },
    {
      "epoch": 0.014692707400107507,
      "grad_norm": 0.2915135324001312,
      "learning_rate": 4.976708074534161e-05,
      "loss": 0.5431,
      "step": 41
    },
    {
      "epoch": 0.0150510661171833,
      "grad_norm": 0.21010395884513855,
      "learning_rate": 4.9761108456760635e-05,
      "loss": 0.5521,
      "step": 42
    },
    {
      "epoch": 0.015409424834259094,
      "grad_norm": 0.17786701023578644,
      "learning_rate": 4.975513616817965e-05,
      "loss": 0.5294,
      "step": 43
    },
    {
      "epoch": 0.015767783551334884,
      "grad_norm": 0.19910797476768494,
      "learning_rate": 4.974916387959867e-05,
      "loss": 0.5891,
      "step": 44
    },
    {
      "epoch": 0.01612614226841068,
      "grad_norm": 0.20209500193595886,
      "learning_rate": 4.9743191591017676e-05,
      "loss": 0.5571,
      "step": 45
    },
    {
      "epoch": 0.016484500985486473,
      "grad_norm": 0.18157163262367249,
      "learning_rate": 4.973721930243669e-05,
      "loss": 0.5636,
      "step": 46
    },
    {
      "epoch": 0.016842859702562264,
      "grad_norm": 0.20442518591880798,
      "learning_rate": 4.9731247013855715e-05,
      "loss": 0.6321,
      "step": 47
    },
    {
      "epoch": 0.017201218419638058,
      "grad_norm": 0.1713770627975464,
      "learning_rate": 4.972527472527473e-05,
      "loss": 0.5126,
      "step": 48
    },
    {
      "epoch": 0.017559577136713852,
      "grad_norm": 0.17678509652614594,
      "learning_rate": 4.971930243669374e-05,
      "loss": 0.5431,
      "step": 49
    },
    {
      "epoch": 0.017917935853789643,
      "grad_norm": 0.17626811563968658,
      "learning_rate": 4.9713330148112756e-05,
      "loss": 0.4775,
      "step": 50
    },
    {
      "epoch": 0.018276294570865437,
      "grad_norm": 0.1643982082605362,
      "learning_rate": 4.970735785953177e-05,
      "loss": 0.5168,
      "step": 51
    },
    {
      "epoch": 0.018634653287941227,
      "grad_norm": 0.16538436710834503,
      "learning_rate": 4.9701385570950795e-05,
      "loss": 0.4727,
      "step": 52
    },
    {
      "epoch": 0.01899301200501702,
      "grad_norm": 0.17588351666927338,
      "learning_rate": 4.9695413282369804e-05,
      "loss": 0.5673,
      "step": 53
    },
    {
      "epoch": 0.019351370722092816,
      "grad_norm": 0.1993636041879654,
      "learning_rate": 4.968944099378882e-05,
      "loss": 0.7076,
      "step": 54
    },
    {
      "epoch": 0.019709729439168606,
      "grad_norm": 0.19924584031105042,
      "learning_rate": 4.9683468705207836e-05,
      "loss": 0.5164,
      "step": 55
    },
    {
      "epoch": 0.0200680881562444,
      "grad_norm": 0.17927846312522888,
      "learning_rate": 4.967749641662686e-05,
      "loss": 0.5028,
      "step": 56
    },
    {
      "epoch": 0.020426446873320195,
      "grad_norm": 0.18965227901935577,
      "learning_rate": 4.967152412804587e-05,
      "loss": 0.515,
      "step": 57
    },
    {
      "epoch": 0.020784805590395986,
      "grad_norm": 0.16628322005271912,
      "learning_rate": 4.9665551839464884e-05,
      "loss": 0.4669,
      "step": 58
    },
    {
      "epoch": 0.02114316430747178,
      "grad_norm": 0.1917169988155365,
      "learning_rate": 4.96595795508839e-05,
      "loss": 0.5215,
      "step": 59
    },
    {
      "epoch": 0.02150152302454757,
      "grad_norm": 0.17563919723033905,
      "learning_rate": 4.9653607262302916e-05,
      "loss": 0.5229,
      "step": 60
    },
    {
      "epoch": 0.021859881741623365,
      "grad_norm": 0.19013163447380066,
      "learning_rate": 4.964763497372193e-05,
      "loss": 0.5886,
      "step": 61
    },
    {
      "epoch": 0.02221824045869916,
      "grad_norm": 0.17237918078899384,
      "learning_rate": 4.964166268514095e-05,
      "loss": 0.5383,
      "step": 62
    },
    {
      "epoch": 0.02257659917577495,
      "grad_norm": 0.16468462347984314,
      "learning_rate": 4.9635690396559964e-05,
      "loss": 0.4271,
      "step": 63
    },
    {
      "epoch": 0.022934957892850744,
      "grad_norm": 0.1634742170572281,
      "learning_rate": 4.962971810797898e-05,
      "loss": 0.3951,
      "step": 64
    },
    {
      "epoch": 0.023293316609926538,
      "grad_norm": 0.18364879488945007,
      "learning_rate": 4.9623745819397996e-05,
      "loss": 0.495,
      "step": 65
    },
    {
      "epoch": 0.02365167532700233,
      "grad_norm": 0.19067758321762085,
      "learning_rate": 4.961777353081701e-05,
      "loss": 0.5052,
      "step": 66
    },
    {
      "epoch": 0.024010034044078123,
      "grad_norm": 0.17453205585479736,
      "learning_rate": 4.961180124223603e-05,
      "loss": 0.4463,
      "step": 67
    },
    {
      "epoch": 0.024368392761153913,
      "grad_norm": 0.18469516932964325,
      "learning_rate": 4.9605828953655044e-05,
      "loss": 0.489,
      "step": 68
    },
    {
      "epoch": 0.024726751478229708,
      "grad_norm": 0.18034368753433228,
      "learning_rate": 4.959985666507406e-05,
      "loss": 0.5132,
      "step": 69
    },
    {
      "epoch": 0.0250851101953055,
      "grad_norm": 0.20600056648254395,
      "learning_rate": 4.9593884376493076e-05,
      "loss": 0.5373,
      "step": 70
    },
    {
      "epoch": 0.025443468912381292,
      "grad_norm": 0.1936316043138504,
      "learning_rate": 4.958791208791209e-05,
      "loss": 0.5291,
      "step": 71
    },
    {
      "epoch": 0.025801827629457087,
      "grad_norm": 0.1939651519060135,
      "learning_rate": 4.958193979933111e-05,
      "loss": 0.4826,
      "step": 72
    },
    {
      "epoch": 0.02616018634653288,
      "grad_norm": 0.18426266312599182,
      "learning_rate": 4.9575967510750124e-05,
      "loss": 0.4285,
      "step": 73
    },
    {
      "epoch": 0.02651854506360867,
      "grad_norm": 0.1788136512041092,
      "learning_rate": 4.9569995222169133e-05,
      "loss": 0.3925,
      "step": 74
    },
    {
      "epoch": 0.026876903780684466,
      "grad_norm": 0.31094104051589966,
      "learning_rate": 4.9564022933588156e-05,
      "loss": 0.4312,
      "step": 75
    },
    {
      "epoch": 0.027235262497760256,
      "grad_norm": 0.2370317578315735,
      "learning_rate": 4.955805064500717e-05,
      "loss": 0.5078,
      "step": 76
    },
    {
      "epoch": 0.02759362121483605,
      "grad_norm": 0.21878604590892792,
      "learning_rate": 4.955207835642619e-05,
      "loss": 0.4294,
      "step": 77
    },
    {
      "epoch": 0.027951979931911845,
      "grad_norm": 0.19398300349712372,
      "learning_rate": 4.95461060678452e-05,
      "loss": 0.3609,
      "step": 78
    },
    {
      "epoch": 0.028310338648987635,
      "grad_norm": 0.19758161902427673,
      "learning_rate": 4.954013377926421e-05,
      "loss": 0.397,
      "step": 79
    },
    {
      "epoch": 0.02866869736606343,
      "grad_norm": 0.21422527730464935,
      "learning_rate": 4.9534161490683236e-05,
      "loss": 0.4557,
      "step": 80
    },
    {
      "epoch": 0.029027056083139224,
      "grad_norm": 0.22260454297065735,
      "learning_rate": 4.9528189202102245e-05,
      "loss": 0.4256,
      "step": 81
    },
    {
      "epoch": 0.029385414800215014,
      "grad_norm": 0.2079811841249466,
      "learning_rate": 4.952221691352126e-05,
      "loss": 0.4305,
      "step": 82
    },
    {
      "epoch": 0.02974377351729081,
      "grad_norm": 0.2266877442598343,
      "learning_rate": 4.951624462494028e-05,
      "loss": 0.478,
      "step": 83
    },
    {
      "epoch": 0.0301021322343666,
      "grad_norm": 0.2325388342142105,
      "learning_rate": 4.95102723363593e-05,
      "loss": 0.4552,
      "step": 84
    },
    {
      "epoch": 0.030460490951442393,
      "grad_norm": 0.218533456325531,
      "learning_rate": 4.950430004777831e-05,
      "loss": 0.3833,
      "step": 85
    },
    {
      "epoch": 0.030818849668518188,
      "grad_norm": 0.24167372286319733,
      "learning_rate": 4.9498327759197325e-05,
      "loss": 0.4908,
      "step": 86
    },
    {
      "epoch": 0.03117720838559398,
      "grad_norm": 0.2361288219690323,
      "learning_rate": 4.949235547061634e-05,
      "loss": 0.4302,
      "step": 87
    },
    {
      "epoch": 0.03153556710266977,
      "grad_norm": 0.24358461797237396,
      "learning_rate": 4.948638318203536e-05,
      "loss": 0.4491,
      "step": 88
    },
    {
      "epoch": 0.03189392581974557,
      "grad_norm": 0.25181522965431213,
      "learning_rate": 4.948041089345437e-05,
      "loss": 0.4773,
      "step": 89
    },
    {
      "epoch": 0.03225228453682136,
      "grad_norm": 0.275645911693573,
      "learning_rate": 4.947443860487339e-05,
      "loss": 0.4813,
      "step": 90
    },
    {
      "epoch": 0.03261064325389715,
      "grad_norm": 0.3181723654270172,
      "learning_rate": 4.9468466316292405e-05,
      "loss": 0.5817,
      "step": 91
    },
    {
      "epoch": 0.032969001970972946,
      "grad_norm": 0.28946229815483093,
      "learning_rate": 4.946249402771142e-05,
      "loss": 0.3797,
      "step": 92
    },
    {
      "epoch": 0.033327360688048736,
      "grad_norm": 0.4307362735271454,
      "learning_rate": 4.945652173913044e-05,
      "loss": 0.4234,
      "step": 93
    },
    {
      "epoch": 0.03368571940512453,
      "grad_norm": 0.28920120000839233,
      "learning_rate": 4.945054945054945e-05,
      "loss": 0.3889,
      "step": 94
    },
    {
      "epoch": 0.034044078122200325,
      "grad_norm": 0.3062282204627991,
      "learning_rate": 4.944457716196847e-05,
      "loss": 0.327,
      "step": 95
    },
    {
      "epoch": 0.034402436839276115,
      "grad_norm": 0.309128999710083,
      "learning_rate": 4.9438604873387485e-05,
      "loss": 0.3212,
      "step": 96
    },
    {
      "epoch": 0.034760795556351906,
      "grad_norm": 0.3194722533226013,
      "learning_rate": 4.94326325848065e-05,
      "loss": 0.3503,
      "step": 97
    },
    {
      "epoch": 0.035119154273427704,
      "grad_norm": 0.36419907212257385,
      "learning_rate": 4.942666029622552e-05,
      "loss": 0.4363,
      "step": 98
    },
    {
      "epoch": 0.035477512990503494,
      "grad_norm": 0.35646557807922363,
      "learning_rate": 4.942068800764453e-05,
      "loss": 0.3646,
      "step": 99
    },
    {
      "epoch": 0.035835871707579285,
      "grad_norm": 0.3518977165222168,
      "learning_rate": 4.941471571906355e-05,
      "loss": 0.3358,
      "step": 100
    },
    {
      "epoch": 0.03619423042465508,
      "grad_norm": 0.37824687361717224,
      "learning_rate": 4.9408743430482565e-05,
      "loss": 0.4602,
      "step": 101
    },
    {
      "epoch": 0.036552589141730873,
      "grad_norm": 0.33916598558425903,
      "learning_rate": 4.9402771141901574e-05,
      "loss": 0.2513,
      "step": 102
    },
    {
      "epoch": 0.036910947858806664,
      "grad_norm": 0.37959322333335876,
      "learning_rate": 4.93967988533206e-05,
      "loss": 0.3577,
      "step": 103
    },
    {
      "epoch": 0.037269306575882455,
      "grad_norm": 0.32848769426345825,
      "learning_rate": 4.939082656473961e-05,
      "loss": 0.2202,
      "step": 104
    },
    {
      "epoch": 0.03762766529295825,
      "grad_norm": 0.3451696038246155,
      "learning_rate": 4.938485427615863e-05,
      "loss": 0.4048,
      "step": 105
    },
    {
      "epoch": 0.03798602401003404,
      "grad_norm": 0.33096784353256226,
      "learning_rate": 4.937888198757764e-05,
      "loss": 0.4056,
      "step": 106
    },
    {
      "epoch": 0.038344382727109834,
      "grad_norm": 0.30412161350250244,
      "learning_rate": 4.9372909698996654e-05,
      "loss": 0.3498,
      "step": 107
    },
    {
      "epoch": 0.03870274144418563,
      "grad_norm": 0.2997516393661499,
      "learning_rate": 4.936693741041568e-05,
      "loss": 0.2483,
      "step": 108
    },
    {
      "epoch": 0.03906110016126142,
      "grad_norm": 0.2972385585308075,
      "learning_rate": 4.936096512183469e-05,
      "loss": 0.3165,
      "step": 109
    },
    {
      "epoch": 0.03941945887833721,
      "grad_norm": 0.29209890961647034,
      "learning_rate": 4.93549928332537e-05,
      "loss": 0.247,
      "step": 110
    },
    {
      "epoch": 0.03977781759541301,
      "grad_norm": 0.27570584416389465,
      "learning_rate": 4.934902054467272e-05,
      "loss": 0.3293,
      "step": 111
    },
    {
      "epoch": 0.0401361763124888,
      "grad_norm": 0.2765166759490967,
      "learning_rate": 4.9343048256091734e-05,
      "loss": 0.2333,
      "step": 112
    },
    {
      "epoch": 0.04049453502956459,
      "grad_norm": 0.26391762495040894,
      "learning_rate": 4.933707596751076e-05,
      "loss": 0.2451,
      "step": 113
    },
    {
      "epoch": 0.04085289374664039,
      "grad_norm": 0.3353630304336548,
      "learning_rate": 4.9331103678929766e-05,
      "loss": 0.512,
      "step": 114
    },
    {
      "epoch": 0.04121125246371618,
      "grad_norm": 0.26737532019615173,
      "learning_rate": 4.932513139034878e-05,
      "loss": 0.2342,
      "step": 115
    },
    {
      "epoch": 0.04156961118079197,
      "grad_norm": 0.22714956104755402,
      "learning_rate": 4.93191591017678e-05,
      "loss": 0.2209,
      "step": 116
    },
    {
      "epoch": 0.04192796989786777,
      "grad_norm": 0.23078717291355133,
      "learning_rate": 4.931318681318682e-05,
      "loss": 0.2065,
      "step": 117
    },
    {
      "epoch": 0.04228632861494356,
      "grad_norm": 0.276682585477829,
      "learning_rate": 4.930721452460583e-05,
      "loss": 0.2281,
      "step": 118
    },
    {
      "epoch": 0.04264468733201935,
      "grad_norm": 0.3047322928905487,
      "learning_rate": 4.9301242236024846e-05,
      "loss": 0.2358,
      "step": 119
    },
    {
      "epoch": 0.04300304604909514,
      "grad_norm": 0.33730417490005493,
      "learning_rate": 4.929526994744386e-05,
      "loss": 0.3056,
      "step": 120
    },
    {
      "epoch": 0.04336140476617094,
      "grad_norm": 0.2963522970676422,
      "learning_rate": 4.928929765886288e-05,
      "loss": 0.2079,
      "step": 121
    },
    {
      "epoch": 0.04371976348324673,
      "grad_norm": 0.47749483585357666,
      "learning_rate": 4.9283325370281894e-05,
      "loss": 0.3273,
      "step": 122
    },
    {
      "epoch": 0.04407812220032252,
      "grad_norm": 0.30452725291252136,
      "learning_rate": 4.927735308170091e-05,
      "loss": 0.2249,
      "step": 123
    },
    {
      "epoch": 0.04443648091739832,
      "grad_norm": 0.24357815086841583,
      "learning_rate": 4.9271380793119926e-05,
      "loss": 0.1787,
      "step": 124
    },
    {
      "epoch": 0.04479483963447411,
      "grad_norm": 0.2482309490442276,
      "learning_rate": 4.926540850453894e-05,
      "loss": 0.1985,
      "step": 125
    },
    {
      "epoch": 0.0451531983515499,
      "grad_norm": 0.278746634721756,
      "learning_rate": 4.925943621595796e-05,
      "loss": 0.215,
      "step": 126
    },
    {
      "epoch": 0.045511557068625696,
      "grad_norm": 0.29208019375801086,
      "learning_rate": 4.9253463927376974e-05,
      "loss": 0.2115,
      "step": 127
    },
    {
      "epoch": 0.04586991578570149,
      "grad_norm": 0.34888461232185364,
      "learning_rate": 4.924749163879599e-05,
      "loss": 0.2906,
      "step": 128
    },
    {
      "epoch": 0.04622827450277728,
      "grad_norm": 0.25117626786231995,
      "learning_rate": 4.9241519350215006e-05,
      "loss": 0.1571,
      "step": 129
    },
    {
      "epoch": 0.046586633219853076,
      "grad_norm": 0.27335798740386963,
      "learning_rate": 4.9235547061634015e-05,
      "loss": 0.1428,
      "step": 130
    },
    {
      "epoch": 0.046944991936928866,
      "grad_norm": 0.28515613079071045,
      "learning_rate": 4.922957477305304e-05,
      "loss": 0.2184,
      "step": 131
    },
    {
      "epoch": 0.04730335065400466,
      "grad_norm": 0.25557321310043335,
      "learning_rate": 4.9223602484472054e-05,
      "loss": 0.1955,
      "step": 132
    },
    {
      "epoch": 0.047661709371080455,
      "grad_norm": 0.18515686690807343,
      "learning_rate": 4.921763019589107e-05,
      "loss": 0.1739,
      "step": 133
    },
    {
      "epoch": 0.048020068088156245,
      "grad_norm": 0.2710229754447937,
      "learning_rate": 4.921165790731008e-05,
      "loss": 0.2893,
      "step": 134
    },
    {
      "epoch": 0.048378426805232036,
      "grad_norm": 0.2276855856180191,
      "learning_rate": 4.9205685618729095e-05,
      "loss": 0.242,
      "step": 135
    },
    {
      "epoch": 0.04873678552230783,
      "grad_norm": 0.15873204171657562,
      "learning_rate": 4.919971333014812e-05,
      "loss": 0.1818,
      "step": 136
    },
    {
      "epoch": 0.049095144239383624,
      "grad_norm": 0.2027217149734497,
      "learning_rate": 4.9193741041567134e-05,
      "loss": 0.181,
      "step": 137
    },
    {
      "epoch": 0.049453502956459415,
      "grad_norm": 0.23012855648994446,
      "learning_rate": 4.918776875298614e-05,
      "loss": 0.206,
      "step": 138
    },
    {
      "epoch": 0.049811861673535206,
      "grad_norm": 0.18605534732341766,
      "learning_rate": 4.918179646440516e-05,
      "loss": 0.1549,
      "step": 139
    },
    {
      "epoch": 0.050170220390611,
      "grad_norm": 0.24357332289218903,
      "learning_rate": 4.9175824175824175e-05,
      "loss": 0.2215,
      "step": 140
    },
    {
      "epoch": 0.050528579107686794,
      "grad_norm": 0.3944641947746277,
      "learning_rate": 4.91698518872432e-05,
      "loss": 0.2344,
      "step": 141
    },
    {
      "epoch": 0.050886937824762585,
      "grad_norm": 0.17388564348220825,
      "learning_rate": 4.916387959866221e-05,
      "loss": 0.1727,
      "step": 142
    },
    {
      "epoch": 0.05124529654183838,
      "grad_norm": 0.16706374287605286,
      "learning_rate": 4.915790731008122e-05,
      "loss": 0.1469,
      "step": 143
    },
    {
      "epoch": 0.05160365525891417,
      "grad_norm": 0.16845233738422394,
      "learning_rate": 4.915193502150024e-05,
      "loss": 0.1376,
      "step": 144
    },
    {
      "epoch": 0.051962013975989964,
      "grad_norm": 0.2852131724357605,
      "learning_rate": 4.914596273291926e-05,
      "loss": 0.3542,
      "step": 145
    },
    {
      "epoch": 0.05232037269306576,
      "grad_norm": 0.245087668299675,
      "learning_rate": 4.913999044433827e-05,
      "loss": 0.2857,
      "step": 146
    },
    {
      "epoch": 0.05267873141014155,
      "grad_norm": 0.26394104957580566,
      "learning_rate": 4.913401815575729e-05,
      "loss": 0.2417,
      "step": 147
    },
    {
      "epoch": 0.05303709012721734,
      "grad_norm": 0.25003480911254883,
      "learning_rate": 4.91280458671763e-05,
      "loss": 0.234,
      "step": 148
    },
    {
      "epoch": 0.05339544884429314,
      "grad_norm": 0.19619432091712952,
      "learning_rate": 4.912207357859532e-05,
      "loss": 0.1799,
      "step": 149
    },
    {
      "epoch": 0.05375380756136893,
      "grad_norm": 0.268304705619812,
      "learning_rate": 4.9116101290014335e-05,
      "loss": 0.2243,
      "step": 150
    },
    {
      "epoch": 0.05411216627844472,
      "grad_norm": 0.2035079300403595,
      "learning_rate": 4.911012900143335e-05,
      "loss": 0.1853,
      "step": 151
    },
    {
      "epoch": 0.05447052499552051,
      "grad_norm": 0.4195922613143921,
      "learning_rate": 4.910415671285237e-05,
      "loss": 0.2935,
      "step": 152
    },
    {
      "epoch": 0.05482888371259631,
      "grad_norm": 0.1838088184595108,
      "learning_rate": 4.909818442427138e-05,
      "loss": 0.2057,
      "step": 153
    },
    {
      "epoch": 0.0551872424296721,
      "grad_norm": 0.20268432796001434,
      "learning_rate": 4.90922121356904e-05,
      "loss": 0.2003,
      "step": 154
    },
    {
      "epoch": 0.05554560114674789,
      "grad_norm": 0.16689835488796234,
      "learning_rate": 4.9086239847109415e-05,
      "loss": 0.0988,
      "step": 155
    },
    {
      "epoch": 0.05590395986382369,
      "grad_norm": 0.17259439826011658,
      "learning_rate": 4.908026755852843e-05,
      "loss": 0.1697,
      "step": 156
    },
    {
      "epoch": 0.05626231858089948,
      "grad_norm": 0.425293505191803,
      "learning_rate": 4.907429526994745e-05,
      "loss": 0.8619,
      "step": 157
    },
    {
      "epoch": 0.05662067729797527,
      "grad_norm": 0.202299565076828,
      "learning_rate": 4.906832298136646e-05,
      "loss": 0.2007,
      "step": 158
    },
    {
      "epoch": 0.05697903601505107,
      "grad_norm": 0.2395390123128891,
      "learning_rate": 4.906235069278548e-05,
      "loss": 0.2047,
      "step": 159
    },
    {
      "epoch": 0.05733739473212686,
      "grad_norm": 0.2528805434703827,
      "learning_rate": 4.9056378404204495e-05,
      "loss": 0.2257,
      "step": 160
    },
    {
      "epoch": 0.05769575344920265,
      "grad_norm": 0.22283227741718292,
      "learning_rate": 4.905040611562351e-05,
      "loss": 0.195,
      "step": 161
    },
    {
      "epoch": 0.05805411216627845,
      "grad_norm": 0.16007082164287567,
      "learning_rate": 4.904443382704253e-05,
      "loss": 0.2034,
      "step": 162
    },
    {
      "epoch": 0.05841247088335424,
      "grad_norm": 0.20828954875469208,
      "learning_rate": 4.9038461538461536e-05,
      "loss": 0.2275,
      "step": 163
    },
    {
      "epoch": 0.05877082960043003,
      "grad_norm": 0.14837133884429932,
      "learning_rate": 4.903248924988056e-05,
      "loss": 0.1344,
      "step": 164
    },
    {
      "epoch": 0.059129188317505826,
      "grad_norm": 0.19820012152194977,
      "learning_rate": 4.9026516961299575e-05,
      "loss": 0.1575,
      "step": 165
    },
    {
      "epoch": 0.05948754703458162,
      "grad_norm": 0.15185175836086273,
      "learning_rate": 4.902054467271859e-05,
      "loss": 0.1556,
      "step": 166
    },
    {
      "epoch": 0.05984590575165741,
      "grad_norm": 0.188672736287117,
      "learning_rate": 4.90145723841376e-05,
      "loss": 0.191,
      "step": 167
    },
    {
      "epoch": 0.0602042644687332,
      "grad_norm": 0.3711465895175934,
      "learning_rate": 4.9008600095556616e-05,
      "loss": 0.2497,
      "step": 168
    },
    {
      "epoch": 0.060562623185808996,
      "grad_norm": 0.22556760907173157,
      "learning_rate": 4.900262780697564e-05,
      "loss": 0.1873,
      "step": 169
    },
    {
      "epoch": 0.06092098190288479,
      "grad_norm": 0.16881833970546722,
      "learning_rate": 4.8996655518394655e-05,
      "loss": 0.1777,
      "step": 170
    },
    {
      "epoch": 0.06127934061996058,
      "grad_norm": 0.15546242892742157,
      "learning_rate": 4.8990683229813664e-05,
      "loss": 0.163,
      "step": 171
    },
    {
      "epoch": 0.061637699337036375,
      "grad_norm": 0.1774119883775711,
      "learning_rate": 4.898471094123268e-05,
      "loss": 0.1222,
      "step": 172
    },
    {
      "epoch": 0.061996058054112166,
      "grad_norm": 0.17930057644844055,
      "learning_rate": 4.89787386526517e-05,
      "loss": 0.1517,
      "step": 173
    },
    {
      "epoch": 0.06235441677118796,
      "grad_norm": 0.1834963858127594,
      "learning_rate": 4.897276636407072e-05,
      "loss": 0.1497,
      "step": 174
    },
    {
      "epoch": 0.06271277548826375,
      "grad_norm": 0.1875227838754654,
      "learning_rate": 4.896679407548973e-05,
      "loss": 0.1545,
      "step": 175
    },
    {
      "epoch": 0.06307113420533954,
      "grad_norm": 0.1656855046749115,
      "learning_rate": 4.8960821786908744e-05,
      "loss": 0.1983,
      "step": 176
    },
    {
      "epoch": 0.06342949292241534,
      "grad_norm": 0.22786028683185577,
      "learning_rate": 4.895484949832776e-05,
      "loss": 0.1876,
      "step": 177
    },
    {
      "epoch": 0.06378785163949113,
      "grad_norm": 0.17633187770843506,
      "learning_rate": 4.894887720974678e-05,
      "loss": 0.1076,
      "step": 178
    },
    {
      "epoch": 0.06414621035656692,
      "grad_norm": 0.29575493931770325,
      "learning_rate": 4.894290492116579e-05,
      "loss": 0.3248,
      "step": 179
    },
    {
      "epoch": 0.06450456907364271,
      "grad_norm": 0.16504469513893127,
      "learning_rate": 4.893693263258481e-05,
      "loss": 0.1622,
      "step": 180
    },
    {
      "epoch": 0.0648629277907185,
      "grad_norm": 0.14046438038349152,
      "learning_rate": 4.8930960344003824e-05,
      "loss": 0.1325,
      "step": 181
    },
    {
      "epoch": 0.0652212865077943,
      "grad_norm": 0.14508800208568573,
      "learning_rate": 4.892498805542284e-05,
      "loss": 0.1274,
      "step": 182
    },
    {
      "epoch": 0.0655796452248701,
      "grad_norm": 0.2110680788755417,
      "learning_rate": 4.8919015766841856e-05,
      "loss": 0.1441,
      "step": 183
    },
    {
      "epoch": 0.06593800394194589,
      "grad_norm": 0.3298022747039795,
      "learning_rate": 4.891304347826087e-05,
      "loss": 0.1579,
      "step": 184
    },
    {
      "epoch": 0.06629636265902168,
      "grad_norm": 0.22833582758903503,
      "learning_rate": 4.890707118967989e-05,
      "loss": 0.1632,
      "step": 185
    },
    {
      "epoch": 0.06665472137609747,
      "grad_norm": 0.18038471043109894,
      "learning_rate": 4.8901098901098904e-05,
      "loss": 0.1662,
      "step": 186
    },
    {
      "epoch": 0.06701308009317326,
      "grad_norm": 0.1786368489265442,
      "learning_rate": 4.889512661251792e-05,
      "loss": 0.163,
      "step": 187
    },
    {
      "epoch": 0.06737143881024905,
      "grad_norm": 0.16630782186985016,
      "learning_rate": 4.8889154323936936e-05,
      "loss": 0.1424,
      "step": 188
    },
    {
      "epoch": 0.06772979752732486,
      "grad_norm": 0.35446611046791077,
      "learning_rate": 4.888318203535595e-05,
      "loss": 0.2509,
      "step": 189
    },
    {
      "epoch": 0.06808815624440065,
      "grad_norm": 0.2174365222454071,
      "learning_rate": 4.887720974677497e-05,
      "loss": 0.2056,
      "step": 190
    },
    {
      "epoch": 0.06844651496147644,
      "grad_norm": 0.20328527688980103,
      "learning_rate": 4.887123745819398e-05,
      "loss": 0.1799,
      "step": 191
    },
    {
      "epoch": 0.06880487367855223,
      "grad_norm": 0.264043390750885,
      "learning_rate": 4.8865265169613e-05,
      "loss": 0.2879,
      "step": 192
    },
    {
      "epoch": 0.06916323239562802,
      "grad_norm": 0.22081898152828217,
      "learning_rate": 4.8859292881032016e-05,
      "loss": 0.2481,
      "step": 193
    },
    {
      "epoch": 0.06952159111270381,
      "grad_norm": 0.15981683135032654,
      "learning_rate": 4.885332059245103e-05,
      "loss": 0.127,
      "step": 194
    },
    {
      "epoch": 0.0698799498297796,
      "grad_norm": 0.17570604383945465,
      "learning_rate": 4.884734830387004e-05,
      "loss": 0.1021,
      "step": 195
    },
    {
      "epoch": 0.07023830854685541,
      "grad_norm": 0.17946885526180267,
      "learning_rate": 4.884137601528906e-05,
      "loss": 0.174,
      "step": 196
    },
    {
      "epoch": 0.0705966672639312,
      "grad_norm": 0.16112278401851654,
      "learning_rate": 4.883540372670808e-05,
      "loss": 0.1913,
      "step": 197
    },
    {
      "epoch": 0.07095502598100699,
      "grad_norm": 0.2605041563510895,
      "learning_rate": 4.8829431438127096e-05,
      "loss": 0.2226,
      "step": 198
    },
    {
      "epoch": 0.07131338469808278,
      "grad_norm": 0.19358345866203308,
      "learning_rate": 4.8823459149546105e-05,
      "loss": 0.1755,
      "step": 199
    },
    {
      "epoch": 0.07167174341515857,
      "grad_norm": 0.17513500154018402,
      "learning_rate": 4.881748686096512e-05,
      "loss": 0.1092,
      "step": 200
    },
    {
      "epoch": 0.07203010213223436,
      "grad_norm": 0.28034499287605286,
      "learning_rate": 4.8811514572384144e-05,
      "loss": 0.1965,
      "step": 201
    },
    {
      "epoch": 0.07238846084931017,
      "grad_norm": 0.20823431015014648,
      "learning_rate": 4.880554228380316e-05,
      "loss": 0.2224,
      "step": 202
    },
    {
      "epoch": 0.07274681956638596,
      "grad_norm": 0.20210617780685425,
      "learning_rate": 4.879956999522217e-05,
      "loss": 0.2133,
      "step": 203
    },
    {
      "epoch": 0.07310517828346175,
      "grad_norm": 0.1569713056087494,
      "learning_rate": 4.8793597706641185e-05,
      "loss": 0.1345,
      "step": 204
    },
    {
      "epoch": 0.07346353700053754,
      "grad_norm": 0.20693735778331757,
      "learning_rate": 4.87876254180602e-05,
      "loss": 0.2205,
      "step": 205
    },
    {
      "epoch": 0.07382189571761333,
      "grad_norm": 0.2902527153491974,
      "learning_rate": 4.8781653129479224e-05,
      "loss": 0.1108,
      "step": 206
    },
    {
      "epoch": 0.07418025443468912,
      "grad_norm": 0.24802440404891968,
      "learning_rate": 4.877568084089823e-05,
      "loss": 0.1391,
      "step": 207
    },
    {
      "epoch": 0.07453861315176491,
      "grad_norm": 0.17619319260120392,
      "learning_rate": 4.876970855231725e-05,
      "loss": 0.1344,
      "step": 208
    },
    {
      "epoch": 0.07489697186884071,
      "grad_norm": 0.1938244104385376,
      "learning_rate": 4.8763736263736265e-05,
      "loss": 0.1763,
      "step": 209
    },
    {
      "epoch": 0.0752553305859165,
      "grad_norm": 0.3311384618282318,
      "learning_rate": 4.875776397515528e-05,
      "loss": 0.3614,
      "step": 210
    },
    {
      "epoch": 0.0756136893029923,
      "grad_norm": 0.2320941835641861,
      "learning_rate": 4.87517916865743e-05,
      "loss": 0.203,
      "step": 211
    },
    {
      "epoch": 0.07597204802006809,
      "grad_norm": 0.16116313636302948,
      "learning_rate": 4.874581939799331e-05,
      "loss": 0.1668,
      "step": 212
    },
    {
      "epoch": 0.07633040673714388,
      "grad_norm": 0.2070159614086151,
      "learning_rate": 4.873984710941233e-05,
      "loss": 0.2494,
      "step": 213
    },
    {
      "epoch": 0.07668876545421967,
      "grad_norm": 0.1838577836751938,
      "learning_rate": 4.8733874820831345e-05,
      "loss": 0.1427,
      "step": 214
    },
    {
      "epoch": 0.07704712417129547,
      "grad_norm": 0.18143275380134583,
      "learning_rate": 4.872790253225036e-05,
      "loss": 0.1571,
      "step": 215
    },
    {
      "epoch": 0.07740548288837126,
      "grad_norm": 0.16583941876888275,
      "learning_rate": 4.872193024366938e-05,
      "loss": 0.1505,
      "step": 216
    },
    {
      "epoch": 0.07776384160544705,
      "grad_norm": 0.3242007791996002,
      "learning_rate": 4.871595795508839e-05,
      "loss": 0.2162,
      "step": 217
    },
    {
      "epoch": 0.07812220032252284,
      "grad_norm": 0.1706347018480301,
      "learning_rate": 4.870998566650741e-05,
      "loss": 0.1433,
      "step": 218
    },
    {
      "epoch": 0.07848055903959864,
      "grad_norm": 0.20231230556964874,
      "learning_rate": 4.8704013377926425e-05,
      "loss": 0.2143,
      "step": 219
    },
    {
      "epoch": 0.07883891775667443,
      "grad_norm": 0.15817378461360931,
      "learning_rate": 4.869804108934544e-05,
      "loss": 0.1418,
      "step": 220
    },
    {
      "epoch": 0.07919727647375023,
      "grad_norm": 0.21155042946338654,
      "learning_rate": 4.869206880076446e-05,
      "loss": 0.2045,
      "step": 221
    },
    {
      "epoch": 0.07955563519082602,
      "grad_norm": 0.2261083722114563,
      "learning_rate": 4.868609651218347e-05,
      "loss": 0.1401,
      "step": 222
    },
    {
      "epoch": 0.07991399390790181,
      "grad_norm": 0.2058093547821045,
      "learning_rate": 4.868012422360249e-05,
      "loss": 0.1936,
      "step": 223
    },
    {
      "epoch": 0.0802723526249776,
      "grad_norm": 0.17346060276031494,
      "learning_rate": 4.86741519350215e-05,
      "loss": 0.1408,
      "step": 224
    },
    {
      "epoch": 0.0806307113420534,
      "grad_norm": 0.17506147921085358,
      "learning_rate": 4.866817964644052e-05,
      "loss": 0.1448,
      "step": 225
    },
    {
      "epoch": 0.08098907005912918,
      "grad_norm": 0.20292793214321136,
      "learning_rate": 4.866220735785954e-05,
      "loss": 0.2196,
      "step": 226
    },
    {
      "epoch": 0.08134742877620497,
      "grad_norm": 0.20290492475032806,
      "learning_rate": 4.865623506927855e-05,
      "loss": 0.1728,
      "step": 227
    },
    {
      "epoch": 0.08170578749328078,
      "grad_norm": 0.16371123492717743,
      "learning_rate": 4.865026278069756e-05,
      "loss": 0.1339,
      "step": 228
    },
    {
      "epoch": 0.08206414621035657,
      "grad_norm": 0.3035982549190521,
      "learning_rate": 4.864429049211658e-05,
      "loss": 0.197,
      "step": 229
    },
    {
      "epoch": 0.08242250492743236,
      "grad_norm": 0.2649548649787903,
      "learning_rate": 4.86383182035356e-05,
      "loss": 0.2061,
      "step": 230
    },
    {
      "epoch": 0.08278086364450815,
      "grad_norm": 0.18573452532291412,
      "learning_rate": 4.863234591495461e-05,
      "loss": 0.1768,
      "step": 231
    },
    {
      "epoch": 0.08313922236158394,
      "grad_norm": 0.2315385937690735,
      "learning_rate": 4.8626373626373626e-05,
      "loss": 0.1966,
      "step": 232
    },
    {
      "epoch": 0.08349758107865973,
      "grad_norm": 0.18457846343517303,
      "learning_rate": 4.862040133779264e-05,
      "loss": 0.183,
      "step": 233
    },
    {
      "epoch": 0.08385593979573554,
      "grad_norm": 0.21711626648902893,
      "learning_rate": 4.8614429049211665e-05,
      "loss": 0.2104,
      "step": 234
    },
    {
      "epoch": 0.08421429851281133,
      "grad_norm": 0.34826910495758057,
      "learning_rate": 4.8608456760630674e-05,
      "loss": 0.4612,
      "step": 235
    },
    {
      "epoch": 0.08457265722988712,
      "grad_norm": 0.19323702156543732,
      "learning_rate": 4.860248447204969e-05,
      "loss": 0.1248,
      "step": 236
    },
    {
      "epoch": 0.08493101594696291,
      "grad_norm": 0.2868496775627136,
      "learning_rate": 4.8596512183468706e-05,
      "loss": 0.2578,
      "step": 237
    },
    {
      "epoch": 0.0852893746640387,
      "grad_norm": 0.25920167565345764,
      "learning_rate": 4.859053989488772e-05,
      "loss": 0.1426,
      "step": 238
    },
    {
      "epoch": 0.08564773338111449,
      "grad_norm": 0.2116648554801941,
      "learning_rate": 4.858456760630674e-05,
      "loss": 0.2168,
      "step": 239
    },
    {
      "epoch": 0.08600609209819028,
      "grad_norm": 0.23402974009513855,
      "learning_rate": 4.8578595317725754e-05,
      "loss": 0.2948,
      "step": 240
    },
    {
      "epoch": 0.08636445081526609,
      "grad_norm": 0.19198866188526154,
      "learning_rate": 4.857262302914477e-05,
      "loss": 0.1261,
      "step": 241
    },
    {
      "epoch": 0.08672280953234188,
      "grad_norm": 0.21391993761062622,
      "learning_rate": 4.8566650740563786e-05,
      "loss": 0.1751,
      "step": 242
    },
    {
      "epoch": 0.08708116824941767,
      "grad_norm": 0.20832787454128265,
      "learning_rate": 4.85606784519828e-05,
      "loss": 0.1741,
      "step": 243
    },
    {
      "epoch": 0.08743952696649346,
      "grad_norm": 0.19579313695430756,
      "learning_rate": 4.855470616340182e-05,
      "loss": 0.1336,
      "step": 244
    },
    {
      "epoch": 0.08779788568356925,
      "grad_norm": 0.23382562398910522,
      "learning_rate": 4.8548733874820834e-05,
      "loss": 0.2087,
      "step": 245
    },
    {
      "epoch": 0.08815624440064504,
      "grad_norm": 0.2735370993614197,
      "learning_rate": 4.854276158623985e-05,
      "loss": 0.1453,
      "step": 246
    },
    {
      "epoch": 0.08851460311772084,
      "grad_norm": 0.18614327907562256,
      "learning_rate": 4.8536789297658866e-05,
      "loss": 0.205,
      "step": 247
    },
    {
      "epoch": 0.08887296183479663,
      "grad_norm": 0.2037499099969864,
      "learning_rate": 4.853081700907788e-05,
      "loss": 0.1712,
      "step": 248
    },
    {
      "epoch": 0.08923132055187243,
      "grad_norm": 0.19306589663028717,
      "learning_rate": 4.85248447204969e-05,
      "loss": 0.1752,
      "step": 249
    },
    {
      "epoch": 0.08958967926894822,
      "grad_norm": 0.17412534356117249,
      "learning_rate": 4.8518872431915914e-05,
      "loss": 0.1176,
      "step": 250
    },
    {
      "epoch": 0.08994803798602401,
      "grad_norm": 0.14464156329631805,
      "learning_rate": 4.851290014333493e-05,
      "loss": 0.1402,
      "step": 251
    },
    {
      "epoch": 0.0903063967030998,
      "grad_norm": 0.15614837408065796,
      "learning_rate": 4.850692785475394e-05,
      "loss": 0.1253,
      "step": 252
    },
    {
      "epoch": 0.0906647554201756,
      "grad_norm": 0.17321301996707916,
      "learning_rate": 4.850095556617296e-05,
      "loss": 0.1661,
      "step": 253
    },
    {
      "epoch": 0.09102311413725139,
      "grad_norm": 0.2313585728406906,
      "learning_rate": 4.849498327759198e-05,
      "loss": 0.2048,
      "step": 254
    },
    {
      "epoch": 0.09138147285432718,
      "grad_norm": 0.2553004026412964,
      "learning_rate": 4.8489010989010994e-05,
      "loss": 0.2078,
      "step": 255
    },
    {
      "epoch": 0.09173983157140297,
      "grad_norm": 0.20027665793895721,
      "learning_rate": 4.848303870043e-05,
      "loss": 0.1978,
      "step": 256
    },
    {
      "epoch": 0.09209819028847877,
      "grad_norm": 0.2089284062385559,
      "learning_rate": 4.847706641184902e-05,
      "loss": 0.1701,
      "step": 257
    },
    {
      "epoch": 0.09245654900555456,
      "grad_norm": 0.1998046636581421,
      "learning_rate": 4.847109412326804e-05,
      "loss": 0.1732,
      "step": 258
    },
    {
      "epoch": 0.09281490772263035,
      "grad_norm": 0.16686256229877472,
      "learning_rate": 4.846512183468706e-05,
      "loss": 0.161,
      "step": 259
    },
    {
      "epoch": 0.09317326643970615,
      "grad_norm": 0.19310146570205688,
      "learning_rate": 4.845914954610607e-05,
      "loss": 0.1586,
      "step": 260
    },
    {
      "epoch": 0.09353162515678194,
      "grad_norm": 0.28791970014572144,
      "learning_rate": 4.845317725752508e-05,
      "loss": 0.1772,
      "step": 261
    },
    {
      "epoch": 0.09388998387385773,
      "grad_norm": 0.23530277609825134,
      "learning_rate": 4.8447204968944106e-05,
      "loss": 0.192,
      "step": 262
    },
    {
      "epoch": 0.09424834259093352,
      "grad_norm": 0.19018632173538208,
      "learning_rate": 4.844123268036312e-05,
      "loss": 0.1543,
      "step": 263
    },
    {
      "epoch": 0.09460670130800931,
      "grad_norm": 0.1882745772600174,
      "learning_rate": 4.843526039178213e-05,
      "loss": 0.144,
      "step": 264
    },
    {
      "epoch": 0.0949650600250851,
      "grad_norm": 0.21120695769786835,
      "learning_rate": 4.842928810320115e-05,
      "loss": 0.1976,
      "step": 265
    },
    {
      "epoch": 0.09532341874216091,
      "grad_norm": 0.30511561036109924,
      "learning_rate": 4.842331581462016e-05,
      "loss": 0.4284,
      "step": 266
    },
    {
      "epoch": 0.0956817774592367,
      "grad_norm": 0.2265871912240982,
      "learning_rate": 4.8417343526039186e-05,
      "loss": 0.1448,
      "step": 267
    },
    {
      "epoch": 0.09604013617631249,
      "grad_norm": 0.1520865559577942,
      "learning_rate": 4.8411371237458195e-05,
      "loss": 0.143,
      "step": 268
    },
    {
      "epoch": 0.09639849489338828,
      "grad_norm": 0.2078649401664734,
      "learning_rate": 4.840539894887721e-05,
      "loss": 0.1769,
      "step": 269
    },
    {
      "epoch": 0.09675685361046407,
      "grad_norm": 0.19680829346179962,
      "learning_rate": 4.839942666029623e-05,
      "loss": 0.1909,
      "step": 270
    },
    {
      "epoch": 0.09711521232753986,
      "grad_norm": 0.3116722106933594,
      "learning_rate": 4.839345437171524e-05,
      "loss": 0.1664,
      "step": 271
    },
    {
      "epoch": 0.09747357104461565,
      "grad_norm": 0.22728586196899414,
      "learning_rate": 4.838748208313426e-05,
      "loss": 0.2675,
      "step": 272
    },
    {
      "epoch": 0.09783192976169146,
      "grad_norm": 0.18516074120998383,
      "learning_rate": 4.8381509794553275e-05,
      "loss": 0.183,
      "step": 273
    },
    {
      "epoch": 0.09819028847876725,
      "grad_norm": 0.1576915681362152,
      "learning_rate": 4.837553750597229e-05,
      "loss": 0.1276,
      "step": 274
    },
    {
      "epoch": 0.09854864719584304,
      "grad_norm": 0.2539936602115631,
      "learning_rate": 4.836956521739131e-05,
      "loss": 0.3053,
      "step": 275
    },
    {
      "epoch": 0.09890700591291883,
      "grad_norm": 0.259624719619751,
      "learning_rate": 4.836359292881032e-05,
      "loss": 0.3226,
      "step": 276
    },
    {
      "epoch": 0.09926536462999462,
      "grad_norm": 0.39196422696113586,
      "learning_rate": 4.835762064022934e-05,
      "loss": 0.4496,
      "step": 277
    },
    {
      "epoch": 0.09962372334707041,
      "grad_norm": 0.18025647103786469,
      "learning_rate": 4.8351648351648355e-05,
      "loss": 0.1605,
      "step": 278
    },
    {
      "epoch": 0.09998208206414622,
      "grad_norm": 0.2623847723007202,
      "learning_rate": 4.834567606306737e-05,
      "loss": 0.23,
      "step": 279
    },
    {
      "epoch": 0.100340440781222,
      "grad_norm": 0.19100594520568848,
      "learning_rate": 4.833970377448638e-05,
      "loss": 0.1692,
      "step": 280
    },
    {
      "epoch": 0.1006987994982978,
      "grad_norm": 0.3554922342300415,
      "learning_rate": 4.83337314859054e-05,
      "loss": 0.188,
      "step": 281
    },
    {
      "epoch": 0.10105715821537359,
      "grad_norm": 0.20372998714447021,
      "learning_rate": 4.832775919732442e-05,
      "loss": 0.2066,
      "step": 282
    },
    {
      "epoch": 0.10141551693244938,
      "grad_norm": 0.17304092645645142,
      "learning_rate": 4.8321786908743435e-05,
      "loss": 0.11,
      "step": 283
    },
    {
      "epoch": 0.10177387564952517,
      "grad_norm": 0.19256143271923065,
      "learning_rate": 4.8315814620162444e-05,
      "loss": 0.1998,
      "step": 284
    },
    {
      "epoch": 0.10213223436660097,
      "grad_norm": 0.18189814686775208,
      "learning_rate": 4.830984233158146e-05,
      "loss": 0.1903,
      "step": 285
    },
    {
      "epoch": 0.10249059308367676,
      "grad_norm": 0.2188909649848938,
      "learning_rate": 4.830387004300048e-05,
      "loss": 0.174,
      "step": 286
    },
    {
      "epoch": 0.10284895180075256,
      "grad_norm": 0.274290531873703,
      "learning_rate": 4.82978977544195e-05,
      "loss": 0.2737,
      "step": 287
    },
    {
      "epoch": 0.10320731051782835,
      "grad_norm": 0.18820689618587494,
      "learning_rate": 4.829192546583851e-05,
      "loss": 0.2183,
      "step": 288
    },
    {
      "epoch": 0.10356566923490414,
      "grad_norm": 0.23179493844509125,
      "learning_rate": 4.8285953177257524e-05,
      "loss": 0.2787,
      "step": 289
    },
    {
      "epoch": 0.10392402795197993,
      "grad_norm": 0.1640051007270813,
      "learning_rate": 4.827998088867655e-05,
      "loss": 0.0916,
      "step": 290
    },
    {
      "epoch": 0.10428238666905572,
      "grad_norm": 0.19165942072868347,
      "learning_rate": 4.827400860009556e-05,
      "loss": 0.1214,
      "step": 291
    },
    {
      "epoch": 0.10464074538613152,
      "grad_norm": 0.1831214427947998,
      "learning_rate": 4.826803631151457e-05,
      "loss": 0.154,
      "step": 292
    },
    {
      "epoch": 0.10499910410320731,
      "grad_norm": 0.18833032250404358,
      "learning_rate": 4.826206402293359e-05,
      "loss": 0.159,
      "step": 293
    },
    {
      "epoch": 0.1053574628202831,
      "grad_norm": 0.18044595420360565,
      "learning_rate": 4.8256091734352604e-05,
      "loss": 0.1376,
      "step": 294
    },
    {
      "epoch": 0.1057158215373589,
      "grad_norm": 0.2715647518634796,
      "learning_rate": 4.825011944577163e-05,
      "loss": 0.2239,
      "step": 295
    },
    {
      "epoch": 0.10607418025443469,
      "grad_norm": 0.22580409049987793,
      "learning_rate": 4.8244147157190636e-05,
      "loss": 0.2151,
      "step": 296
    },
    {
      "epoch": 0.10643253897151048,
      "grad_norm": 0.14449560642242432,
      "learning_rate": 4.823817486860965e-05,
      "loss": 0.1223,
      "step": 297
    },
    {
      "epoch": 0.10679089768858628,
      "grad_norm": 0.16570311784744263,
      "learning_rate": 4.823220258002867e-05,
      "loss": 0.1416,
      "step": 298
    },
    {
      "epoch": 0.10714925640566207,
      "grad_norm": 0.16707049310207367,
      "learning_rate": 4.8226230291447684e-05,
      "loss": 0.174,
      "step": 299
    },
    {
      "epoch": 0.10750761512273786,
      "grad_norm": 0.26125356554985046,
      "learning_rate": 4.82202580028667e-05,
      "loss": 0.2471,
      "step": 300
    },
    {
      "epoch": 0.10786597383981365,
      "grad_norm": 0.16848918795585632,
      "learning_rate": 4.8214285714285716e-05,
      "loss": 0.1334,
      "step": 301
    },
    {
      "epoch": 0.10822433255688944,
      "grad_norm": 0.2165093719959259,
      "learning_rate": 4.820831342570473e-05,
      "loss": 0.1965,
      "step": 302
    },
    {
      "epoch": 0.10858269127396523,
      "grad_norm": 0.16058401763439178,
      "learning_rate": 4.820234113712375e-05,
      "loss": 0.1655,
      "step": 303
    },
    {
      "epoch": 0.10894104999104103,
      "grad_norm": 0.22932620346546173,
      "learning_rate": 4.8196368848542764e-05,
      "loss": 0.254,
      "step": 304
    },
    {
      "epoch": 0.10929940870811683,
      "grad_norm": 0.3184254467487335,
      "learning_rate": 4.819039655996178e-05,
      "loss": 0.3037,
      "step": 305
    },
    {
      "epoch": 0.10965776742519262,
      "grad_norm": 0.19843073189258575,
      "learning_rate": 4.8184424271380796e-05,
      "loss": 0.1233,
      "step": 306
    },
    {
      "epoch": 0.11001612614226841,
      "grad_norm": 0.17621824145317078,
      "learning_rate": 4.817845198279981e-05,
      "loss": 0.1823,
      "step": 307
    },
    {
      "epoch": 0.1103744848593442,
      "grad_norm": 0.34529227018356323,
      "learning_rate": 4.817247969421883e-05,
      "loss": 0.4699,
      "step": 308
    },
    {
      "epoch": 0.11073284357641999,
      "grad_norm": 0.2942957878112793,
      "learning_rate": 4.8166507405637844e-05,
      "loss": 0.1473,
      "step": 309
    },
    {
      "epoch": 0.11109120229349578,
      "grad_norm": 0.1713462471961975,
      "learning_rate": 4.816053511705686e-05,
      "loss": 0.202,
      "step": 310
    },
    {
      "epoch": 0.11144956101057159,
      "grad_norm": 0.19702017307281494,
      "learning_rate": 4.8154562828475876e-05,
      "loss": 0.1872,
      "step": 311
    },
    {
      "epoch": 0.11180791972764738,
      "grad_norm": 0.20231960713863373,
      "learning_rate": 4.814859053989489e-05,
      "loss": 0.1861,
      "step": 312
    },
    {
      "epoch": 0.11216627844472317,
      "grad_norm": 0.48554569482803345,
      "learning_rate": 4.81426182513139e-05,
      "loss": 0.1905,
      "step": 313
    },
    {
      "epoch": 0.11252463716179896,
      "grad_norm": 0.22501389682292938,
      "learning_rate": 4.8136645962732924e-05,
      "loss": 0.1849,
      "step": 314
    },
    {
      "epoch": 0.11288299587887475,
      "grad_norm": 0.230373814702034,
      "learning_rate": 4.813067367415194e-05,
      "loss": 0.1989,
      "step": 315
    },
    {
      "epoch": 0.11324135459595054,
      "grad_norm": 0.19817309081554413,
      "learning_rate": 4.8124701385570956e-05,
      "loss": 0.1559,
      "step": 316
    },
    {
      "epoch": 0.11359971331302635,
      "grad_norm": 0.2728217542171478,
      "learning_rate": 4.8118729096989965e-05,
      "loss": 0.1675,
      "step": 317
    },
    {
      "epoch": 0.11395807203010214,
      "grad_norm": 0.21605871617794037,
      "learning_rate": 4.811275680840898e-05,
      "loss": 0.1672,
      "step": 318
    },
    {
      "epoch": 0.11431643074717793,
      "grad_norm": 0.257455974817276,
      "learning_rate": 4.8106784519828004e-05,
      "loss": 0.1887,
      "step": 319
    },
    {
      "epoch": 0.11467478946425372,
      "grad_norm": 0.23033775389194489,
      "learning_rate": 4.810081223124702e-05,
      "loss": 0.1871,
      "step": 320
    },
    {
      "epoch": 0.11503314818132951,
      "grad_norm": 0.32298317551612854,
      "learning_rate": 4.809483994266603e-05,
      "loss": 0.298,
      "step": 321
    },
    {
      "epoch": 0.1153915068984053,
      "grad_norm": 0.16458848118782043,
      "learning_rate": 4.8088867654085045e-05,
      "loss": 0.1967,
      "step": 322
    },
    {
      "epoch": 0.11574986561548109,
      "grad_norm": 0.3014976680278778,
      "learning_rate": 4.808289536550407e-05,
      "loss": 0.1571,
      "step": 323
    },
    {
      "epoch": 0.1161082243325569,
      "grad_norm": 0.1348830908536911,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 0.1168,
      "step": 324
    },
    {
      "epoch": 0.11646658304963269,
      "grad_norm": 0.237985759973526,
      "learning_rate": 4.807095078834209e-05,
      "loss": 0.1853,
      "step": 325
    },
    {
      "epoch": 0.11682494176670848,
      "grad_norm": 0.266922265291214,
      "learning_rate": 4.806497849976111e-05,
      "loss": 0.2404,
      "step": 326
    },
    {
      "epoch": 0.11718330048378427,
      "grad_norm": 0.1841922104358673,
      "learning_rate": 4.8059006211180125e-05,
      "loss": 0.1939,
      "step": 327
    },
    {
      "epoch": 0.11754165920086006,
      "grad_norm": 0.17936623096466064,
      "learning_rate": 4.805303392259915e-05,
      "loss": 0.1604,
      "step": 328
    },
    {
      "epoch": 0.11790001791793585,
      "grad_norm": 0.12957172095775604,
      "learning_rate": 4.804706163401816e-05,
      "loss": 0.1186,
      "step": 329
    },
    {
      "epoch": 0.11825837663501165,
      "grad_norm": 0.21150729060173035,
      "learning_rate": 4.804108934543717e-05,
      "loss": 0.1857,
      "step": 330
    },
    {
      "epoch": 0.11861673535208744,
      "grad_norm": 0.17973606288433075,
      "learning_rate": 4.803511705685619e-05,
      "loss": 0.1848,
      "step": 331
    },
    {
      "epoch": 0.11897509406916323,
      "grad_norm": 0.2263954132795334,
      "learning_rate": 4.8029144768275205e-05,
      "loss": 0.1793,
      "step": 332
    },
    {
      "epoch": 0.11933345278623902,
      "grad_norm": 0.20646104216575623,
      "learning_rate": 4.802317247969422e-05,
      "loss": 0.2051,
      "step": 333
    },
    {
      "epoch": 0.11969181150331482,
      "grad_norm": 0.15283742547035217,
      "learning_rate": 4.801720019111324e-05,
      "loss": 0.1028,
      "step": 334
    },
    {
      "epoch": 0.1200501702203906,
      "grad_norm": 0.14966009557247162,
      "learning_rate": 4.801122790253225e-05,
      "loss": 0.0917,
      "step": 335
    },
    {
      "epoch": 0.1204085289374664,
      "grad_norm": 0.18084414303302765,
      "learning_rate": 4.800525561395127e-05,
      "loss": 0.1592,
      "step": 336
    },
    {
      "epoch": 0.1207668876545422,
      "grad_norm": 0.16464100778102875,
      "learning_rate": 4.7999283325370285e-05,
      "loss": 0.1506,
      "step": 337
    },
    {
      "epoch": 0.12112524637161799,
      "grad_norm": 0.2255524843931198,
      "learning_rate": 4.79933110367893e-05,
      "loss": 0.2078,
      "step": 338
    },
    {
      "epoch": 0.12148360508869378,
      "grad_norm": 0.1961214244365692,
      "learning_rate": 4.798733874820832e-05,
      "loss": 0.1163,
      "step": 339
    },
    {
      "epoch": 0.12184196380576957,
      "grad_norm": 0.2758529484272003,
      "learning_rate": 4.798136645962733e-05,
      "loss": 0.2671,
      "step": 340
    },
    {
      "epoch": 0.12220032252284536,
      "grad_norm": 0.15619142353534698,
      "learning_rate": 4.797539417104634e-05,
      "loss": 0.1332,
      "step": 341
    },
    {
      "epoch": 0.12255868123992116,
      "grad_norm": 0.22554323077201843,
      "learning_rate": 4.7969421882465365e-05,
      "loss": 0.1316,
      "step": 342
    },
    {
      "epoch": 0.12291703995699696,
      "grad_norm": 0.16453111171722412,
      "learning_rate": 4.796344959388438e-05,
      "loss": 0.1355,
      "step": 343
    },
    {
      "epoch": 0.12327539867407275,
      "grad_norm": 0.1991758644580841,
      "learning_rate": 4.79574773053034e-05,
      "loss": 0.1129,
      "step": 344
    },
    {
      "epoch": 0.12363375739114854,
      "grad_norm": 0.19335108995437622,
      "learning_rate": 4.7951505016722406e-05,
      "loss": 0.1869,
      "step": 345
    },
    {
      "epoch": 0.12399211610822433,
      "grad_norm": 0.18022440373897552,
      "learning_rate": 4.794553272814142e-05,
      "loss": 0.1942,
      "step": 346
    },
    {
      "epoch": 0.12435047482530012,
      "grad_norm": 0.22600328922271729,
      "learning_rate": 4.7939560439560445e-05,
      "loss": 0.1735,
      "step": 347
    },
    {
      "epoch": 0.12470883354237591,
      "grad_norm": 0.2608330249786377,
      "learning_rate": 4.793358815097946e-05,
      "loss": 0.2206,
      "step": 348
    },
    {
      "epoch": 0.1250671922594517,
      "grad_norm": 0.23099298775196075,
      "learning_rate": 4.792761586239847e-05,
      "loss": 0.2199,
      "step": 349
    },
    {
      "epoch": 0.1254255509765275,
      "grad_norm": 0.2363959550857544,
      "learning_rate": 4.7921643573817486e-05,
      "loss": 0.2274,
      "step": 350
    },
    {
      "epoch": 0.12578390969360329,
      "grad_norm": 0.20946656167507172,
      "learning_rate": 4.791567128523651e-05,
      "loss": 0.1447,
      "step": 351
    },
    {
      "epoch": 0.12614226841067908,
      "grad_norm": 0.26059529185295105,
      "learning_rate": 4.7909698996655525e-05,
      "loss": 0.2692,
      "step": 352
    },
    {
      "epoch": 0.1265006271277549,
      "grad_norm": 0.21267260611057281,
      "learning_rate": 4.7903726708074534e-05,
      "loss": 0.2167,
      "step": 353
    },
    {
      "epoch": 0.12685898584483069,
      "grad_norm": 0.19998577237129211,
      "learning_rate": 4.789775441949355e-05,
      "loss": 0.1897,
      "step": 354
    },
    {
      "epoch": 0.12721734456190648,
      "grad_norm": 0.1707315891981125,
      "learning_rate": 4.7891782130912566e-05,
      "loss": 0.1461,
      "step": 355
    },
    {
      "epoch": 0.12757570327898227,
      "grad_norm": 0.20767273008823395,
      "learning_rate": 4.788580984233159e-05,
      "loss": 0.1672,
      "step": 356
    },
    {
      "epoch": 0.12793406199605806,
      "grad_norm": 0.18104779720306396,
      "learning_rate": 4.78798375537506e-05,
      "loss": 0.1926,
      "step": 357
    },
    {
      "epoch": 0.12829242071313385,
      "grad_norm": 0.17748454213142395,
      "learning_rate": 4.7873865265169614e-05,
      "loss": 0.1226,
      "step": 358
    },
    {
      "epoch": 0.12865077943020964,
      "grad_norm": 0.18670439720153809,
      "learning_rate": 4.786789297658863e-05,
      "loss": 0.1066,
      "step": 359
    },
    {
      "epoch": 0.12900913814728543,
      "grad_norm": 0.15073427557945251,
      "learning_rate": 4.7861920688007646e-05,
      "loss": 0.1462,
      "step": 360
    },
    {
      "epoch": 0.12936749686436122,
      "grad_norm": 0.21025782823562622,
      "learning_rate": 4.785594839942666e-05,
      "loss": 0.2457,
      "step": 361
    },
    {
      "epoch": 0.129725855581437,
      "grad_norm": 0.17313922941684723,
      "learning_rate": 4.784997611084568e-05,
      "loss": 0.1694,
      "step": 362
    },
    {
      "epoch": 0.1300842142985128,
      "grad_norm": 0.14352422952651978,
      "learning_rate": 4.7844003822264694e-05,
      "loss": 0.1296,
      "step": 363
    },
    {
      "epoch": 0.1304425730155886,
      "grad_norm": 0.15288153290748596,
      "learning_rate": 4.783803153368371e-05,
      "loss": 0.1047,
      "step": 364
    },
    {
      "epoch": 0.1308009317326644,
      "grad_norm": 0.14427630603313446,
      "learning_rate": 4.7832059245102726e-05,
      "loss": 0.1241,
      "step": 365
    },
    {
      "epoch": 0.1311592904497402,
      "grad_norm": 0.2957625985145569,
      "learning_rate": 4.782608695652174e-05,
      "loss": 0.2786,
      "step": 366
    },
    {
      "epoch": 0.131517649166816,
      "grad_norm": 0.2032882273197174,
      "learning_rate": 4.782011466794076e-05,
      "loss": 0.137,
      "step": 367
    },
    {
      "epoch": 0.13187600788389178,
      "grad_norm": 0.1646978110074997,
      "learning_rate": 4.7814142379359774e-05,
      "loss": 0.1553,
      "step": 368
    },
    {
      "epoch": 0.13223436660096757,
      "grad_norm": 0.27491581439971924,
      "learning_rate": 4.780817009077879e-05,
      "loss": 0.2826,
      "step": 369
    },
    {
      "epoch": 0.13259272531804336,
      "grad_norm": 0.17466263473033905,
      "learning_rate": 4.7802197802197806e-05,
      "loss": 0.1528,
      "step": 370
    },
    {
      "epoch": 0.13295108403511915,
      "grad_norm": 0.43430331349372864,
      "learning_rate": 4.779622551361682e-05,
      "loss": 0.3166,
      "step": 371
    },
    {
      "epoch": 0.13330944275219495,
      "grad_norm": 0.1756991446018219,
      "learning_rate": 4.779025322503584e-05,
      "loss": 0.1947,
      "step": 372
    },
    {
      "epoch": 0.13366780146927074,
      "grad_norm": 0.17526891827583313,
      "learning_rate": 4.7784280936454854e-05,
      "loss": 0.1425,
      "step": 373
    },
    {
      "epoch": 0.13402616018634653,
      "grad_norm": 0.21853409707546234,
      "learning_rate": 4.777830864787386e-05,
      "loss": 0.2249,
      "step": 374
    },
    {
      "epoch": 0.13438451890342232,
      "grad_norm": 0.28267672657966614,
      "learning_rate": 4.7772336359292886e-05,
      "loss": 0.1807,
      "step": 375
    },
    {
      "epoch": 0.1347428776204981,
      "grad_norm": 0.1384216547012329,
      "learning_rate": 4.77663640707119e-05,
      "loss": 0.13,
      "step": 376
    },
    {
      "epoch": 0.1351012363375739,
      "grad_norm": 0.21759532392024994,
      "learning_rate": 4.776039178213092e-05,
      "loss": 0.2057,
      "step": 377
    },
    {
      "epoch": 0.13545959505464972,
      "grad_norm": 0.20485736429691315,
      "learning_rate": 4.775441949354993e-05,
      "loss": 0.229,
      "step": 378
    },
    {
      "epoch": 0.1358179537717255,
      "grad_norm": 0.2138381004333496,
      "learning_rate": 4.774844720496895e-05,
      "loss": 0.2429,
      "step": 379
    },
    {
      "epoch": 0.1361763124888013,
      "grad_norm": 0.16025729477405548,
      "learning_rate": 4.7742474916387966e-05,
      "loss": 0.1126,
      "step": 380
    },
    {
      "epoch": 0.1365346712058771,
      "grad_norm": 0.16794805228710175,
      "learning_rate": 4.773650262780698e-05,
      "loss": 0.1498,
      "step": 381
    },
    {
      "epoch": 0.13689302992295288,
      "grad_norm": 0.1725483387708664,
      "learning_rate": 4.773053033922599e-05,
      "loss": 0.1675,
      "step": 382
    },
    {
      "epoch": 0.13725138864002867,
      "grad_norm": 0.20082011818885803,
      "learning_rate": 4.772455805064501e-05,
      "loss": 0.1805,
      "step": 383
    },
    {
      "epoch": 0.13760974735710446,
      "grad_norm": 0.18548794090747833,
      "learning_rate": 4.771858576206403e-05,
      "loss": 0.1302,
      "step": 384
    },
    {
      "epoch": 0.13796810607418025,
      "grad_norm": 0.2254781424999237,
      "learning_rate": 4.771261347348304e-05,
      "loss": 0.2523,
      "step": 385
    },
    {
      "epoch": 0.13832646479125604,
      "grad_norm": 0.15692943334579468,
      "learning_rate": 4.7706641184902055e-05,
      "loss": 0.1097,
      "step": 386
    },
    {
      "epoch": 0.13868482350833183,
      "grad_norm": 0.18153534829616547,
      "learning_rate": 4.770066889632107e-05,
      "loss": 0.2042,
      "step": 387
    },
    {
      "epoch": 0.13904318222540762,
      "grad_norm": 0.16482952237129211,
      "learning_rate": 4.769469660774009e-05,
      "loss": 0.1323,
      "step": 388
    },
    {
      "epoch": 0.13940154094248342,
      "grad_norm": 0.1528128683567047,
      "learning_rate": 4.76887243191591e-05,
      "loss": 0.1273,
      "step": 389
    },
    {
      "epoch": 0.1397598996595592,
      "grad_norm": 0.14421212673187256,
      "learning_rate": 4.768275203057812e-05,
      "loss": 0.1518,
      "step": 390
    },
    {
      "epoch": 0.14011825837663502,
      "grad_norm": 0.2990242838859558,
      "learning_rate": 4.7676779741997135e-05,
      "loss": 0.3051,
      "step": 391
    },
    {
      "epoch": 0.14047661709371081,
      "grad_norm": 0.28184157609939575,
      "learning_rate": 4.767080745341615e-05,
      "loss": 0.2674,
      "step": 392
    },
    {
      "epoch": 0.1408349758107866,
      "grad_norm": 0.1565357893705368,
      "learning_rate": 4.766483516483517e-05,
      "loss": 0.1385,
      "step": 393
    },
    {
      "epoch": 0.1411933345278624,
      "grad_norm": 0.14436431229114532,
      "learning_rate": 4.765886287625418e-05,
      "loss": 0.1117,
      "step": 394
    },
    {
      "epoch": 0.1415516932449382,
      "grad_norm": 0.20991404354572296,
      "learning_rate": 4.76528905876732e-05,
      "loss": 0.185,
      "step": 395
    },
    {
      "epoch": 0.14191005196201398,
      "grad_norm": 0.19987134635448456,
      "learning_rate": 4.7646918299092215e-05,
      "loss": 0.1643,
      "step": 396
    },
    {
      "epoch": 0.14226841067908977,
      "grad_norm": 0.1443943977355957,
      "learning_rate": 4.764094601051123e-05,
      "loss": 0.1383,
      "step": 397
    },
    {
      "epoch": 0.14262676939616556,
      "grad_norm": 0.1945263147354126,
      "learning_rate": 4.763497372193025e-05,
      "loss": 0.1892,
      "step": 398
    },
    {
      "epoch": 0.14298512811324135,
      "grad_norm": 0.1547277569770813,
      "learning_rate": 4.762900143334926e-05,
      "loss": 0.1579,
      "step": 399
    },
    {
      "epoch": 0.14334348683031714,
      "grad_norm": 0.14752888679504395,
      "learning_rate": 4.762302914476828e-05,
      "loss": 0.1333,
      "step": 400
    },
    {
      "epoch": 0.14370184554739293,
      "grad_norm": 0.18747912347316742,
      "learning_rate": 4.7617056856187295e-05,
      "loss": 0.1453,
      "step": 401
    },
    {
      "epoch": 0.14406020426446872,
      "grad_norm": 0.20999163389205933,
      "learning_rate": 4.7611084567606304e-05,
      "loss": 0.1649,
      "step": 402
    },
    {
      "epoch": 0.1444185629815445,
      "grad_norm": 0.1916685849428177,
      "learning_rate": 4.7605112279025327e-05,
      "loss": 0.1797,
      "step": 403
    },
    {
      "epoch": 0.14477692169862033,
      "grad_norm": 0.2967888414859772,
      "learning_rate": 4.759913999044434e-05,
      "loss": 0.316,
      "step": 404
    },
    {
      "epoch": 0.14513528041569612,
      "grad_norm": 0.18200621008872986,
      "learning_rate": 4.759316770186336e-05,
      "loss": 0.154,
      "step": 405
    },
    {
      "epoch": 0.1454936391327719,
      "grad_norm": 0.181245356798172,
      "learning_rate": 4.758719541328237e-05,
      "loss": 0.199,
      "step": 406
    },
    {
      "epoch": 0.1458519978498477,
      "grad_norm": 0.2086484581232071,
      "learning_rate": 4.7581223124701384e-05,
      "loss": 0.2249,
      "step": 407
    },
    {
      "epoch": 0.1462103565669235,
      "grad_norm": 0.17961135506629944,
      "learning_rate": 4.7575250836120407e-05,
      "loss": 0.1381,
      "step": 408
    },
    {
      "epoch": 0.14656871528399928,
      "grad_norm": 0.1613384336233139,
      "learning_rate": 4.756927854753942e-05,
      "loss": 0.117,
      "step": 409
    },
    {
      "epoch": 0.14692707400107508,
      "grad_norm": 0.21244969964027405,
      "learning_rate": 4.756330625895843e-05,
      "loss": 0.1869,
      "step": 410
    },
    {
      "epoch": 0.14728543271815087,
      "grad_norm": 0.20856021344661713,
      "learning_rate": 4.755733397037745e-05,
      "loss": 0.1847,
      "step": 411
    },
    {
      "epoch": 0.14764379143522666,
      "grad_norm": 0.1654929220676422,
      "learning_rate": 4.755136168179647e-05,
      "loss": 0.161,
      "step": 412
    },
    {
      "epoch": 0.14800215015230245,
      "grad_norm": 0.23197486996650696,
      "learning_rate": 4.7545389393215487e-05,
      "loss": 0.2293,
      "step": 413
    },
    {
      "epoch": 0.14836050886937824,
      "grad_norm": 0.15061546862125397,
      "learning_rate": 4.7539417104634496e-05,
      "loss": 0.0827,
      "step": 414
    },
    {
      "epoch": 0.14871886758645403,
      "grad_norm": 0.19336839020252228,
      "learning_rate": 4.753344481605351e-05,
      "loss": 0.1853,
      "step": 415
    },
    {
      "epoch": 0.14907722630352982,
      "grad_norm": 0.20004305243492126,
      "learning_rate": 4.752747252747253e-05,
      "loss": 0.1379,
      "step": 416
    },
    {
      "epoch": 0.14943558502060564,
      "grad_norm": 0.22204972803592682,
      "learning_rate": 4.752150023889155e-05,
      "loss": 0.2011,
      "step": 417
    },
    {
      "epoch": 0.14979394373768143,
      "grad_norm": 0.2779114842414856,
      "learning_rate": 4.751552795031056e-05,
      "loss": 0.3159,
      "step": 418
    },
    {
      "epoch": 0.15015230245475722,
      "grad_norm": 0.15340526401996613,
      "learning_rate": 4.7509555661729576e-05,
      "loss": 0.1124,
      "step": 419
    },
    {
      "epoch": 0.150510661171833,
      "grad_norm": 0.17041608691215515,
      "learning_rate": 4.750358337314859e-05,
      "loss": 0.1638,
      "step": 420
    },
    {
      "epoch": 0.1508690198889088,
      "grad_norm": 0.16783905029296875,
      "learning_rate": 4.749761108456761e-05,
      "loss": 0.1464,
      "step": 421
    },
    {
      "epoch": 0.1512273786059846,
      "grad_norm": 0.18996158242225647,
      "learning_rate": 4.7491638795986624e-05,
      "loss": 0.1266,
      "step": 422
    },
    {
      "epoch": 0.15158573732306038,
      "grad_norm": 0.16355761885643005,
      "learning_rate": 4.748566650740564e-05,
      "loss": 0.0889,
      "step": 423
    },
    {
      "epoch": 0.15194409604013617,
      "grad_norm": 0.2580919563770294,
      "learning_rate": 4.7479694218824656e-05,
      "loss": 0.1874,
      "step": 424
    },
    {
      "epoch": 0.15230245475721196,
      "grad_norm": 0.1683245152235031,
      "learning_rate": 4.747372193024367e-05,
      "loss": 0.1412,
      "step": 425
    },
    {
      "epoch": 0.15266081347428775,
      "grad_norm": 0.202839657664299,
      "learning_rate": 4.746774964166269e-05,
      "loss": 0.2301,
      "step": 426
    },
    {
      "epoch": 0.15301917219136354,
      "grad_norm": 0.16661031544208527,
      "learning_rate": 4.7461777353081704e-05,
      "loss": 0.1249,
      "step": 427
    },
    {
      "epoch": 0.15337753090843934,
      "grad_norm": 0.16195149719715118,
      "learning_rate": 4.745580506450072e-05,
      "loss": 0.1349,
      "step": 428
    },
    {
      "epoch": 0.15373588962551515,
      "grad_norm": 0.18144017457962036,
      "learning_rate": 4.7449832775919736e-05,
      "loss": 0.1423,
      "step": 429
    },
    {
      "epoch": 0.15409424834259094,
      "grad_norm": 0.26362088322639465,
      "learning_rate": 4.744386048733875e-05,
      "loss": 0.3048,
      "step": 430
    },
    {
      "epoch": 0.15445260705966674,
      "grad_norm": 0.18950216472148895,
      "learning_rate": 4.743788819875777e-05,
      "loss": 0.1695,
      "step": 431
    },
    {
      "epoch": 0.15481096577674253,
      "grad_norm": 0.2044963240623474,
      "learning_rate": 4.7431915910176784e-05,
      "loss": 0.1787,
      "step": 432
    },
    {
      "epoch": 0.15516932449381832,
      "grad_norm": 0.24110156297683716,
      "learning_rate": 4.74259436215958e-05,
      "loss": 0.2179,
      "step": 433
    },
    {
      "epoch": 0.1555276832108941,
      "grad_norm": 0.1528530865907669,
      "learning_rate": 4.741997133301481e-05,
      "loss": 0.1865,
      "step": 434
    },
    {
      "epoch": 0.1558860419279699,
      "grad_norm": 0.22697749733924866,
      "learning_rate": 4.7413999044433825e-05,
      "loss": 0.2093,
      "step": 435
    },
    {
      "epoch": 0.1562444006450457,
      "grad_norm": 0.14982664585113525,
      "learning_rate": 4.740802675585285e-05,
      "loss": 0.1548,
      "step": 436
    },
    {
      "epoch": 0.15660275936212148,
      "grad_norm": 0.30597826838493347,
      "learning_rate": 4.7402054467271864e-05,
      "loss": 0.3103,
      "step": 437
    },
    {
      "epoch": 0.15696111807919727,
      "grad_norm": 0.19238965213298798,
      "learning_rate": 4.739608217869087e-05,
      "loss": 0.1754,
      "step": 438
    },
    {
      "epoch": 0.15731947679627306,
      "grad_norm": 0.15320934355258942,
      "learning_rate": 4.739010989010989e-05,
      "loss": 0.1057,
      "step": 439
    },
    {
      "epoch": 0.15767783551334885,
      "grad_norm": 0.2629055976867676,
      "learning_rate": 4.738413760152891e-05,
      "loss": 0.167,
      "step": 440
    },
    {
      "epoch": 0.15803619423042464,
      "grad_norm": 0.16580288112163544,
      "learning_rate": 4.737816531294793e-05,
      "loss": 0.1612,
      "step": 441
    },
    {
      "epoch": 0.15839455294750046,
      "grad_norm": 0.19626162946224213,
      "learning_rate": 4.737219302436694e-05,
      "loss": 0.1369,
      "step": 442
    },
    {
      "epoch": 0.15875291166457625,
      "grad_norm": 0.31162938475608826,
      "learning_rate": 4.736622073578595e-05,
      "loss": 0.167,
      "step": 443
    },
    {
      "epoch": 0.15911127038165204,
      "grad_norm": 0.21923336386680603,
      "learning_rate": 4.736024844720497e-05,
      "loss": 0.2068,
      "step": 444
    },
    {
      "epoch": 0.15946962909872783,
      "grad_norm": 0.1923987865447998,
      "learning_rate": 4.735427615862399e-05,
      "loss": 0.1163,
      "step": 445
    },
    {
      "epoch": 0.15982798781580362,
      "grad_norm": 0.15880361199378967,
      "learning_rate": 4.7348303870043e-05,
      "loss": 0.1347,
      "step": 446
    },
    {
      "epoch": 0.16018634653287941,
      "grad_norm": 0.17735373973846436,
      "learning_rate": 4.734233158146202e-05,
      "loss": 0.1474,
      "step": 447
    },
    {
      "epoch": 0.1605447052499552,
      "grad_norm": 0.18092595040798187,
      "learning_rate": 4.733635929288103e-05,
      "loss": 0.1414,
      "step": 448
    },
    {
      "epoch": 0.160903063967031,
      "grad_norm": 0.1827719509601593,
      "learning_rate": 4.733038700430005e-05,
      "loss": 0.1914,
      "step": 449
    },
    {
      "epoch": 0.1612614226841068,
      "grad_norm": 0.21096307039260864,
      "learning_rate": 4.7324414715719065e-05,
      "loss": 0.1793,
      "step": 450
    },
    {
      "epoch": 0.16161978140118258,
      "grad_norm": 0.22954261302947998,
      "learning_rate": 4.731844242713808e-05,
      "loss": 0.1875,
      "step": 451
    },
    {
      "epoch": 0.16197814011825837,
      "grad_norm": 0.15404082834720612,
      "learning_rate": 4.7312470138557097e-05,
      "loss": 0.1153,
      "step": 452
    },
    {
      "epoch": 0.16233649883533416,
      "grad_norm": 0.2325499951839447,
      "learning_rate": 4.730649784997611e-05,
      "loss": 0.188,
      "step": 453
    },
    {
      "epoch": 0.16269485755240995,
      "grad_norm": 0.22255609929561615,
      "learning_rate": 4.730052556139513e-05,
      "loss": 0.1647,
      "step": 454
    },
    {
      "epoch": 0.16305321626948577,
      "grad_norm": 0.2782913148403168,
      "learning_rate": 4.7294553272814145e-05,
      "loss": 0.2246,
      "step": 455
    },
    {
      "epoch": 0.16341157498656156,
      "grad_norm": 0.28007984161376953,
      "learning_rate": 4.728858098423316e-05,
      "loss": 0.2598,
      "step": 456
    },
    {
      "epoch": 0.16376993370363735,
      "grad_norm": 0.23850944638252258,
      "learning_rate": 4.7282608695652177e-05,
      "loss": 0.2267,
      "step": 457
    },
    {
      "epoch": 0.16412829242071314,
      "grad_norm": 0.20311163365840912,
      "learning_rate": 4.727663640707119e-05,
      "loss": 0.1876,
      "step": 458
    },
    {
      "epoch": 0.16448665113778893,
      "grad_norm": 0.15122289955615997,
      "learning_rate": 4.727066411849021e-05,
      "loss": 0.1348,
      "step": 459
    },
    {
      "epoch": 0.16484500985486472,
      "grad_norm": 0.1903991550207138,
      "learning_rate": 4.7264691829909225e-05,
      "loss": 0.2441,
      "step": 460
    },
    {
      "epoch": 0.1652033685719405,
      "grad_norm": 0.20918703079223633,
      "learning_rate": 4.725871954132824e-05,
      "loss": 0.1862,
      "step": 461
    },
    {
      "epoch": 0.1655617272890163,
      "grad_norm": 0.13650277256965637,
      "learning_rate": 4.7252747252747257e-05,
      "loss": 0.1211,
      "step": 462
    },
    {
      "epoch": 0.1659200860060921,
      "grad_norm": 0.16657856106758118,
      "learning_rate": 4.7246774964166266e-05,
      "loss": 0.1187,
      "step": 463
    },
    {
      "epoch": 0.16627844472316788,
      "grad_norm": 0.20941855013370514,
      "learning_rate": 4.724080267558529e-05,
      "loss": 0.1808,
      "step": 464
    },
    {
      "epoch": 0.16663680344024367,
      "grad_norm": 0.22016799449920654,
      "learning_rate": 4.7234830387004304e-05,
      "loss": 0.2267,
      "step": 465
    },
    {
      "epoch": 0.16699516215731947,
      "grad_norm": 0.24252177774906158,
      "learning_rate": 4.722885809842332e-05,
      "loss": 0.2891,
      "step": 466
    },
    {
      "epoch": 0.16735352087439526,
      "grad_norm": 0.20499445497989655,
      "learning_rate": 4.722288580984233e-05,
      "loss": 0.161,
      "step": 467
    },
    {
      "epoch": 0.16771187959147107,
      "grad_norm": 0.22234024107456207,
      "learning_rate": 4.721691352126135e-05,
      "loss": 0.2021,
      "step": 468
    },
    {
      "epoch": 0.16807023830854687,
      "grad_norm": 0.18736688792705536,
      "learning_rate": 4.721094123268037e-05,
      "loss": 0.1751,
      "step": 469
    },
    {
      "epoch": 0.16842859702562266,
      "grad_norm": 0.15194706618785858,
      "learning_rate": 4.7204968944099384e-05,
      "loss": 0.1286,
      "step": 470
    },
    {
      "epoch": 0.16878695574269845,
      "grad_norm": 0.2493574172258377,
      "learning_rate": 4.7198996655518394e-05,
      "loss": 0.2864,
      "step": 471
    },
    {
      "epoch": 0.16914531445977424,
      "grad_norm": 0.21230362355709076,
      "learning_rate": 4.719302436693741e-05,
      "loss": 0.151,
      "step": 472
    },
    {
      "epoch": 0.16950367317685003,
      "grad_norm": 0.16734963655471802,
      "learning_rate": 4.718705207835643e-05,
      "loss": 0.1637,
      "step": 473
    },
    {
      "epoch": 0.16986203189392582,
      "grad_norm": 0.20784497261047363,
      "learning_rate": 4.718107978977545e-05,
      "loss": 0.1624,
      "step": 474
    },
    {
      "epoch": 0.1702203906110016,
      "grad_norm": 0.25666868686676025,
      "learning_rate": 4.717510750119446e-05,
      "loss": 0.4398,
      "step": 475
    },
    {
      "epoch": 0.1705787493280774,
      "grad_norm": 0.24025797843933105,
      "learning_rate": 4.7169135212613474e-05,
      "loss": 0.2285,
      "step": 476
    },
    {
      "epoch": 0.1709371080451532,
      "grad_norm": 0.2573355734348297,
      "learning_rate": 4.716316292403249e-05,
      "loss": 0.2375,
      "step": 477
    },
    {
      "epoch": 0.17129546676222898,
      "grad_norm": 0.19193996489048004,
      "learning_rate": 4.715719063545151e-05,
      "loss": 0.1545,
      "step": 478
    },
    {
      "epoch": 0.17165382547930477,
      "grad_norm": 0.24866075813770294,
      "learning_rate": 4.715121834687052e-05,
      "loss": 0.1816,
      "step": 479
    },
    {
      "epoch": 0.17201218419638056,
      "grad_norm": 0.25740256905555725,
      "learning_rate": 4.714524605828954e-05,
      "loss": 0.1871,
      "step": 480
    },
    {
      "epoch": 0.17237054291345638,
      "grad_norm": 0.23296897113323212,
      "learning_rate": 4.7139273769708554e-05,
      "loss": 0.2166,
      "step": 481
    },
    {
      "epoch": 0.17272890163053217,
      "grad_norm": 0.17531336843967438,
      "learning_rate": 4.7133301481127576e-05,
      "loss": 0.1468,
      "step": 482
    },
    {
      "epoch": 0.17308726034760796,
      "grad_norm": 0.17735061049461365,
      "learning_rate": 4.7127329192546586e-05,
      "loss": 0.1736,
      "step": 483
    },
    {
      "epoch": 0.17344561906468375,
      "grad_norm": 0.21728408336639404,
      "learning_rate": 4.71213569039656e-05,
      "loss": 0.2066,
      "step": 484
    },
    {
      "epoch": 0.17380397778175954,
      "grad_norm": 0.1600300520658493,
      "learning_rate": 4.711538461538462e-05,
      "loss": 0.1185,
      "step": 485
    },
    {
      "epoch": 0.17416233649883534,
      "grad_norm": 0.22690342366695404,
      "learning_rate": 4.7109412326803634e-05,
      "loss": 0.0987,
      "step": 486
    },
    {
      "epoch": 0.17452069521591113,
      "grad_norm": 0.15538348257541656,
      "learning_rate": 4.710344003822265e-05,
      "loss": 0.1217,
      "step": 487
    },
    {
      "epoch": 0.17487905393298692,
      "grad_norm": 0.17211270332336426,
      "learning_rate": 4.7097467749641665e-05,
      "loss": 0.1208,
      "step": 488
    },
    {
      "epoch": 0.1752374126500627,
      "grad_norm": 0.19915585219860077,
      "learning_rate": 4.709149546106068e-05,
      "loss": 0.1385,
      "step": 489
    },
    {
      "epoch": 0.1755957713671385,
      "grad_norm": 0.2717455327510834,
      "learning_rate": 4.70855231724797e-05,
      "loss": 0.2492,
      "step": 490
    },
    {
      "epoch": 0.1759541300842143,
      "grad_norm": 0.2083529233932495,
      "learning_rate": 4.707955088389871e-05,
      "loss": 0.1454,
      "step": 491
    },
    {
      "epoch": 0.17631248880129008,
      "grad_norm": 0.18208454549312592,
      "learning_rate": 4.707357859531773e-05,
      "loss": 0.1761,
      "step": 492
    },
    {
      "epoch": 0.1766708475183659,
      "grad_norm": 0.2131224274635315,
      "learning_rate": 4.7067606306736745e-05,
      "loss": 0.1234,
      "step": 493
    },
    {
      "epoch": 0.1770292062354417,
      "grad_norm": 0.18862146139144897,
      "learning_rate": 4.706163401815576e-05,
      "loss": 0.1909,
      "step": 494
    },
    {
      "epoch": 0.17738756495251748,
      "grad_norm": 0.2309047281742096,
      "learning_rate": 4.705566172957477e-05,
      "loss": 0.1944,
      "step": 495
    },
    {
      "epoch": 0.17774592366959327,
      "grad_norm": 0.27982571721076965,
      "learning_rate": 4.7049689440993793e-05,
      "loss": 0.1994,
      "step": 496
    },
    {
      "epoch": 0.17810428238666906,
      "grad_norm": 0.166084885597229,
      "learning_rate": 4.704371715241281e-05,
      "loss": 0.1206,
      "step": 497
    },
    {
      "epoch": 0.17846264110374485,
      "grad_norm": 0.20303687453269958,
      "learning_rate": 4.7037744863831825e-05,
      "loss": 0.1928,
      "step": 498
    },
    {
      "epoch": 0.17882099982082064,
      "grad_norm": 0.14494453370571136,
      "learning_rate": 4.7031772575250835e-05,
      "loss": 0.1208,
      "step": 499
    },
    {
      "epoch": 0.17917935853789643,
      "grad_norm": 0.1485375463962555,
      "learning_rate": 4.702580028666985e-05,
      "loss": 0.1111,
      "step": 500
    },
    {
      "epoch": 0.17953771725497222,
      "grad_norm": 0.22027017176151276,
      "learning_rate": 4.701982799808887e-05,
      "loss": 0.2447,
      "step": 501
    },
    {
      "epoch": 0.17989607597204801,
      "grad_norm": 0.28344494104385376,
      "learning_rate": 4.701385570950789e-05,
      "loss": 0.2445,
      "step": 502
    },
    {
      "epoch": 0.1802544346891238,
      "grad_norm": 0.28447410464286804,
      "learning_rate": 4.70078834209269e-05,
      "loss": 0.3135,
      "step": 503
    },
    {
      "epoch": 0.1806127934061996,
      "grad_norm": 0.20360811054706573,
      "learning_rate": 4.7001911132345915e-05,
      "loss": 0.193,
      "step": 504
    },
    {
      "epoch": 0.1809711521232754,
      "grad_norm": 0.20095843076705933,
      "learning_rate": 4.699593884376493e-05,
      "loss": 0.2119,
      "step": 505
    },
    {
      "epoch": 0.1813295108403512,
      "grad_norm": 0.1700664460659027,
      "learning_rate": 4.698996655518395e-05,
      "loss": 0.1604,
      "step": 506
    },
    {
      "epoch": 0.181687869557427,
      "grad_norm": 0.22283320128917694,
      "learning_rate": 4.698399426660296e-05,
      "loss": 0.1537,
      "step": 507
    },
    {
      "epoch": 0.18204622827450279,
      "grad_norm": 0.1724284142255783,
      "learning_rate": 4.697802197802198e-05,
      "loss": 0.1466,
      "step": 508
    },
    {
      "epoch": 0.18240458699157858,
      "grad_norm": 0.21433447301387787,
      "learning_rate": 4.6972049689440995e-05,
      "loss": 0.173,
      "step": 509
    },
    {
      "epoch": 0.18276294570865437,
      "grad_norm": 0.1879352182149887,
      "learning_rate": 4.696607740086001e-05,
      "loss": 0.1712,
      "step": 510
    }
  ],
  "logging_steps": 1,
  "max_steps": 8373,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.767912637464576e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
