{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 255,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 19.71453857421875,
      "learning_rate": 0.0,
      "loss": 17.0568,
      "step": 1
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 19.253700256347656,
      "learning_rate": 5e-05,
      "loss": 16.9121,
      "step": 2
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 19.999454498291016,
      "learning_rate": 4.9803149606299216e-05,
      "loss": 16.8206,
      "step": 3
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 21.963058471679688,
      "learning_rate": 4.960629921259843e-05,
      "loss": 16.5012,
      "step": 4
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 22.274314880371094,
      "learning_rate": 4.940944881889764e-05,
      "loss": 16.1286,
      "step": 5
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 24.613849639892578,
      "learning_rate": 4.9212598425196856e-05,
      "loss": 15.6734,
      "step": 6
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 27.666152954101562,
      "learning_rate": 4.901574803149607e-05,
      "loss": 15.3158,
      "step": 7
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 31.723876953125,
      "learning_rate": 4.881889763779528e-05,
      "loss": 14.843,
      "step": 8
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 32.3161506652832,
      "learning_rate": 4.862204724409449e-05,
      "loss": 14.1063,
      "step": 9
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 36.14498519897461,
      "learning_rate": 4.84251968503937e-05,
      "loss": 13.4385,
      "step": 10
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 40.220123291015625,
      "learning_rate": 4.822834645669291e-05,
      "loss": 12.7488,
      "step": 11
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 43.16788101196289,
      "learning_rate": 4.8031496062992124e-05,
      "loss": 11.9614,
      "step": 12
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 49.02158737182617,
      "learning_rate": 4.783464566929134e-05,
      "loss": 11.1083,
      "step": 13
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 49.61189651489258,
      "learning_rate": 4.763779527559055e-05,
      "loss": 10.329,
      "step": 14
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 53.87152099609375,
      "learning_rate": 4.7440944881889765e-05,
      "loss": 9.0408,
      "step": 15
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 59.601837158203125,
      "learning_rate": 4.724409448818898e-05,
      "loss": 7.9309,
      "step": 16
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 64.16035461425781,
      "learning_rate": 4.704724409448819e-05,
      "loss": 6.6753,
      "step": 17
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 69.17866516113281,
      "learning_rate": 4.6850393700787405e-05,
      "loss": 5.5471,
      "step": 18
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 69.24131774902344,
      "learning_rate": 4.665354330708662e-05,
      "loss": 4.3393,
      "step": 19
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 58.76420593261719,
      "learning_rate": 4.645669291338583e-05,
      "loss": 2.3103,
      "step": 20
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 46.889381408691406,
      "learning_rate": 4.6259842519685046e-05,
      "loss": 1.6196,
      "step": 21
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 29.07752227783203,
      "learning_rate": 4.606299212598425e-05,
      "loss": 0.9544,
      "step": 22
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 19.701187133789062,
      "learning_rate": 4.5866141732283466e-05,
      "loss": 0.7278,
      "step": 23
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 5.173194408416748,
      "learning_rate": 4.566929133858268e-05,
      "loss": 0.441,
      "step": 24
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 2.0216946601867676,
      "learning_rate": 4.547244094488189e-05,
      "loss": 0.3796,
      "step": 25
    },
    {
      "epoch": 0.5148514851485149,
      "grad_norm": 1.1444936990737915,
      "learning_rate": 4.52755905511811e-05,
      "loss": 0.3139,
      "step": 26
    },
    {
      "epoch": 0.5346534653465347,
      "grad_norm": 0.9134384393692017,
      "learning_rate": 4.507874015748031e-05,
      "loss": 0.3322,
      "step": 27
    },
    {
      "epoch": 0.5544554455445545,
      "grad_norm": 0.8866304755210876,
      "learning_rate": 4.488188976377953e-05,
      "loss": 0.3208,
      "step": 28
    },
    {
      "epoch": 0.5742574257425742,
      "grad_norm": 0.6660789847373962,
      "learning_rate": 4.468503937007874e-05,
      "loss": 0.3148,
      "step": 29
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 0.4974392354488373,
      "learning_rate": 4.4488188976377954e-05,
      "loss": 0.2956,
      "step": 30
    },
    {
      "epoch": 0.6138613861386139,
      "grad_norm": 0.41499263048171997,
      "learning_rate": 4.429133858267717e-05,
      "loss": 0.3222,
      "step": 31
    },
    {
      "epoch": 0.6336633663366337,
      "grad_norm": 0.3339206576347351,
      "learning_rate": 4.409448818897638e-05,
      "loss": 0.3048,
      "step": 32
    },
    {
      "epoch": 0.6534653465346535,
      "grad_norm": 0.2535311281681061,
      "learning_rate": 4.3897637795275594e-05,
      "loss": 0.3012,
      "step": 33
    },
    {
      "epoch": 0.6732673267326733,
      "grad_norm": 0.24696612358093262,
      "learning_rate": 4.370078740157481e-05,
      "loss": 0.328,
      "step": 34
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 0.19810286164283752,
      "learning_rate": 4.350393700787402e-05,
      "loss": 0.3236,
      "step": 35
    },
    {
      "epoch": 0.7128712871287128,
      "grad_norm": 0.6652829647064209,
      "learning_rate": 4.330708661417323e-05,
      "loss": 0.306,
      "step": 36
    },
    {
      "epoch": 0.7326732673267327,
      "grad_norm": 0.1857920140028,
      "learning_rate": 4.311023622047244e-05,
      "loss": 0.3239,
      "step": 37
    },
    {
      "epoch": 0.7524752475247525,
      "grad_norm": 0.18203872442245483,
      "learning_rate": 4.2913385826771655e-05,
      "loss": 0.3194,
      "step": 38
    },
    {
      "epoch": 0.7722772277227723,
      "grad_norm": 0.20419485867023468,
      "learning_rate": 4.271653543307087e-05,
      "loss": 0.315,
      "step": 39
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 0.22628575563430786,
      "learning_rate": 4.251968503937008e-05,
      "loss": 0.3115,
      "step": 40
    },
    {
      "epoch": 0.8118811881188119,
      "grad_norm": 0.17898505926132202,
      "learning_rate": 4.2322834645669296e-05,
      "loss": 0.3061,
      "step": 41
    },
    {
      "epoch": 0.8316831683168316,
      "grad_norm": 0.1632712185382843,
      "learning_rate": 4.21259842519685e-05,
      "loss": 0.2882,
      "step": 42
    },
    {
      "epoch": 0.8514851485148515,
      "grad_norm": 0.1575722098350525,
      "learning_rate": 4.1929133858267716e-05,
      "loss": 0.3401,
      "step": 43
    },
    {
      "epoch": 0.8712871287128713,
      "grad_norm": 0.17314085364341736,
      "learning_rate": 4.173228346456693e-05,
      "loss": 0.2735,
      "step": 44
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 0.153128981590271,
      "learning_rate": 4.153543307086614e-05,
      "loss": 0.284,
      "step": 45
    },
    {
      "epoch": 0.9108910891089109,
      "grad_norm": 0.1661253124475479,
      "learning_rate": 4.133858267716536e-05,
      "loss": 0.3231,
      "step": 46
    },
    {
      "epoch": 0.9306930693069307,
      "grad_norm": 0.1769111305475235,
      "learning_rate": 4.114173228346457e-05,
      "loss": 0.3516,
      "step": 47
    },
    {
      "epoch": 0.9504950495049505,
      "grad_norm": 0.15351778268814087,
      "learning_rate": 4.0944881889763784e-05,
      "loss": 0.3056,
      "step": 48
    },
    {
      "epoch": 0.9702970297029703,
      "grad_norm": 0.15017551183700562,
      "learning_rate": 4.074803149606299e-05,
      "loss": 0.3118,
      "step": 49
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 0.1696397364139557,
      "learning_rate": 4.0551181102362204e-05,
      "loss": 0.3171,
      "step": 50
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.17060767114162445,
      "learning_rate": 4.035433070866142e-05,
      "loss": 0.292,
      "step": 51
    },
    {
      "epoch": 1.0198019801980198,
      "grad_norm": 0.17160660028457642,
      "learning_rate": 4.015748031496063e-05,
      "loss": 0.3155,
      "step": 52
    },
    {
      "epoch": 1.0396039603960396,
      "grad_norm": 0.14191998541355133,
      "learning_rate": 3.9960629921259845e-05,
      "loss": 0.284,
      "step": 53
    },
    {
      "epoch": 1.0594059405940595,
      "grad_norm": 0.14254917204380035,
      "learning_rate": 3.976377952755906e-05,
      "loss": 0.2741,
      "step": 54
    },
    {
      "epoch": 1.0792079207920793,
      "grad_norm": 0.1508607119321823,
      "learning_rate": 3.956692913385827e-05,
      "loss": 0.2617,
      "step": 55
    },
    {
      "epoch": 1.099009900990099,
      "grad_norm": 0.14435501396656036,
      "learning_rate": 3.9370078740157485e-05,
      "loss": 0.2879,
      "step": 56
    },
    {
      "epoch": 1.118811881188119,
      "grad_norm": 0.15804214775562286,
      "learning_rate": 3.91732283464567e-05,
      "loss": 0.2902,
      "step": 57
    },
    {
      "epoch": 1.1386138613861387,
      "grad_norm": 0.22109895944595337,
      "learning_rate": 3.8976377952755905e-05,
      "loss": 0.2715,
      "step": 58
    },
    {
      "epoch": 1.1584158415841583,
      "grad_norm": 0.1442597359418869,
      "learning_rate": 3.877952755905512e-05,
      "loss": 0.2986,
      "step": 59
    },
    {
      "epoch": 1.1782178217821782,
      "grad_norm": 0.17902468144893646,
      "learning_rate": 3.858267716535433e-05,
      "loss": 0.2914,
      "step": 60
    },
    {
      "epoch": 1.198019801980198,
      "grad_norm": 0.13956832885742188,
      "learning_rate": 3.8385826771653546e-05,
      "loss": 0.2645,
      "step": 61
    },
    {
      "epoch": 1.2178217821782178,
      "grad_norm": 0.14601925015449524,
      "learning_rate": 3.818897637795276e-05,
      "loss": 0.2825,
      "step": 62
    },
    {
      "epoch": 1.2376237623762376,
      "grad_norm": 0.1427881270647049,
      "learning_rate": 3.7992125984251966e-05,
      "loss": 0.2574,
      "step": 63
    },
    {
      "epoch": 1.2574257425742574,
      "grad_norm": 0.157621368765831,
      "learning_rate": 3.779527559055118e-05,
      "loss": 0.2742,
      "step": 64
    },
    {
      "epoch": 1.2772277227722773,
      "grad_norm": 0.12736189365386963,
      "learning_rate": 3.759842519685039e-05,
      "loss": 0.2669,
      "step": 65
    },
    {
      "epoch": 1.297029702970297,
      "grad_norm": 0.13830077648162842,
      "learning_rate": 3.740157480314961e-05,
      "loss": 0.2899,
      "step": 66
    },
    {
      "epoch": 1.316831683168317,
      "grad_norm": 0.14940917491912842,
      "learning_rate": 3.720472440944882e-05,
      "loss": 0.2877,
      "step": 67
    },
    {
      "epoch": 1.3366336633663367,
      "grad_norm": 0.19400830566883087,
      "learning_rate": 3.7007874015748034e-05,
      "loss": 0.2949,
      "step": 68
    },
    {
      "epoch": 1.3564356435643563,
      "grad_norm": 0.11651231348514557,
      "learning_rate": 3.681102362204725e-05,
      "loss": 0.2772,
      "step": 69
    },
    {
      "epoch": 1.3762376237623761,
      "grad_norm": 0.13781291246414185,
      "learning_rate": 3.661417322834646e-05,
      "loss": 0.2759,
      "step": 70
    },
    {
      "epoch": 1.396039603960396,
      "grad_norm": 0.13886995613574982,
      "learning_rate": 3.6417322834645674e-05,
      "loss": 0.2636,
      "step": 71
    },
    {
      "epoch": 1.4158415841584158,
      "grad_norm": 0.17200873792171478,
      "learning_rate": 3.622047244094489e-05,
      "loss": 0.3079,
      "step": 72
    },
    {
      "epoch": 1.4356435643564356,
      "grad_norm": 0.13948851823806763,
      "learning_rate": 3.60236220472441e-05,
      "loss": 0.2758,
      "step": 73
    },
    {
      "epoch": 1.4554455445544554,
      "grad_norm": 0.15476366877555847,
      "learning_rate": 3.582677165354331e-05,
      "loss": 0.2822,
      "step": 74
    },
    {
      "epoch": 1.4752475247524752,
      "grad_norm": 0.13344408571720123,
      "learning_rate": 3.562992125984252e-05,
      "loss": 0.2638,
      "step": 75
    },
    {
      "epoch": 1.495049504950495,
      "grad_norm": 0.14363287389278412,
      "learning_rate": 3.5433070866141735e-05,
      "loss": 0.2583,
      "step": 76
    },
    {
      "epoch": 1.5148514851485149,
      "grad_norm": 0.12788929045200348,
      "learning_rate": 3.523622047244094e-05,
      "loss": 0.2602,
      "step": 77
    },
    {
      "epoch": 1.5346534653465347,
      "grad_norm": 0.11865593492984772,
      "learning_rate": 3.5039370078740156e-05,
      "loss": 0.2475,
      "step": 78
    },
    {
      "epoch": 1.5544554455445545,
      "grad_norm": 0.1405976563692093,
      "learning_rate": 3.484251968503937e-05,
      "loss": 0.2966,
      "step": 79
    },
    {
      "epoch": 1.5742574257425743,
      "grad_norm": 0.1260933130979538,
      "learning_rate": 3.464566929133858e-05,
      "loss": 0.3056,
      "step": 80
    },
    {
      "epoch": 1.5940594059405941,
      "grad_norm": 0.15959982573986053,
      "learning_rate": 3.4448818897637796e-05,
      "loss": 0.2923,
      "step": 81
    },
    {
      "epoch": 1.613861386138614,
      "grad_norm": 0.18430615961551666,
      "learning_rate": 3.425196850393701e-05,
      "loss": 0.2926,
      "step": 82
    },
    {
      "epoch": 1.6336633663366338,
      "grad_norm": 0.12249752134084702,
      "learning_rate": 3.405511811023622e-05,
      "loss": 0.2679,
      "step": 83
    },
    {
      "epoch": 1.6534653465346536,
      "grad_norm": 0.1593465656042099,
      "learning_rate": 3.385826771653544e-05,
      "loss": 0.2745,
      "step": 84
    },
    {
      "epoch": 1.6732673267326734,
      "grad_norm": 0.1195632740855217,
      "learning_rate": 3.366141732283465e-05,
      "loss": 0.295,
      "step": 85
    },
    {
      "epoch": 1.693069306930693,
      "grad_norm": 0.13620726764202118,
      "learning_rate": 3.3464566929133864e-05,
      "loss": 0.2643,
      "step": 86
    },
    {
      "epoch": 1.7128712871287128,
      "grad_norm": 0.13680380582809448,
      "learning_rate": 3.326771653543308e-05,
      "loss": 0.2753,
      "step": 87
    },
    {
      "epoch": 1.7326732673267327,
      "grad_norm": 0.1251526176929474,
      "learning_rate": 3.3070866141732284e-05,
      "loss": 0.2915,
      "step": 88
    },
    {
      "epoch": 1.7524752475247525,
      "grad_norm": 0.12729936838150024,
      "learning_rate": 3.28740157480315e-05,
      "loss": 0.2667,
      "step": 89
    },
    {
      "epoch": 1.7722772277227723,
      "grad_norm": 0.14303477108478546,
      "learning_rate": 3.2677165354330704e-05,
      "loss": 0.2273,
      "step": 90
    },
    {
      "epoch": 1.7920792079207921,
      "grad_norm": 0.137057363986969,
      "learning_rate": 3.248031496062992e-05,
      "loss": 0.2669,
      "step": 91
    },
    {
      "epoch": 1.811881188118812,
      "grad_norm": 0.15849120914936066,
      "learning_rate": 3.228346456692913e-05,
      "loss": 0.2488,
      "step": 92
    },
    {
      "epoch": 1.8316831683168315,
      "grad_norm": 0.12968683242797852,
      "learning_rate": 3.2086614173228345e-05,
      "loss": 0.2754,
      "step": 93
    },
    {
      "epoch": 1.8514851485148514,
      "grad_norm": 0.15065538883209229,
      "learning_rate": 3.188976377952756e-05,
      "loss": 0.2552,
      "step": 94
    },
    {
      "epoch": 1.8712871287128712,
      "grad_norm": 0.17308245599269867,
      "learning_rate": 3.169291338582677e-05,
      "loss": 0.2547,
      "step": 95
    },
    {
      "epoch": 1.891089108910891,
      "grad_norm": 0.12170065939426422,
      "learning_rate": 3.1496062992125985e-05,
      "loss": 0.2521,
      "step": 96
    },
    {
      "epoch": 1.9108910891089108,
      "grad_norm": 0.1249566301703453,
      "learning_rate": 3.12992125984252e-05,
      "loss": 0.2599,
      "step": 97
    },
    {
      "epoch": 1.9306930693069306,
      "grad_norm": 0.13744047284126282,
      "learning_rate": 3.110236220472441e-05,
      "loss": 0.2261,
      "step": 98
    },
    {
      "epoch": 1.9504950495049505,
      "grad_norm": 0.145630344748497,
      "learning_rate": 3.0905511811023626e-05,
      "loss": 0.2553,
      "step": 99
    },
    {
      "epoch": 1.9702970297029703,
      "grad_norm": 0.1564365029335022,
      "learning_rate": 3.070866141732284e-05,
      "loss": 0.2378,
      "step": 100
    },
    {
      "epoch": 1.99009900990099,
      "grad_norm": 0.19228406250476837,
      "learning_rate": 3.051181102362205e-05,
      "loss": 0.2387,
      "step": 101
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.1708296537399292,
      "learning_rate": 3.0314960629921263e-05,
      "loss": 0.232,
      "step": 102
    },
    {
      "epoch": 2.01980198019802,
      "grad_norm": 0.14083874225616455,
      "learning_rate": 3.0118110236220477e-05,
      "loss": 0.2238,
      "step": 103
    },
    {
      "epoch": 2.0396039603960396,
      "grad_norm": 0.14168477058410645,
      "learning_rate": 2.992125984251969e-05,
      "loss": 0.2594,
      "step": 104
    },
    {
      "epoch": 2.0594059405940595,
      "grad_norm": 0.1785484254360199,
      "learning_rate": 2.97244094488189e-05,
      "loss": 0.2241,
      "step": 105
    },
    {
      "epoch": 2.0792079207920793,
      "grad_norm": 0.16839325428009033,
      "learning_rate": 2.952755905511811e-05,
      "loss": 0.2911,
      "step": 106
    },
    {
      "epoch": 2.099009900990099,
      "grad_norm": 0.1532835215330124,
      "learning_rate": 2.933070866141732e-05,
      "loss": 0.2369,
      "step": 107
    },
    {
      "epoch": 2.118811881188119,
      "grad_norm": 0.12272965908050537,
      "learning_rate": 2.9133858267716534e-05,
      "loss": 0.2408,
      "step": 108
    },
    {
      "epoch": 2.1386138613861387,
      "grad_norm": 0.1395876407623291,
      "learning_rate": 2.8937007874015748e-05,
      "loss": 0.2368,
      "step": 109
    },
    {
      "epoch": 2.1584158415841586,
      "grad_norm": 0.13436202704906464,
      "learning_rate": 2.874015748031496e-05,
      "loss": 0.2445,
      "step": 110
    },
    {
      "epoch": 2.1782178217821784,
      "grad_norm": 0.16352657973766327,
      "learning_rate": 2.8543307086614175e-05,
      "loss": 0.2695,
      "step": 111
    },
    {
      "epoch": 2.198019801980198,
      "grad_norm": 0.15960074961185455,
      "learning_rate": 2.8346456692913388e-05,
      "loss": 0.2263,
      "step": 112
    },
    {
      "epoch": 2.217821782178218,
      "grad_norm": 0.13397057354450226,
      "learning_rate": 2.81496062992126e-05,
      "loss": 0.1919,
      "step": 113
    },
    {
      "epoch": 2.237623762376238,
      "grad_norm": 0.13477912545204163,
      "learning_rate": 2.7952755905511812e-05,
      "loss": 0.25,
      "step": 114
    },
    {
      "epoch": 2.2574257425742577,
      "grad_norm": 0.16521519422531128,
      "learning_rate": 2.7755905511811025e-05,
      "loss": 0.2292,
      "step": 115
    },
    {
      "epoch": 2.2772277227722775,
      "grad_norm": 0.1521223932504654,
      "learning_rate": 2.755905511811024e-05,
      "loss": 0.2286,
      "step": 116
    },
    {
      "epoch": 2.297029702970297,
      "grad_norm": 0.14391447603702545,
      "learning_rate": 2.7362204724409452e-05,
      "loss": 0.2299,
      "step": 117
    },
    {
      "epoch": 2.3168316831683167,
      "grad_norm": 0.16619576513767242,
      "learning_rate": 2.7165354330708666e-05,
      "loss": 0.2634,
      "step": 118
    },
    {
      "epoch": 2.3366336633663365,
      "grad_norm": 0.15883806347846985,
      "learning_rate": 2.6968503937007876e-05,
      "loss": 0.2243,
      "step": 119
    },
    {
      "epoch": 2.3564356435643563,
      "grad_norm": 0.15988689661026,
      "learning_rate": 2.677165354330709e-05,
      "loss": 0.2192,
      "step": 120
    },
    {
      "epoch": 2.376237623762376,
      "grad_norm": 0.17835742235183716,
      "learning_rate": 2.6574803149606303e-05,
      "loss": 0.234,
      "step": 121
    },
    {
      "epoch": 2.396039603960396,
      "grad_norm": 0.17285269498825073,
      "learning_rate": 2.637795275590551e-05,
      "loss": 0.2129,
      "step": 122
    },
    {
      "epoch": 2.4158415841584158,
      "grad_norm": 0.21704521775245667,
      "learning_rate": 2.6181102362204723e-05,
      "loss": 0.2258,
      "step": 123
    },
    {
      "epoch": 2.4356435643564356,
      "grad_norm": 0.17467202246189117,
      "learning_rate": 2.5984251968503937e-05,
      "loss": 0.2127,
      "step": 124
    },
    {
      "epoch": 2.4554455445544554,
      "grad_norm": 0.16866809129714966,
      "learning_rate": 2.578740157480315e-05,
      "loss": 0.2323,
      "step": 125
    },
    {
      "epoch": 2.4752475247524752,
      "grad_norm": 0.1558874547481537,
      "learning_rate": 2.5590551181102364e-05,
      "loss": 0.2304,
      "step": 126
    },
    {
      "epoch": 2.495049504950495,
      "grad_norm": 0.149862140417099,
      "learning_rate": 2.5393700787401574e-05,
      "loss": 0.2245,
      "step": 127
    },
    {
      "epoch": 2.514851485148515,
      "grad_norm": 0.14459988474845886,
      "learning_rate": 2.5196850393700788e-05,
      "loss": 0.1792,
      "step": 128
    },
    {
      "epoch": 2.5346534653465347,
      "grad_norm": 0.23399324715137482,
      "learning_rate": 2.5e-05,
      "loss": 0.2409,
      "step": 129
    },
    {
      "epoch": 2.5544554455445545,
      "grad_norm": 0.14608442783355713,
      "learning_rate": 2.4803149606299215e-05,
      "loss": 0.2039,
      "step": 130
    },
    {
      "epoch": 2.5742574257425743,
      "grad_norm": 0.13774733245372772,
      "learning_rate": 2.4606299212598428e-05,
      "loss": 0.2235,
      "step": 131
    },
    {
      "epoch": 2.594059405940594,
      "grad_norm": 0.1577409952878952,
      "learning_rate": 2.440944881889764e-05,
      "loss": 0.2249,
      "step": 132
    },
    {
      "epoch": 2.613861386138614,
      "grad_norm": 0.1336652934551239,
      "learning_rate": 2.421259842519685e-05,
      "loss": 0.1758,
      "step": 133
    },
    {
      "epoch": 2.633663366336634,
      "grad_norm": 0.15572400391101837,
      "learning_rate": 2.4015748031496062e-05,
      "loss": 0.2516,
      "step": 134
    },
    {
      "epoch": 2.6534653465346536,
      "grad_norm": 0.13440078496932983,
      "learning_rate": 2.3818897637795276e-05,
      "loss": 0.1963,
      "step": 135
    },
    {
      "epoch": 2.6732673267326734,
      "grad_norm": 0.13364750146865845,
      "learning_rate": 2.362204724409449e-05,
      "loss": 0.1882,
      "step": 136
    },
    {
      "epoch": 2.693069306930693,
      "grad_norm": 0.13460496068000793,
      "learning_rate": 2.3425196850393703e-05,
      "loss": 0.2188,
      "step": 137
    },
    {
      "epoch": 2.7128712871287126,
      "grad_norm": 0.1419312059879303,
      "learning_rate": 2.3228346456692916e-05,
      "loss": 0.187,
      "step": 138
    },
    {
      "epoch": 2.7326732673267324,
      "grad_norm": 0.17342601716518402,
      "learning_rate": 2.3031496062992126e-05,
      "loss": 0.2185,
      "step": 139
    },
    {
      "epoch": 2.7524752475247523,
      "grad_norm": 0.17396043241024017,
      "learning_rate": 2.283464566929134e-05,
      "loss": 0.1955,
      "step": 140
    },
    {
      "epoch": 2.772277227722772,
      "grad_norm": 0.15619756281375885,
      "learning_rate": 2.263779527559055e-05,
      "loss": 0.2119,
      "step": 141
    },
    {
      "epoch": 2.792079207920792,
      "grad_norm": 0.27558374404907227,
      "learning_rate": 2.2440944881889763e-05,
      "loss": 0.2442,
      "step": 142
    },
    {
      "epoch": 2.8118811881188117,
      "grad_norm": 0.16408875584602356,
      "learning_rate": 2.2244094488188977e-05,
      "loss": 0.2203,
      "step": 143
    },
    {
      "epoch": 2.8316831683168315,
      "grad_norm": 0.12808479368686676,
      "learning_rate": 2.204724409448819e-05,
      "loss": 0.1949,
      "step": 144
    },
    {
      "epoch": 2.8514851485148514,
      "grad_norm": 0.15894761681556702,
      "learning_rate": 2.1850393700787404e-05,
      "loss": 0.2125,
      "step": 145
    },
    {
      "epoch": 2.871287128712871,
      "grad_norm": 0.20727750658988953,
      "learning_rate": 2.1653543307086614e-05,
      "loss": 0.1857,
      "step": 146
    },
    {
      "epoch": 2.891089108910891,
      "grad_norm": 0.18315398693084717,
      "learning_rate": 2.1456692913385828e-05,
      "loss": 0.1934,
      "step": 147
    },
    {
      "epoch": 2.910891089108911,
      "grad_norm": 0.13809241354465485,
      "learning_rate": 2.125984251968504e-05,
      "loss": 0.1984,
      "step": 148
    },
    {
      "epoch": 2.9306930693069306,
      "grad_norm": 0.1576603502035141,
      "learning_rate": 2.106299212598425e-05,
      "loss": 0.1903,
      "step": 149
    },
    {
      "epoch": 2.9504950495049505,
      "grad_norm": 0.1464318484067917,
      "learning_rate": 2.0866141732283465e-05,
      "loss": 0.1921,
      "step": 150
    },
    {
      "epoch": 2.9702970297029703,
      "grad_norm": 0.14904767274856567,
      "learning_rate": 2.066929133858268e-05,
      "loss": 0.2005,
      "step": 151
    },
    {
      "epoch": 2.99009900990099,
      "grad_norm": 0.133843794465065,
      "learning_rate": 2.0472440944881892e-05,
      "loss": 0.2014,
      "step": 152
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.20650672912597656,
      "learning_rate": 2.0275590551181102e-05,
      "loss": 0.1805,
      "step": 153
    },
    {
      "epoch": 3.01980198019802,
      "grad_norm": 0.13029538094997406,
      "learning_rate": 2.0078740157480316e-05,
      "loss": 0.1938,
      "step": 154
    },
    {
      "epoch": 3.0396039603960396,
      "grad_norm": 0.1432717740535736,
      "learning_rate": 1.988188976377953e-05,
      "loss": 0.1613,
      "step": 155
    },
    {
      "epoch": 3.0594059405940595,
      "grad_norm": 0.12791474163532257,
      "learning_rate": 1.9685039370078743e-05,
      "loss": 0.1845,
      "step": 156
    },
    {
      "epoch": 3.0792079207920793,
      "grad_norm": 0.1294650286436081,
      "learning_rate": 1.9488188976377953e-05,
      "loss": 0.1822,
      "step": 157
    },
    {
      "epoch": 3.099009900990099,
      "grad_norm": 0.19018225371837616,
      "learning_rate": 1.9291338582677166e-05,
      "loss": 0.2277,
      "step": 158
    },
    {
      "epoch": 3.118811881188119,
      "grad_norm": 0.13053877651691437,
      "learning_rate": 1.909448818897638e-05,
      "loss": 0.1562,
      "step": 159
    },
    {
      "epoch": 3.1386138613861387,
      "grad_norm": 0.19172057509422302,
      "learning_rate": 1.889763779527559e-05,
      "loss": 0.2188,
      "step": 160
    },
    {
      "epoch": 3.1584158415841586,
      "grad_norm": 0.1482454538345337,
      "learning_rate": 1.8700787401574803e-05,
      "loss": 0.1869,
      "step": 161
    },
    {
      "epoch": 3.1782178217821784,
      "grad_norm": 0.16117160022258759,
      "learning_rate": 1.8503937007874017e-05,
      "loss": 0.2078,
      "step": 162
    },
    {
      "epoch": 3.198019801980198,
      "grad_norm": 0.13646164536476135,
      "learning_rate": 1.830708661417323e-05,
      "loss": 0.2067,
      "step": 163
    },
    {
      "epoch": 3.217821782178218,
      "grad_norm": 0.145894855260849,
      "learning_rate": 1.8110236220472444e-05,
      "loss": 0.2005,
      "step": 164
    },
    {
      "epoch": 3.237623762376238,
      "grad_norm": 0.15823297202587128,
      "learning_rate": 1.7913385826771654e-05,
      "loss": 0.1954,
      "step": 165
    },
    {
      "epoch": 3.2574257425742577,
      "grad_norm": 0.17035618424415588,
      "learning_rate": 1.7716535433070868e-05,
      "loss": 0.2067,
      "step": 166
    },
    {
      "epoch": 3.2772277227722775,
      "grad_norm": 0.1430296152830124,
      "learning_rate": 1.7519685039370078e-05,
      "loss": 0.2269,
      "step": 167
    },
    {
      "epoch": 3.297029702970297,
      "grad_norm": 0.13649220764636993,
      "learning_rate": 1.732283464566929e-05,
      "loss": 0.195,
      "step": 168
    },
    {
      "epoch": 3.3168316831683167,
      "grad_norm": 0.14319133758544922,
      "learning_rate": 1.7125984251968505e-05,
      "loss": 0.1667,
      "step": 169
    },
    {
      "epoch": 3.3366336633663365,
      "grad_norm": 0.1404385268688202,
      "learning_rate": 1.692913385826772e-05,
      "loss": 0.2015,
      "step": 170
    },
    {
      "epoch": 3.3564356435643563,
      "grad_norm": 0.15601474046707153,
      "learning_rate": 1.6732283464566932e-05,
      "loss": 0.208,
      "step": 171
    },
    {
      "epoch": 3.376237623762376,
      "grad_norm": 0.19225198030471802,
      "learning_rate": 1.6535433070866142e-05,
      "loss": 0.2244,
      "step": 172
    },
    {
      "epoch": 3.396039603960396,
      "grad_norm": 0.15142081677913666,
      "learning_rate": 1.6338582677165352e-05,
      "loss": 0.1844,
      "step": 173
    },
    {
      "epoch": 3.4158415841584158,
      "grad_norm": 0.1632416993379593,
      "learning_rate": 1.6141732283464566e-05,
      "loss": 0.2014,
      "step": 174
    },
    {
      "epoch": 3.4356435643564356,
      "grad_norm": 0.15120622515678406,
      "learning_rate": 1.594488188976378e-05,
      "loss": 0.1842,
      "step": 175
    },
    {
      "epoch": 3.4554455445544554,
      "grad_norm": 0.1707478165626526,
      "learning_rate": 1.5748031496062993e-05,
      "loss": 0.1964,
      "step": 176
    },
    {
      "epoch": 3.4752475247524752,
      "grad_norm": 0.1527477204799652,
      "learning_rate": 1.5551181102362206e-05,
      "loss": 0.1456,
      "step": 177
    },
    {
      "epoch": 3.495049504950495,
      "grad_norm": 0.14967618882656097,
      "learning_rate": 1.535433070866142e-05,
      "loss": 0.1591,
      "step": 178
    },
    {
      "epoch": 3.514851485148515,
      "grad_norm": 0.1487829089164734,
      "learning_rate": 1.5157480314960632e-05,
      "loss": 0.1714,
      "step": 179
    },
    {
      "epoch": 3.5346534653465347,
      "grad_norm": 0.14302381873130798,
      "learning_rate": 1.4960629921259845e-05,
      "loss": 0.1642,
      "step": 180
    },
    {
      "epoch": 3.5544554455445545,
      "grad_norm": 0.376909077167511,
      "learning_rate": 1.4763779527559055e-05,
      "loss": 0.1909,
      "step": 181
    },
    {
      "epoch": 3.5742574257425743,
      "grad_norm": 0.14797155559062958,
      "learning_rate": 1.4566929133858267e-05,
      "loss": 0.1964,
      "step": 182
    },
    {
      "epoch": 3.594059405940594,
      "grad_norm": 0.145481139421463,
      "learning_rate": 1.437007874015748e-05,
      "loss": 0.1797,
      "step": 183
    },
    {
      "epoch": 3.613861386138614,
      "grad_norm": 0.14987491071224213,
      "learning_rate": 1.4173228346456694e-05,
      "loss": 0.1804,
      "step": 184
    },
    {
      "epoch": 3.633663366336634,
      "grad_norm": 0.16435696184635162,
      "learning_rate": 1.3976377952755906e-05,
      "loss": 0.1682,
      "step": 185
    },
    {
      "epoch": 3.6534653465346536,
      "grad_norm": 0.1558253914117813,
      "learning_rate": 1.377952755905512e-05,
      "loss": 0.1634,
      "step": 186
    },
    {
      "epoch": 3.6732673267326734,
      "grad_norm": 0.14653699100017548,
      "learning_rate": 1.3582677165354333e-05,
      "loss": 0.1709,
      "step": 187
    },
    {
      "epoch": 3.693069306930693,
      "grad_norm": 0.1619860678911209,
      "learning_rate": 1.3385826771653545e-05,
      "loss": 0.1629,
      "step": 188
    },
    {
      "epoch": 3.7128712871287126,
      "grad_norm": 0.17726019024848938,
      "learning_rate": 1.3188976377952755e-05,
      "loss": 0.2022,
      "step": 189
    },
    {
      "epoch": 3.7326732673267324,
      "grad_norm": 0.1539468914270401,
      "learning_rate": 1.2992125984251968e-05,
      "loss": 0.1798,
      "step": 190
    },
    {
      "epoch": 3.7524752475247523,
      "grad_norm": 0.15450653433799744,
      "learning_rate": 1.2795275590551182e-05,
      "loss": 0.1819,
      "step": 191
    },
    {
      "epoch": 3.772277227722772,
      "grad_norm": 0.1464281529188156,
      "learning_rate": 1.2598425196850394e-05,
      "loss": 0.1801,
      "step": 192
    },
    {
      "epoch": 3.792079207920792,
      "grad_norm": 0.1645045429468155,
      "learning_rate": 1.2401574803149607e-05,
      "loss": 0.13,
      "step": 193
    },
    {
      "epoch": 3.8118811881188117,
      "grad_norm": 0.18286752700805664,
      "learning_rate": 1.220472440944882e-05,
      "loss": 0.164,
      "step": 194
    },
    {
      "epoch": 3.8316831683168315,
      "grad_norm": 0.1431460678577423,
      "learning_rate": 1.2007874015748031e-05,
      "loss": 0.1519,
      "step": 195
    },
    {
      "epoch": 3.8514851485148514,
      "grad_norm": 0.15322867035865784,
      "learning_rate": 1.1811023622047245e-05,
      "loss": 0.1861,
      "step": 196
    },
    {
      "epoch": 3.871287128712871,
      "grad_norm": 0.19995741546154022,
      "learning_rate": 1.1614173228346458e-05,
      "loss": 0.1657,
      "step": 197
    },
    {
      "epoch": 3.891089108910891,
      "grad_norm": 0.1534164696931839,
      "learning_rate": 1.141732283464567e-05,
      "loss": 0.1504,
      "step": 198
    },
    {
      "epoch": 3.910891089108911,
      "grad_norm": 0.17201939225196838,
      "learning_rate": 1.1220472440944882e-05,
      "loss": 0.1772,
      "step": 199
    },
    {
      "epoch": 3.9306930693069306,
      "grad_norm": 0.15362760424613953,
      "learning_rate": 1.1023622047244095e-05,
      "loss": 0.1693,
      "step": 200
    },
    {
      "epoch": 3.9504950495049505,
      "grad_norm": 0.15993331372737885,
      "learning_rate": 1.0826771653543307e-05,
      "loss": 0.1752,
      "step": 201
    },
    {
      "epoch": 3.9702970297029703,
      "grad_norm": 0.1582729071378708,
      "learning_rate": 1.062992125984252e-05,
      "loss": 0.1951,
      "step": 202
    },
    {
      "epoch": 3.99009900990099,
      "grad_norm": 0.1534530073404312,
      "learning_rate": 1.0433070866141732e-05,
      "loss": 0.1884,
      "step": 203
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.21872729063034058,
      "learning_rate": 1.0236220472440946e-05,
      "loss": 0.1721,
      "step": 204
    },
    {
      "epoch": 4.01980198019802,
      "grad_norm": 0.1938530057668686,
      "learning_rate": 1.0039370078740158e-05,
      "loss": 0.1708,
      "step": 205
    },
    {
      "epoch": 4.03960396039604,
      "grad_norm": 0.18118008971214294,
      "learning_rate": 9.842519685039371e-06,
      "loss": 0.2002,
      "step": 206
    },
    {
      "epoch": 4.0594059405940595,
      "grad_norm": 0.19067193567752838,
      "learning_rate": 9.645669291338583e-06,
      "loss": 0.1478,
      "step": 207
    },
    {
      "epoch": 4.079207920792079,
      "grad_norm": 0.1644013226032257,
      "learning_rate": 9.448818897637795e-06,
      "loss": 0.1427,
      "step": 208
    },
    {
      "epoch": 4.099009900990099,
      "grad_norm": 0.16512839496135712,
      "learning_rate": 9.251968503937008e-06,
      "loss": 0.1555,
      "step": 209
    },
    {
      "epoch": 4.118811881188119,
      "grad_norm": 0.15123528242111206,
      "learning_rate": 9.055118110236222e-06,
      "loss": 0.1676,
      "step": 210
    },
    {
      "epoch": 4.138613861386139,
      "grad_norm": 0.1668044626712799,
      "learning_rate": 8.858267716535434e-06,
      "loss": 0.1441,
      "step": 211
    },
    {
      "epoch": 4.158415841584159,
      "grad_norm": 0.1718062311410904,
      "learning_rate": 8.661417322834646e-06,
      "loss": 0.1617,
      "step": 212
    },
    {
      "epoch": 4.178217821782178,
      "grad_norm": 0.17790904641151428,
      "learning_rate": 8.46456692913386e-06,
      "loss": 0.1606,
      "step": 213
    },
    {
      "epoch": 4.198019801980198,
      "grad_norm": 0.17437264323234558,
      "learning_rate": 8.267716535433071e-06,
      "loss": 0.1502,
      "step": 214
    },
    {
      "epoch": 4.217821782178218,
      "grad_norm": 0.17190620303153992,
      "learning_rate": 8.070866141732283e-06,
      "loss": 0.1522,
      "step": 215
    },
    {
      "epoch": 4.237623762376238,
      "grad_norm": 0.15504249930381775,
      "learning_rate": 7.874015748031496e-06,
      "loss": 0.1561,
      "step": 216
    },
    {
      "epoch": 4.257425742574258,
      "grad_norm": 0.1885247677564621,
      "learning_rate": 7.67716535433071e-06,
      "loss": 0.1444,
      "step": 217
    },
    {
      "epoch": 4.2772277227722775,
      "grad_norm": 0.1620141714811325,
      "learning_rate": 7.4803149606299226e-06,
      "loss": 0.17,
      "step": 218
    },
    {
      "epoch": 4.297029702970297,
      "grad_norm": 0.16580066084861755,
      "learning_rate": 7.2834645669291335e-06,
      "loss": 0.1457,
      "step": 219
    },
    {
      "epoch": 4.316831683168317,
      "grad_norm": 0.18525613844394684,
      "learning_rate": 7.086614173228347e-06,
      "loss": 0.1213,
      "step": 220
    },
    {
      "epoch": 4.336633663366337,
      "grad_norm": 0.16782714426517487,
      "learning_rate": 6.88976377952756e-06,
      "loss": 0.2248,
      "step": 221
    },
    {
      "epoch": 4.356435643564357,
      "grad_norm": 0.19515907764434814,
      "learning_rate": 6.692913385826772e-06,
      "loss": 0.1315,
      "step": 222
    },
    {
      "epoch": 4.376237623762377,
      "grad_norm": 0.18009702861309052,
      "learning_rate": 6.496062992125984e-06,
      "loss": 0.1792,
      "step": 223
    },
    {
      "epoch": 4.396039603960396,
      "grad_norm": 0.17780788242816925,
      "learning_rate": 6.299212598425197e-06,
      "loss": 0.1508,
      "step": 224
    },
    {
      "epoch": 4.415841584158416,
      "grad_norm": 0.20978553593158722,
      "learning_rate": 6.10236220472441e-06,
      "loss": 0.1779,
      "step": 225
    },
    {
      "epoch": 4.435643564356436,
      "grad_norm": 0.16487719118595123,
      "learning_rate": 5.905511811023622e-06,
      "loss": 0.1819,
      "step": 226
    },
    {
      "epoch": 4.455445544554456,
      "grad_norm": 0.21034634113311768,
      "learning_rate": 5.708661417322835e-06,
      "loss": 0.1654,
      "step": 227
    },
    {
      "epoch": 4.475247524752476,
      "grad_norm": 0.18497024476528168,
      "learning_rate": 5.511811023622048e-06,
      "loss": 0.1897,
      "step": 228
    },
    {
      "epoch": 4.4950495049504955,
      "grad_norm": 0.18395379185676575,
      "learning_rate": 5.31496062992126e-06,
      "loss": 0.169,
      "step": 229
    },
    {
      "epoch": 4.514851485148515,
      "grad_norm": 0.17008616030216217,
      "learning_rate": 5.118110236220473e-06,
      "loss": 0.1422,
      "step": 230
    },
    {
      "epoch": 4.534653465346535,
      "grad_norm": 0.15786252915859222,
      "learning_rate": 4.921259842519686e-06,
      "loss": 0.1971,
      "step": 231
    },
    {
      "epoch": 4.554455445544555,
      "grad_norm": 0.1753176897764206,
      "learning_rate": 4.7244094488188975e-06,
      "loss": 0.1462,
      "step": 232
    },
    {
      "epoch": 4.574257425742574,
      "grad_norm": 0.18844591081142426,
      "learning_rate": 4.527559055118111e-06,
      "loss": 0.1701,
      "step": 233
    },
    {
      "epoch": 4.594059405940594,
      "grad_norm": 0.17928394675254822,
      "learning_rate": 4.330708661417323e-06,
      "loss": 0.1544,
      "step": 234
    },
    {
      "epoch": 4.6138613861386135,
      "grad_norm": 0.19836072623729706,
      "learning_rate": 4.1338582677165355e-06,
      "loss": 0.141,
      "step": 235
    },
    {
      "epoch": 4.633663366336633,
      "grad_norm": 0.21720781922340393,
      "learning_rate": 3.937007874015748e-06,
      "loss": 0.2063,
      "step": 236
    },
    {
      "epoch": 4.653465346534653,
      "grad_norm": 0.17349708080291748,
      "learning_rate": 3.7401574803149613e-06,
      "loss": 0.1863,
      "step": 237
    },
    {
      "epoch": 4.673267326732673,
      "grad_norm": 0.19645293056964874,
      "learning_rate": 3.5433070866141735e-06,
      "loss": 0.1509,
      "step": 238
    },
    {
      "epoch": 4.693069306930693,
      "grad_norm": 0.4390944242477417,
      "learning_rate": 3.346456692913386e-06,
      "loss": 0.1627,
      "step": 239
    },
    {
      "epoch": 4.712871287128713,
      "grad_norm": 0.1961442083120346,
      "learning_rate": 3.1496062992125985e-06,
      "loss": 0.1188,
      "step": 240
    },
    {
      "epoch": 4.732673267326732,
      "grad_norm": 0.19254860281944275,
      "learning_rate": 2.952755905511811e-06,
      "loss": 0.1772,
      "step": 241
    },
    {
      "epoch": 4.752475247524752,
      "grad_norm": 0.18181374669075012,
      "learning_rate": 2.755905511811024e-06,
      "loss": 0.1389,
      "step": 242
    },
    {
      "epoch": 4.772277227722772,
      "grad_norm": 0.22015556693077087,
      "learning_rate": 2.5590551181102365e-06,
      "loss": 0.1548,
      "step": 243
    },
    {
      "epoch": 4.792079207920792,
      "grad_norm": 0.21099373698234558,
      "learning_rate": 2.3622047244094487e-06,
      "loss": 0.1857,
      "step": 244
    },
    {
      "epoch": 4.811881188118812,
      "grad_norm": 0.20058374106884003,
      "learning_rate": 2.1653543307086614e-06,
      "loss": 0.1713,
      "step": 245
    },
    {
      "epoch": 4.8316831683168315,
      "grad_norm": 0.201721653342247,
      "learning_rate": 1.968503937007874e-06,
      "loss": 0.157,
      "step": 246
    },
    {
      "epoch": 4.851485148514851,
      "grad_norm": 0.18324393033981323,
      "learning_rate": 1.7716535433070868e-06,
      "loss": 0.1743,
      "step": 247
    },
    {
      "epoch": 4.871287128712871,
      "grad_norm": 0.20441223680973053,
      "learning_rate": 1.5748031496062992e-06,
      "loss": 0.1661,
      "step": 248
    },
    {
      "epoch": 4.891089108910891,
      "grad_norm": 0.19138465821743011,
      "learning_rate": 1.377952755905512e-06,
      "loss": 0.1571,
      "step": 249
    },
    {
      "epoch": 4.910891089108911,
      "grad_norm": 0.2944197952747345,
      "learning_rate": 1.1811023622047244e-06,
      "loss": 0.1667,
      "step": 250
    },
    {
      "epoch": 4.930693069306931,
      "grad_norm": 0.21054449677467346,
      "learning_rate": 9.84251968503937e-07,
      "loss": 0.1641,
      "step": 251
    },
    {
      "epoch": 4.9504950495049505,
      "grad_norm": 0.2090580016374588,
      "learning_rate": 7.874015748031496e-07,
      "loss": 0.1441,
      "step": 252
    },
    {
      "epoch": 4.97029702970297,
      "grad_norm": 0.20796948671340942,
      "learning_rate": 5.905511811023622e-07,
      "loss": 0.1783,
      "step": 253
    },
    {
      "epoch": 4.99009900990099,
      "grad_norm": 0.20466487109661102,
      "learning_rate": 3.937007874015748e-07,
      "loss": 0.1296,
      "step": 254
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.2888715863227844,
      "learning_rate": 1.968503937007874e-07,
      "loss": 0.1844,
      "step": 255
    }
  ],
  "logging_steps": 1,
  "max_steps": 255,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8752900803133440.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
