{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8247422680412371,
  "eval_steps": 500,
  "global_step": 40,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020618556701030927,
      "grad_norm": 21.5922794342041,
      "learning_rate": 0.0,
      "loss": 17.1898,
      "step": 1
    },
    {
      "epoch": 0.041237113402061855,
      "grad_norm": 19.387638092041016,
      "learning_rate": 5e-05,
      "loss": 17.3364,
      "step": 2
    },
    {
      "epoch": 0.061855670103092786,
      "grad_norm": 21.481809616088867,
      "learning_rate": 4.9657534246575346e-05,
      "loss": 17.0608,
      "step": 3
    },
    {
      "epoch": 0.08247422680412371,
      "grad_norm": 22.30386734008789,
      "learning_rate": 4.9315068493150684e-05,
      "loss": 16.5872,
      "step": 4
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 22.487586975097656,
      "learning_rate": 4.8972602739726034e-05,
      "loss": 16.3947,
      "step": 5
    },
    {
      "epoch": 0.12371134020618557,
      "grad_norm": 25.722307205200195,
      "learning_rate": 4.863013698630137e-05,
      "loss": 15.8105,
      "step": 6
    },
    {
      "epoch": 0.14432989690721648,
      "grad_norm": 28.542400360107422,
      "learning_rate": 4.8287671232876716e-05,
      "loss": 15.2682,
      "step": 7
    },
    {
      "epoch": 0.16494845360824742,
      "grad_norm": 29.77732276916504,
      "learning_rate": 4.794520547945205e-05,
      "loss": 14.9193,
      "step": 8
    },
    {
      "epoch": 0.18556701030927836,
      "grad_norm": 33.32551956176758,
      "learning_rate": 4.7602739726027403e-05,
      "loss": 14.3228,
      "step": 9
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 37.55258560180664,
      "learning_rate": 4.726027397260274e-05,
      "loss": 13.6558,
      "step": 10
    },
    {
      "epoch": 0.2268041237113402,
      "grad_norm": 39.08740234375,
      "learning_rate": 4.6917808219178085e-05,
      "loss": 13.1666,
      "step": 11
    },
    {
      "epoch": 0.24742268041237114,
      "grad_norm": 45.75041198730469,
      "learning_rate": 4.657534246575342e-05,
      "loss": 12.0574,
      "step": 12
    },
    {
      "epoch": 0.26804123711340205,
      "grad_norm": 48.47603988647461,
      "learning_rate": 4.623287671232877e-05,
      "loss": 11.5825,
      "step": 13
    },
    {
      "epoch": 0.28865979381443296,
      "grad_norm": 49.012001037597656,
      "learning_rate": 4.589041095890411e-05,
      "loss": 10.3258,
      "step": 14
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 54.778411865234375,
      "learning_rate": 4.5547945205479454e-05,
      "loss": 9.6727,
      "step": 15
    },
    {
      "epoch": 0.32989690721649484,
      "grad_norm": 61.660728454589844,
      "learning_rate": 4.520547945205479e-05,
      "loss": 8.0491,
      "step": 16
    },
    {
      "epoch": 0.35051546391752575,
      "grad_norm": 67.43046569824219,
      "learning_rate": 4.486301369863014e-05,
      "loss": 7.0713,
      "step": 17
    },
    {
      "epoch": 0.3711340206185567,
      "grad_norm": 68.63656616210938,
      "learning_rate": 4.452054794520548e-05,
      "loss": 6.0531,
      "step": 18
    },
    {
      "epoch": 0.3917525773195876,
      "grad_norm": 70.52140045166016,
      "learning_rate": 4.417808219178082e-05,
      "loss": 4.4854,
      "step": 19
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 65.55616760253906,
      "learning_rate": 4.383561643835617e-05,
      "loss": 3.4128,
      "step": 20
    },
    {
      "epoch": 0.4329896907216495,
      "grad_norm": 46.678749084472656,
      "learning_rate": 4.349315068493151e-05,
      "loss": 1.6794,
      "step": 21
    },
    {
      "epoch": 0.4536082474226804,
      "grad_norm": 28.822498321533203,
      "learning_rate": 4.3150684931506855e-05,
      "loss": 1.0382,
      "step": 22
    },
    {
      "epoch": 0.4742268041237113,
      "grad_norm": 26.917720794677734,
      "learning_rate": 4.280821917808219e-05,
      "loss": 1.0155,
      "step": 23
    },
    {
      "epoch": 0.4948453608247423,
      "grad_norm": 10.786391258239746,
      "learning_rate": 4.2465753424657536e-05,
      "loss": 0.5632,
      "step": 24
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 3.7782979011535645,
      "learning_rate": 4.212328767123288e-05,
      "loss": 0.435,
      "step": 25
    },
    {
      "epoch": 0.5360824742268041,
      "grad_norm": 1.5777652263641357,
      "learning_rate": 4.1780821917808224e-05,
      "loss": 0.3791,
      "step": 26
    },
    {
      "epoch": 0.5567010309278351,
      "grad_norm": 1.2714552879333496,
      "learning_rate": 4.143835616438356e-05,
      "loss": 0.3656,
      "step": 27
    },
    {
      "epoch": 0.5773195876288659,
      "grad_norm": 0.9902676343917847,
      "learning_rate": 4.1095890410958905e-05,
      "loss": 0.3506,
      "step": 28
    },
    {
      "epoch": 0.5979381443298969,
      "grad_norm": 0.8217443227767944,
      "learning_rate": 4.075342465753425e-05,
      "loss": 0.3709,
      "step": 29
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 0.8923215866088867,
      "learning_rate": 4.041095890410959e-05,
      "loss": 0.3408,
      "step": 30
    },
    {
      "epoch": 0.6391752577319587,
      "grad_norm": 0.6736725568771362,
      "learning_rate": 4.006849315068493e-05,
      "loss": 0.3461,
      "step": 31
    },
    {
      "epoch": 0.6597938144329897,
      "grad_norm": 0.4275885820388794,
      "learning_rate": 3.9726027397260274e-05,
      "loss": 0.3548,
      "step": 32
    },
    {
      "epoch": 0.6804123711340206,
      "grad_norm": 0.3304567337036133,
      "learning_rate": 3.938356164383562e-05,
      "loss": 0.4025,
      "step": 33
    },
    {
      "epoch": 0.7010309278350515,
      "grad_norm": 0.2403475046157837,
      "learning_rate": 3.904109589041096e-05,
      "loss": 0.3613,
      "step": 34
    },
    {
      "epoch": 0.7216494845360825,
      "grad_norm": 0.23942090570926666,
      "learning_rate": 3.86986301369863e-05,
      "loss": 0.3602,
      "step": 35
    },
    {
      "epoch": 0.7422680412371134,
      "grad_norm": 0.18404865264892578,
      "learning_rate": 3.8356164383561644e-05,
      "loss": 0.338,
      "step": 36
    },
    {
      "epoch": 0.7628865979381443,
      "grad_norm": 0.17273037135601044,
      "learning_rate": 3.801369863013699e-05,
      "loss": 0.3171,
      "step": 37
    },
    {
      "epoch": 0.7835051546391752,
      "grad_norm": 0.17005254328250885,
      "learning_rate": 3.767123287671233e-05,
      "loss": 0.3466,
      "step": 38
    },
    {
      "epoch": 0.8041237113402062,
      "grad_norm": 0.18212974071502686,
      "learning_rate": 3.7328767123287675e-05,
      "loss": 0.3519,
      "step": 39
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 0.15205325186252594,
      "learning_rate": 3.698630136986301e-05,
      "loss": 0.3356,
      "step": 40
    }
  ],
  "logging_steps": 1,
  "max_steps": 147,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1386598147031040.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
